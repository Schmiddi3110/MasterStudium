{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T23:03:28.838892700Z",
     "start_time": "2023-12-19T23:03:28.824380100Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schmidtfa\\MasterStudium\\PythonEnvironment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import race\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d695c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from race import *\n",
    "class CurvyRaceEnv(gym.Env, CurvyRace):\n",
    "    def __init__(self):\n",
    "        gym.Env.__init__(self)\n",
    "        CurvyRace.__init__(self)\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([0, -VEL_ROT_LIMIT]),\n",
    "            high=np.array([VEL_TRANS_LIMIT, VEL_ROT_LIMIT]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.get_observation_dim(),), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        return CurvyRace.reset(self)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done = CurvyRace.step(self, action)\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        self.plot()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edaaebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schmidtfa\\MasterStudium\\PythonEnvironment\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_checker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_env\n\u001b[0;32m      2\u001b[0m test \u001b[38;5;241m=\u001b[39m CurvyRaceEnv()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcheck_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\schmidtfa\\MasterStudium\\PythonEnvironment\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:409\u001b[0m, in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_env\u001b[39m(env: gym\u001b[38;5;241m.\u001b[39mEnv, warn: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, skip_render_check: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m    Check that an environment follows Gym API.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    This is particularly useful when using a custom environment.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m        True by default (useful for the CI)\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    410\u001b[0m         env, gym\u001b[38;5;241m.\u001b[39mEnv\n\u001b[0;32m    411\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;66;03m# ============= Check the spaces (observation and action) ================\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     _check_spaces(env)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "test = CurvyRaceEnv()\n",
    "check_env(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041d8bc3e0be419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T23:03:30.045890800Z",
     "start_time": "2023-12-19T23:03:30.029876600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e34f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def study():\n",
    "    study = optuna.create_study(study_name='TD3_Hyperparameters', storage='sqlite:///DRL_Studienarbeit2.db',load_if_exists=True, direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial), n_trials=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61ba8968956430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T07:07:28.545647Z",
     "start_time": "2023-12-20T07:07:28.477588300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_best_paramerters(study_name, storage):\n",
    "    \"\"\"\n",
    "    This function retrun a dictionary of parameters of trial in the given study\n",
    "    Parameters:\n",
    "        study_name: Name of the study to load\n",
    "        storage: Location of the study\n",
    "\n",
    "    Return:\n",
    "         Dictionary of best parameters\n",
    "    \"\"\"\n",
    "    loaded_study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "    best_params = loaded_study.best_trial.params\n",
    "    return best_params\n",
    "\n",
    "params = load_best_paramerters(study_name='TD3_Hyperparameters', storage='sqlite:///DRL_Studienarbeit2.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ccae868fcc7f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T23:03:34.470451300Z",
     "start_time": "2023-12-19T23:03:34.456439200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = race.CurvyRace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f618665aed65d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T23:03:35.123714500Z",
     "start_time": "2023-12-19T23:03:35.100695500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, next_state, reward, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, next_state, reward, done = map(np.stack, zip(*batch))\n",
    "        return state, action, next_state, reward, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d890b86fef411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T23:03:35.880435900Z",
     "start_time": "2023-12-19T23:03:35.858992Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "# Define the Actor and Critic networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.layer3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = torch.relu(self.layer1(state))\n",
    "        a = torch.relu(self.layer2(a))\n",
    "        a = self.max_action * torch.tanh(self.layer3(a))\n",
    "        return a\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.layer3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = torch.relu(self.layer1(torch.cat([state, action], 1)))\n",
    "        q = torch.relu(self.layer2(q))\n",
    "        q = self.layer3(q)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22e2536925b7fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T07:09:16.477386900Z",
     "start_time": "2023-12-20T07:09:16.456888800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the TD3 class\n",
    "class TD3:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "\n",
    "        self.critic1 = Critic(state_dim, action_dim)\n",
    "        self.critic1_target = Critic(state_dim, action_dim)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=0.001)\n",
    "\n",
    "        self.critic2 = Critic(state_dim, action_dim)\n",
    "        self.critic2_target = Critic(state_dim, action_dim)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=0.001)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        return action\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=64, gamma=params['gamma'], soft_tau=params['soft_tau'], policy_noise=params['policy_noise'], noise_clip=params['noise_clip'], policy_freq=params['policy_freq']):\n",
    "        #gamma = trial.suggest_float('gamma', 0.8, 0.999)\n",
    "        #soft_tau = trial.suggest_float('soft_tau', 0.01, 0.1)\n",
    "        #policy_noise = trial.suggest_float('policy_noise', 0.1, 0.5)\n",
    "        #noise_clip = trial.suggest_float('noise_clip', 0.1, 0.5)\n",
    "        #policy_freq = trial.suggest_int('policy_freq', 1, 10)\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a batch from the replay buffer\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.FloatTensor(action)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        not_done = torch.FloatTensor(not_done)\n",
    "\n",
    "        # Update Critic networks\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn_like(action) * policy_noise).clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            target_Q1 = self.critic1_target(next_state, next_action)\n",
    "            target_Q2 = self.critic2_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * gamma * target_Q\n",
    "\n",
    "        current_Q1 = self.critic1(state, action)\n",
    "        current_Q2 = self.critic2(state, action)\n",
    "\n",
    "        critic1_loss = nn.functional.mse_loss(current_Q1, target_Q)\n",
    "        critic2_loss = nn.functional.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # Update Actor network\n",
    "        if self.step % policy_freq == 0:\n",
    "            actor_loss = -self.critic1(state, self.actor(state)).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update target networks\n",
    "            self.soft_update(self.actor, self.actor_target, soft_tau)\n",
    "            self.soft_update(self.critic1, self.critic1_target, soft_tau)\n",
    "            self.soft_update(self.critic2, self.critic2_target, soft_tau)\n",
    "\n",
    "    def soft_update(self, model, target_model, soft_tau):\n",
    "        for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "            target_param.data.copy_((1 - soft_tau) * target_param.data + soft_tau * param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32289643d072698b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T07:09:26.592557700Z",
     "start_time": "2023-12-20T07:09:26.562032Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = race.CurvyRace()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Initialize the TD3 agent\n",
    "state_dim = env.get_observation_dim()\n",
    "action_dim = env.get_action_dim()\n",
    "max_action = float(env.get_action_limits()[0])\n",
    "td3_agent = TD3(state_dim, action_dim, max_action)\n",
    "\n",
    "# Training parameters\n",
    "max_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "batch_size = 256\n",
    "actions=[]\n",
    "\n",
    "# Training loop\n",
    "def objective():\n",
    "    #scale_factor = trial.suggest_int('scale_factor', 1, 10)\n",
    "    #reward_shape = trial.suggest_int('reward_shape', 1, 10)\n",
    "    model_reward = 0\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "    \n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = td3_agent.select_action(state)\n",
    "            \n",
    "            actions.append(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "    \n",
    "            if reward == 1:            \n",
    "                total_reward += reward + (params['reward_shape'] * env.get_gate_idx()/len(env.get_gates())) \n",
    "    \n",
    "            td3_agent.train(replay_buffer)  # Update the TD3 agent\n",
    "    \n",
    "            state = next_state\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "        model_reward += total_reward\n",
    "        \n",
    "        #return model_reward\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "        if total_reward > 20:\n",
    "            env.plot()\n",
    "\n",
    "# Test the trained agent\n",
    "#state = env.reset()\n",
    "#done = False\n",
    "#while not done:\n",
    "    #action = td3_agent.select_action(state)\n",
    "    #next_state, reward, done = env.step(action)\n",
    "    #env.plot()\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0a7d60d78eef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T07:09:45.967547800Z",
     "start_time": "2023-12-20T07:09:28.217299300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18e2d769fe84ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T07:06:55.638209400Z",
     "start_time": "2023-12-19T23:16:39.396330400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090dfbd971bd36f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
