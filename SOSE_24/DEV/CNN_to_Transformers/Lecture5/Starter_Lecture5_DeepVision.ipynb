{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer, Swin Transformer, ConvNeXt (Basic usage from the Huggingface Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Install the required packages\n",
    "#!pip install transformers --upgrade\n",
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Inference on the pre-trained ViT (1000 ImageNet classes)\n",
    "- Load your own image\n",
    "- Run the model in the inference mode. Use different models, namely, ViT, Swin, Swinv2, ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "Predicted class: tiger cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "from transformers import ViTImageProcessor\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## Load the model with the given checkpoint\n",
    "## The checkpoint is the model name from the Hugging Face model hub\n",
    "## https://huggingface.co/models\n",
    "## TODO: Experiment with different models. To use Swin, you will need to export SwinForImageClassification and so on\n",
    "model_checkpoint = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_checkpoint)\n",
    "model.to(device)\n",
    "\n",
    "from PIL import Image\n",
    "## TODO: Load the image\n",
    "image = Image.open(\"cat_dog.jpg\")\n",
    "\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(model_checkpoint)\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "print(pixel_values.shape)\n",
    "     \n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values)\n",
    "logits = outputs.logits\n",
    "logits.shape\n",
    "\n",
    "\n",
    "prediction = logits.argmax(-1)\n",
    "print(\"Predicted class:\", model.config.id2label[prediction.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Fine-tuning the models using HuggingFace library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "# load cifar10 (only small portion for demonstration purposes) \n",
    "train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])\n",
    "# split up training into training + validation\n",
    "splits = train_ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "\n",
    "id2label = {id:label for id, label in enumerate(train_ds.features['label'].names)}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the transforms\n",
    "\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "     \n",
    "\n",
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Swinv2ForImageClassification were not initialized from the model checkpoint at microsoft/swinv2-tiny-patch4-window8-256 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## TODO: Experiment with different models. To use Swin, you will need to export SwinForImageClassification and set ignore_mismatched_sizes=True to adjust the last layer\n",
    "## Use Huggingface hub to find the model names: https://huggingface.co/models\n",
    "## Most likely you will have to use the tiny versions only and relatively small batch sizes\n",
    "## You can also use AutoModelForImageClassification to automatically load the correct model\n",
    "from transformers import ViTForImageClassification, SwinForImageClassification, Swinv2ForImageClassification, ConvNextForImageClassification\n",
    "\n",
    "model_checkpoint_swin = \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "model_checkpoint_vit = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "model_vit = ViTForImageClassification.from_pretrained(model_checkpoint_vit,\n",
    "                                                  id2label=id2label,\n",
    "                                                  label2id=label2id)\n",
    "\n",
    "model_swin = SwinForImageClassification.from_pretrained(model_checkpoint_swin,id2label=id2label,label2id=label2id, ignore_mismatched_sizes=True)\n",
    "\n",
    "model_swinv2 = Swinv2ForImageClassification.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\",id2label=id2label,label2id=label2id, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/MasterStudium/PythonEnvrionment/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda71855960847db8287124c771eece0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c72e392490a4470b3ad086296c4aec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1381642818450928, 'eval_accuracy': 0.958, 'eval_runtime': 2.195, 'eval_samples_per_second': 227.793, 'eval_steps_per_second': 56.948, 'epoch': 1.0}\n",
      "{'train_runtime': 60.7941, 'train_samples_per_second': 74.02, 'train_steps_per_second': 7.402, 'train_loss': 1.625765652126736, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =         1.0\n",
      "  total_flos               = 324788462GF\n",
      "  train_loss               =      1.6258\n",
      "  train_runtime            =  0:01:00.79\n",
      "  train_samples_per_second =       74.02\n",
      "  train_steps_per_second   =       7.402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4606fb760f944194a4baf1bfe66ef83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =      0.958\n",
      "  eval_loss               =     1.1382\n",
      "  eval_runtime            = 0:00:02.17\n",
      "  eval_samples_per_second =    230.232\n",
      "  eval_steps_per_second   =     57.558\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "model_checkpoint = model_checkpoint_vit\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-cifar-10\", #output directory\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,                 # for demonstration purposes, adjust as needed\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    "    report_to='tensorboard',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "     \n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_vit,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "train_results = trainer.train()\n",
    "\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "metrics = trainer.evaluate(val_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassifierOutput(loss=None, logits=tensor([[-8.5440e-01, -1.0846e+00, -1.1426e+00, -7.9423e-01, -1.4480e+00,\n",
      "         -2.4472e+00, -1.3543e+00, -9.8359e-01, -4.0294e-02, -9.5356e-01,\n",
      "         -6.1763e-01, -4.9857e-01, -1.6392e+00, -1.0770e+00,  3.7595e-02,\n",
      "         -1.3301e+00, -1.7214e+00, -6.1704e-01, -7.8492e-01, -8.4390e-01,\n",
      "         -1.2626e+00,  5.0594e-02, -1.4786e-02, -5.0586e-01, -1.8826e+00,\n",
      "         -8.1402e-01, -3.3778e-01, -7.1133e-01, -1.2755e+00, -8.0659e-01,\n",
      "         -6.5129e-01, -1.3521e+00, -1.1810e+00, -1.0990e+00, -1.4663e+00,\n",
      "         -5.5994e-01,  2.3655e-02,  1.3408e-01, -1.4066e+00, -4.0680e-01,\n",
      "         -7.3031e-01, -1.2301e+00, -7.2247e-01, -6.7186e-01, -1.1228e+00,\n",
      "         -9.1596e-02, -9.3965e-01, -1.6022e+00,  3.6684e-02, -4.3187e-01,\n",
      "         -9.4347e-01, -1.2348e+00, -7.5140e-01, -5.6927e-01, -9.4776e-01,\n",
      "         -1.0762e+00, -2.6795e-01, -7.9050e-01, -9.8067e-01, -1.4058e+00,\n",
      "         -4.0023e-01, -3.2413e-01, -1.9265e-01, -7.0715e-01, -9.3164e-01,\n",
      "         -1.0326e+00, -1.5334e-01, -1.2921e+00, -5.2908e-01, -8.9121e-01,\n",
      "         -2.5372e-01, -3.6604e-01,  1.6937e-01, -8.3657e-01, -6.9410e-02,\n",
      "         -5.2256e-01, -1.0478e+00, -5.6342e-01, -9.5818e-01,  4.2088e-01,\n",
      "         -7.0655e-01, -1.5944e+00, -1.1199e+00, -2.6821e-01, -3.6979e-02,\n",
      "         -1.6595e+00, -1.2510e+00,  1.7562e-01,  2.1618e-01, -6.3035e-01,\n",
      "         -4.3576e-01, -6.9345e-01, -1.0207e+00, -2.2579e-01, -6.3671e-02,\n",
      "         -7.0673e-01, -3.5404e-01, -3.4460e-01, -4.7082e-01, -3.7638e-01,\n",
      "         -6.5534e-01, -3.0642e-01, -3.2365e-01, -1.8359e-01, -1.3022e+00,\n",
      "         -5.7058e-01, -6.6513e-02,  1.3786e-01, -5.4095e-02, -1.1164e+00,\n",
      "         -1.4942e+00, -7.5110e-01, -6.4239e-01, -2.2341e-01, -9.3493e-01,\n",
      "         -1.7197e-01,  1.4497e-01, -8.8691e-01, -3.6629e-01, -8.8889e-01,\n",
      "         -7.7513e-01, -7.5404e-01, -5.3627e-01, -1.1766e+00, -5.9577e-01,\n",
      "         -2.2121e-01, -2.4455e-01, -2.4177e+00, -9.1149e-01, -1.0529e+00,\n",
      "         -4.9615e-01, -4.4185e-01, -1.2285e+00, -8.0726e-01, -1.3717e+00,\n",
      "         -1.4091e+00,  5.1057e-01, -4.8524e-01, -9.2994e-02, -1.4263e+00,\n",
      "         -1.5595e-01, -1.0994e+00, -1.2821e+00, -9.9164e-01, -6.5558e-01,\n",
      "         -6.6999e-01, -5.0213e-01, -9.7532e-01, -7.9465e-01, -8.6119e-01,\n",
      "         -1.9472e-02,  1.4268e+00,  1.6371e-01,  1.3953e+00,  3.7457e+00,\n",
      "          3.1975e+00,  2.4210e+00,  5.1492e-02,  1.0572e+00,  1.8247e+00,\n",
      "         -9.8558e-01,  3.0369e+00,  2.1554e+00,  2.6426e+00,  1.6044e+00,\n",
      "          9.3822e-01,  1.7394e+00,  1.0552e+00,  9.3446e-01, -1.2227e+00,\n",
      "         -4.0984e-02, -2.0940e-01, -4.0055e-01, -2.5213e-01,  2.2090e+00,\n",
      "          7.0896e-01, -2.7095e+00, -1.8029e+00,  1.3960e+00,  3.9618e+00,\n",
      "          5.5956e+00, -2.3758e+00,  2.2903e+00,  1.1275e-01,  1.7003e+00,\n",
      "          2.9296e+00,  2.0353e+00,  5.5342e-01,  5.9892e-01,  1.3595e+00,\n",
      "          8.1317e-02,  2.1684e+00,  2.4785e+00,  3.2119e+00,  4.6275e-02,\n",
      "          2.3909e+00,  5.8224e-01,  1.0046e+00,  5.7493e-01,  1.7492e+00,\n",
      "          2.6393e+00,  1.3149e+00,  1.6215e+00,  1.6385e+00,  4.6779e+00,\n",
      "         -7.0478e-01, -3.4953e-03,  1.5135e+00,  2.8793e+00,  2.9403e+00,\n",
      "          9.4997e-01,  1.4280e+00,  1.0501e+00, -5.9955e-01, -2.5531e-01,\n",
      "          2.3499e+00,  1.5743e+00,  1.3325e+00,  1.7319e+00,  1.8958e+00,\n",
      "          9.7601e-01, -2.9847e-01,  5.5630e-01,  1.7409e+00, -3.0684e-01,\n",
      "         -1.2455e-01,  1.3463e+00,  3.4254e-01, -1.0374e+00,  8.8361e-01,\n",
      "          2.5617e+00,  7.5640e-01,  1.4269e+00,  1.3370e+00,  3.1447e+00,\n",
      "          1.2444e+00,  1.8531e+00,  9.6595e-01,  2.9252e+00,  2.2552e+00,\n",
      "          1.5413e+00,  1.0314e+00,  4.3136e+00,  7.7434e+00,  1.6942e+00,\n",
      "          3.8399e+00,  2.8285e+00,  4.8166e+00,  1.7094e+00,  7.3320e-01,\n",
      "          1.8373e+00,  1.5032e+00,  5.0795e-01,  1.5332e+00,  3.0250e+00,\n",
      "          6.9367e-01,  2.5997e+00,  2.1834e+00,  1.0859e+00,  1.4785e+00,\n",
      "          3.9448e+00,  7.6881e-01,  1.2799e+00,  2.8257e+00,  2.3782e+00,\n",
      "          1.1228e+00,  2.1330e+00,  7.0635e-01, -7.7482e-01, -3.6207e-01,\n",
      "         -6.4519e-01, -8.4252e-01, -6.5461e-01,  8.3129e-01,  5.9159e-01,\n",
      "          9.9309e-02,  1.0720e+00, -8.5583e-01, -8.7072e-02, -1.2865e+00,\n",
      "         -8.3255e-01,  8.5945e+00,  1.0048e+01,  4.2870e+00,  3.2495e+00,\n",
      "          7.1502e+00,  8.3103e-01,  3.5749e+00,  6.8461e-01,  2.1009e-01,\n",
      "          1.4483e+00,  2.0993e+00,  4.7579e+00,  7.3607e-01,  4.7268e-01,\n",
      "         -3.8075e-02, -1.1423e+00, -7.7965e-01,  8.7672e-03,  5.1405e-01,\n",
      "         -6.2463e-01,  2.7631e-01, -5.0102e-01, -3.6909e-01,  9.8779e-02,\n",
      "         -3.5158e-01, -1.5318e+00, -6.4015e-01, -1.1162e+00, -1.5166e+00,\n",
      "         -8.9881e-01, -7.1421e-01, -8.2075e-01, -1.6245e+00,  2.2066e-01,\n",
      "         -1.5030e+00, -6.0273e-01, -1.2932e+00, -4.7324e-01, -6.9607e-01,\n",
      "         -1.0738e-01, -1.4628e+00, -1.1844e+00,  8.1255e-01, -3.6247e-01,\n",
      "         -8.2180e-01, -1.9106e+00, -8.6815e-01, -5.2697e-01, -8.9731e-02,\n",
      "         -4.2436e-01, -5.9510e-01,  8.7332e-01,  1.7804e+00, -6.4076e-01,\n",
      "          5.2427e-01, -1.4349e-01,  1.2723e+00,  1.9012e+00,  8.3240e-01,\n",
      "          2.5892e-01,  3.5590e-02,  1.7967e-01, -1.5853e-01,  1.6074e-01,\n",
      "          3.2207e-02, -5.6612e-01, -5.0604e-01,  3.5380e-01, -1.0522e+00,\n",
      "         -1.5700e+00, -2.2064e-01,  7.3780e-01,  9.5247e-01, -5.1375e-01,\n",
      "          2.3603e-01,  1.4502e+00,  2.0089e-01,  1.6600e+00,  2.7531e+00,\n",
      "         -1.4695e-01,  1.2659e+00,  1.5826e-01, -6.1990e-01, -1.2521e+00,\n",
      "         -4.6019e-01, -3.4758e-01, -7.5437e-01, -1.6791e+00, -1.3518e+00,\n",
      "         -4.2333e-01,  9.3002e-01, -1.0159e+00, -7.7755e-01, -1.6580e+00,\n",
      "         -2.5729e+00, -6.0170e-01, -6.3359e-01, -4.2891e-01, -3.6864e-01,\n",
      "         -6.3116e-01, -1.7574e+00, -3.6711e-01, -7.7379e-01, -1.8926e+00,\n",
      "          4.5863e-01,  1.9482e-01, -1.0450e-01, -7.3024e-01, -8.0138e-01,\n",
      "         -1.5200e+00, -1.7317e+00, -1.1461e+00, -4.4949e-01, -1.4652e+00,\n",
      "         -1.7537e+00, -2.7061e-01, -1.9244e+00, -4.1262e-01, -1.8358e-01,\n",
      "          7.0660e-01, -7.0862e-01,  7.8174e-01, -1.6667e+00, -1.2047e+00,\n",
      "         -1.8995e-01, -2.7027e-01, -5.6021e-01, -1.1504e+00, -2.5399e-01,\n",
      "         -1.1390e+00, -1.2121e-01, -2.6115e-01,  7.1069e-02,  7.6692e-01,\n",
      "          8.0259e-04, -5.6960e-01,  4.5091e-01, -3.4742e-01,  1.4359e+00,\n",
      "          3.2822e-01, -5.3899e-01,  1.0574e+00, -2.6635e-01, -4.9819e-01,\n",
      "         -6.4894e-01,  3.6756e-01, -1.2355e-01, -7.8799e-01, -1.8411e-01,\n",
      "         -4.4164e-01,  3.1801e-01, -3.0644e-01,  3.1611e-01,  1.3152e+00,\n",
      "          5.9954e-01, -6.3496e-01, -8.9341e-01, -6.2413e-01, -4.0320e-01,\n",
      "         -9.2738e-02,  9.7384e-02, -8.7615e-01, -3.1687e-01, -1.1128e+00,\n",
      "         -1.1850e+00, -5.4211e-01,  3.8329e-01, -2.0285e+00, -1.7165e+00,\n",
      "         -7.8402e-01, -5.2862e-01,  5.6103e-01,  2.0307e+00,  1.9831e-01,\n",
      "          1.4221e+00, -9.9240e-01,  1.6555e+00, -5.5681e-01, -1.0515e-01,\n",
      "         -6.1679e-01, -7.7707e-01,  1.1180e+00,  6.9718e-02, -4.6884e-01,\n",
      "         -1.5415e-01, -7.4760e-01, -2.1165e+00, -8.3039e-01, -2.6920e-01,\n",
      "          8.7102e-01, -9.2994e-01, -1.7738e+00,  1.9167e+00, -6.2174e-01,\n",
      "         -7.3678e-01, -7.3628e-01, -3.6582e-01,  8.2867e-01, -4.2408e-01,\n",
      "          9.4228e-01, -1.5433e+00,  3.1977e-01, -3.9646e-01, -1.5952e+00,\n",
      "          1.6791e+00,  3.6908e-01,  8.7384e-01, -3.0694e-01, -1.1812e+00,\n",
      "         -1.2251e-01, -1.1869e+00,  3.2829e-01,  4.2896e-02, -1.2292e-01,\n",
      "          8.5921e-01,  2.9424e-01, -2.6168e-01, -1.4456e+00, -8.5217e-01,\n",
      "          3.1064e-01, -8.1674e-01, -3.3779e-01,  1.3562e-02,  7.0531e-01,\n",
      "         -4.6174e-01, -3.1926e-01,  4.3621e-01,  2.4664e+00, -9.7497e-01,\n",
      "         -5.8212e-01, -1.2923e+00,  1.2124e+00, -9.5579e-01, -1.3313e+00,\n",
      "          5.1338e-01, -1.1909e-01, -1.5094e+00, -4.9136e-02,  6.0115e-01,\n",
      "          1.5212e-01,  1.5915e+00,  5.3420e-01,  1.4867e+00, -8.1367e-01,\n",
      "         -2.2086e+00,  1.0978e+00,  9.7582e-01,  9.5746e-01, -3.4647e-01,\n",
      "          1.0636e+00,  8.5086e-01,  9.1249e-01,  1.3486e-02,  7.6494e-01,\n",
      "          2.7826e-01, -1.3117e+00, -1.1850e+00, -7.6287e-01,  2.0890e+00,\n",
      "         -7.6211e-01, -1.5032e-01,  8.8495e-01,  3.2146e+00, -9.6405e-01,\n",
      "          1.3264e+00, -5.2182e-01, -1.5868e+00,  4.2517e-01, -4.6312e-01,\n",
      "          7.3274e-01,  9.5328e-01,  4.8787e-01,  1.2982e+00, -7.4635e-01,\n",
      "         -1.1491e+00, -2.1877e-01,  4.7651e-01, -6.8526e-01,  1.1171e+00,\n",
      "         -5.1380e-01, -2.0009e-01, -3.3558e-01, -7.3446e-01, -5.6561e-02,\n",
      "          1.5114e-01,  2.6642e-01, -1.1921e-01,  5.6877e-01, -1.1236e+00,\n",
      "         -1.8744e-01, -1.0477e+00, -2.2347e-01, -1.9968e+00, -9.0448e-01,\n",
      "         -1.3430e+00, -7.9197e-01,  1.7044e-01, -9.8440e-01,  3.3579e-01,\n",
      "         -1.2261e+00, -1.2851e+00, -1.3362e+00, -2.6172e-01, -8.7525e-02,\n",
      "          1.0567e+00, -1.0951e+00,  8.6397e-01,  1.5138e+00,  1.6630e+00,\n",
      "         -8.2521e-01,  1.9595e-01, -4.5739e-01,  4.3042e-01,  1.3511e-01,\n",
      "         -1.6587e+00,  1.1770e-01, -1.1267e+00, -1.5112e-02, -1.5540e+00,\n",
      "         -2.9234e-01, -5.2945e-01, -9.6744e-01, -9.9684e-01, -8.0472e-02,\n",
      "          9.3296e-01,  1.6343e+00,  1.0756e+00,  1.9797e-01, -9.1002e-01,\n",
      "          7.9318e-02,  4.8905e-01, -1.1636e+00,  4.5335e-01, -8.8946e-01,\n",
      "         -7.6437e-02, -8.6211e-01,  1.7543e-01, -9.5510e-01,  5.5807e-01,\n",
      "          2.2349e+00, -7.9594e-01,  2.1937e+00,  1.6980e-01,  7.2390e-02,\n",
      "         -9.5681e-01,  1.6173e-01, -1.3479e+00, -1.3565e+00, -1.8733e-01,\n",
      "          2.2229e-01,  8.1631e-01,  5.5297e-01, -1.2830e-01, -1.0588e+00,\n",
      "         -2.4376e-01, -9.8317e-01, -1.1492e+00, -8.5959e-01, -1.4900e-01,\n",
      "         -1.4203e+00, -2.0036e-01, -2.9372e-01, -1.0123e+00, -7.9548e-02,\n",
      "          1.9140e-01, -5.2810e-01,  6.4448e-01,  7.0169e-01, -4.0406e-01,\n",
      "          4.7229e-01, -1.4552e-02,  2.3451e-01, -9.2823e-01, -1.3281e+00,\n",
      "          4.3101e-01, -3.4133e-02, -1.2181e-01,  2.3465e-01,  1.0470e+00,\n",
      "         -8.4705e-02, -8.4394e-01,  2.7886e-01, -5.9490e-01,  1.8672e+00,\n",
      "         -2.9236e-01, -1.7815e-01,  3.4688e-01, -1.1162e+00, -1.5765e+00,\n",
      "         -2.7881e-01, -1.9864e-01, -4.7996e-01,  3.2387e+00,  8.6675e-01,\n",
      "         -3.6405e-01,  1.7848e+00, -7.9941e-01,  1.4276e+00, -2.6344e-01,\n",
      "          7.3207e-01,  9.5048e-01, -1.1909e+00, -8.7914e-01, -1.4694e+00,\n",
      "          1.9090e-02,  9.1131e-01, -7.8453e-01, -7.7758e-01, -4.7424e-01,\n",
      "         -7.9485e-01,  1.0117e+00, -3.8079e-01,  4.9776e-02, -9.9277e-01,\n",
      "         -6.6223e-01, -7.3386e-01,  3.8433e-01, -1.3326e+00, -2.7857e-01,\n",
      "          1.4104e+00, -1.0622e-01, -6.0195e-01, -6.6414e-01, -5.1655e-01,\n",
      "         -1.4596e+00,  3.0574e-01,  1.0635e-01,  6.6188e-01, -3.7399e-01,\n",
      "          2.3171e-01, -1.6362e-01, -1.5915e-01,  6.5601e-01, -3.0096e-01,\n",
      "          3.6014e-01,  1.0132e-02, -1.7602e+00, -1.1233e+00,  2.9094e-01,\n",
      "          6.4678e-01,  6.5095e-01,  1.6641e+00,  5.6075e-01, -9.2234e-01,\n",
      "         -8.1841e-01, -8.7035e-01, -2.1987e+00,  1.5523e+00,  1.8334e-01,\n",
      "         -1.1575e+00,  1.7952e+00,  5.7989e-01, -1.3260e-01, -1.0808e+00,\n",
      "         -3.2102e-01,  7.9634e-01, -3.4346e-01, -5.1344e-01, -3.3378e-01,\n",
      "          3.7498e-01,  1.2895e+00,  1.9635e+00, -5.3608e-01, -6.5520e-02,\n",
      "          6.7789e-01,  1.5129e+00,  4.7320e-01, -1.1524e+00,  1.0396e-01,\n",
      "          1.9702e+00, -9.6642e-01,  7.1119e-01,  2.2010e+00,  6.2660e-01,\n",
      "         -1.7298e+00,  4.4131e-01, -3.5877e-01, -6.0712e-01, -6.4237e-02,\n",
      "          8.4548e-01,  1.8275e+00, -1.4072e+00, -1.1475e+00,  3.1104e-01,\n",
      "          1.1953e+00, -1.0306e+00,  4.7247e-01, -1.0227e+00,  1.3973e-01,\n",
      "          1.0753e+00,  6.3380e-01, -8.7699e-01, -1.4396e+00,  8.0728e-02,\n",
      "         -2.2608e-01, -5.6059e-01, -2.0069e-01, -1.4176e-01, -6.5283e-01,\n",
      "         -1.0246e+00,  8.6652e-01,  1.3953e+00,  9.0308e-02,  8.6560e-01,\n",
      "          5.8060e-01,  1.0869e+00, -4.7630e-01, -5.7102e-01,  1.1435e+00,\n",
      "         -3.1980e-01, -1.2889e+00,  7.8631e-01,  9.0412e-01,  1.6195e+00,\n",
      "         -7.1393e-02,  5.6578e-01,  4.7397e-01, -7.2351e-01,  2.7689e+00,\n",
      "          5.0124e-02, -4.0766e-01, -7.8296e-01, -1.5654e-01,  7.7871e-01,\n",
      "          1.3250e+00,  3.5948e-01, -3.4375e-01,  6.6254e-01,  5.4087e-01,\n",
      "          1.2088e-01,  4.7516e+00, -4.6926e-01, -1.1203e-01, -1.2968e+00,\n",
      "         -2.2121e-01,  4.9013e-01, -8.2030e-01,  9.2321e-01,  9.9005e-01,\n",
      "         -1.2250e+00, -1.8690e+00, -6.3215e-01,  8.8317e-01, -2.0066e-01,\n",
      "         -4.8591e-01,  7.1139e-01,  4.3494e-01,  3.6220e-01, -1.3672e+00,\n",
      "         -7.6922e-01,  1.0936e+00, -3.8395e-01, -1.5654e+00,  3.1351e-01,\n",
      "         -1.6635e+00, -3.4938e-01,  8.8904e-01,  7.3835e-01, -1.2500e+00,\n",
      "          1.8097e+00,  6.5132e-01,  1.4735e-01, -1.9406e+00, -1.0911e-01,\n",
      "          7.5895e-01,  1.6709e+00, -1.1788e+00,  6.3947e-01,  1.3852e-01,\n",
      "          1.9954e-01,  8.3506e-01,  3.6592e+00, -1.7319e+00,  2.2398e-01,\n",
      "         -1.2286e+00, -8.4318e-01, -1.4345e+00, -2.2711e-01,  2.3769e-01,\n",
      "         -6.5230e-01,  1.0255e+00,  6.4993e-01, -1.4409e+00, -1.5560e+00,\n",
      "         -5.8699e-01, -1.1243e+00, -9.8011e-01, -5.6099e-01,  6.7463e-02,\n",
      "         -1.0302e+00, -1.6772e+00,  1.3201e+00, -8.3949e-01, -4.3104e-01,\n",
      "          6.8018e-01,  1.8402e-01, -1.2953e+00, -2.9050e-01,  5.6263e-01,\n",
      "          4.6707e-01,  1.0247e-01,  2.7413e+00, -2.4991e-01, -1.2945e+00,\n",
      "         -9.2269e-01,  4.2381e-01, -1.0441e+00, -1.4191e+00,  4.8048e-01,\n",
      "         -4.8463e-01,  3.6685e-01,  2.3069e-01, -1.0036e+00,  1.5075e+00,\n",
      "         -9.3454e-01,  7.3128e-01,  6.9216e-01,  1.1332e+00, -4.6331e-01,\n",
      "         -8.4433e-01, -3.1030e-01, -5.4902e-01, -1.3521e-01,  1.3560e+00,\n",
      "          2.0348e+00,  6.4542e-01, -2.5180e-01, -5.7418e-01,  4.1103e-02,\n",
      "          4.0846e-01,  8.7784e-01, -1.0926e+00, -1.9850e+00, -1.1234e+00,\n",
      "         -3.2890e-01, -4.9385e-01,  1.6051e-01,  1.4296e+00, -4.9707e-01,\n",
      "         -3.8131e-01,  2.2076e-01, -9.4260e-01,  3.0307e-02, -9.2011e-01,\n",
      "         -6.7295e-01,  1.5317e-01, -5.0714e-02, -1.7773e+00, -5.7981e-01,\n",
      "          3.8121e-02, -1.0735e+00,  2.7195e-01, -2.8222e-01, -4.1408e-01,\n",
      "          5.3566e-01, -2.3445e-01,  7.0405e-01, -7.9786e-01,  1.5383e-01,\n",
      "          6.2700e-01,  5.8202e-01,  5.5098e-01, -1.0439e+00, -1.4067e-01,\n",
      "         -7.3619e-02, -9.3323e-01, -4.7462e-01,  5.3140e-01, -2.0399e+00,\n",
      "         -8.5420e-01, -7.1446e-01, -1.7785e+00, -5.9674e-01,  5.4242e-01,\n",
      "         -1.0302e+00, -8.6745e-01, -8.5824e-01, -8.6707e-01,  3.1639e-01,\n",
      "         -9.2741e-01,  4.1148e-01,  3.7547e-01, -1.6502e+00, -7.8691e-01,\n",
      "         -3.8332e-01, -9.5842e-02, -2.0290e-01, -2.9643e-01,  9.7399e-01,\n",
      "         -6.1669e-01, -4.1794e-01, -3.9203e-01, -9.8125e-01, -7.6715e-01,\n",
      "          1.6333e-01, -5.0082e-01, -8.4434e-01, -3.0067e-01, -3.0393e-01,\n",
      "         -4.8686e-01,  9.9407e-02,  3.0677e-01, -1.3329e+00, -7.5470e-01,\n",
      "         -7.4137e-01, -1.7047e-01,  1.4523e+00, -1.2997e+00, -1.0646e+00,\n",
      "         -3.4563e-01, -8.6701e-01, -6.1391e-01, -5.7531e-01, -1.5184e+00,\n",
      "         -2.2454e+00, -9.8083e-02, -7.1285e-01,  1.3138e+00,  1.1889e+00]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "282\n"
     ]
    }
   ],
   "source": [
    "## use your fine-tuned model to predict\n",
    "## TODO: Load the image and the preprocessor\n",
    "from transformers import ViTImageProcessor\n",
    "\n",
    "img = Image.open(\"cat_dog.jpg\")\n",
    "processor = ViTImageProcessor.from_pretrained (\"vit-base-patch16-224-in21k-finetuned-cifar-10/checkpoint-450\") # e.g. vit-swin-test-cifar-10/checkpoint-4500\n",
    "inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print (outputs)\n",
    "predicted_class_idx = torch.argmax(outputs.logits[0]).item()\n",
    "print (predicted_class_idx)\n",
    "# print (id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example of fine-tuning of the Hugging Face model using the standard Pytorch loop\n",
    "### Task 3: create the validation set and change the training loop to compute the training and validation losses and accuracies after the epoch. Experiment with fine-tuning options of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-tiny-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNextForImageClassification(\n",
      "  (convnext): ConvNextModel(\n",
      "    (embeddings): ConvNextEmbeddings(\n",
      "      (patch_embeddings): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (layernorm): ConvNextLayerNorm()\n",
      "    )\n",
      "    (encoder): ConvNextEncoder(\n",
      "      (stages): ModuleList(\n",
      "        (0): ConvNextStage(\n",
      "          (downsampling_layer): Identity()\n",
      "          (layers): Sequential(\n",
      "            (0): ConvNextLayer(\n",
      "              (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNextLayer(\n",
      "              (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNextLayer(\n",
      "              (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ConvNextStage(\n",
      "          (downsampling_layer): Sequential(\n",
      "            (0): ConvNextLayerNorm()\n",
      "            (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (layers): Sequential(\n",
      "            (0): ConvNextLayer(\n",
      "              (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNextLayer(\n",
      "              (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNextLayer(\n",
      "              (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ConvNextStage(\n",
      "          (downsampling_layer): Sequential(\n",
      "            (0): ConvNextLayerNorm()\n",
      "            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (layers): Sequential(\n",
      "            (0): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (3): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (4): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (5): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (6): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (7): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (8): ConvNextLayer(\n",
      "              (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNextStage(\n",
      "          (downsampling_layer): Sequential(\n",
      "            (0): ConvNextLayerNorm()\n",
      "            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (layers): Sequential(\n",
      "            (0): ConvNextLayer(\n",
      "              (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNextLayer(\n",
      "              (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNextLayer(\n",
      "              (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (layernorm): ConvNextLayerNorm()\n",
      "              (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELUActivation()\n",
      "              (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
      ")\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     45\u001b[0m loss_in_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;66;03m# move batch to GPU\u001b[39;00m\n\u001b[1;32m     48\u001b[0m   batch \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     50\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/MasterStudium/PythonEnvrionment/lib/python3.12/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/MasterStudium/PythonEnvrionment/lib/python3.12/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\",\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "###########################################\n",
    "## TODO: Experiment with fine-tuning the model: \n",
    "## 1. Freeze all layers except the classifier\n",
    "## 2. Unfreeze the last few layers\n",
    "## 3. Unfreeze all layers\n",
    "###########################################\n",
    "# freeze all layers except the classifier\n",
    "print (model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=5e-5)\n",
    "\n",
    "# move model to GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "  #print(\"Epoch:\", epoch)\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  loss_in_epoch = 0\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "    # move batch to GPU\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "\n",
    "    loss, logits = outputs.loss, outputs.logits\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # metrics\n",
    "    total += batch[\"labels\"].shape[0]\n",
    "    predicted = logits.argmax(-1)\n",
    "    correct += (predicted == batch[\"labels\"]).sum().item()\n",
    "\n",
    "    accuracy = correct/total\n",
    "    loss_in_epoch += loss.item()\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "      print(f\"Loss after {idx} steps:\", loss.item())\n",
    "      print(f\"Accuracy after {idx} steps:\", accuracy)\n",
    "  print(f\"Loss in epoch {epoch}:\", loss_in_epoch/len(train_dataloader)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonaiLatestEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
