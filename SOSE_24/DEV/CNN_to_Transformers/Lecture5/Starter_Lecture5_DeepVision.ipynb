{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer, Swin Transformer, ConvNeXt (Basic usage from the Huggingface Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Install the required packages\n",
    "#!pip install transformers --upgrade\n",
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Inference on the pre-trained ViT (1000 ImageNet classes)\n",
    "- Load your own image\n",
    "- Run the model in the inference mode. Use different models, namely, ViT, Swin, Swinv2, ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "from transformers import ViTImageProcessor\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## Load the model with the given checkpoint\n",
    "## The checkpoint is the model name from the Hugging Face model hub\n",
    "## https://huggingface.co/models\n",
    "## TODO: Experiment with different models. To use Swin, you will need to export SwinForImageClassification and so on\n",
    "model_checkpoint = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_checkpoint)\n",
    "model.to(device)\n",
    "\n",
    "from PIL import Image\n",
    "## TODO: Load the image\n",
    "image = None\n",
    "\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(model_checkpoint)\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "print(pixel_values.shape)\n",
    "     \n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values)\n",
    "logits = outputs.logits\n",
    "logits.shape\n",
    "\n",
    "\n",
    "prediction = logits.argmax(-1)\n",
    "print(\"Predicted class:\", model.config.id2label[prediction.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Fine-tuning the models using HuggingFace library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "# load cifar10 (only small portion for demonstration purposes) \n",
    "train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])\n",
    "# split up training into training + validation\n",
    "splits = train_ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "\n",
    "id2label = {id:label for id, label in enumerate(train_ds.features['label'].names)}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the transforms\n",
    "\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "     \n",
    "\n",
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO: Experiment with different models. To use Swin, you will need to export SwinForImageClassification and set ignore_mismatched_sizes=True to adjust the last layer\n",
    "## Use Huggingface hub to find the model names: https://huggingface.co/models\n",
    "## Most likely you will have to use the tiny versions only and relatively small batch sizes\n",
    "## You can also use AutoModelForImageClassification to automatically load the correct model\n",
    "from transformers import ViTForImageClassification, SwinForImageClassification, Swinv2ForImageClassification, ConvNextForImageClassification\n",
    "\n",
    "model_checkpoint_swin = \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "model_checkpoint_vit = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "model_vit = ViTForImageClassification.from_pretrained(model_checkpoint_vit,\n",
    "                                                  id2label=id2label,\n",
    "                                                  label2id=label2id)\n",
    "\n",
    "model_swin = SwinForImageClassification.from_pretrained(model_checkpoint_swin,id2label=id2label,label2id=label2id, ignore_mismatched_sizes=True)\n",
    "\n",
    "model_swinv2 = Swinv2ForImageClassification.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\",id2label=id2label,label2id=label2id, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "model_checkpoint = model_checkpoint_vit\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-cifar-10\", #output directory\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,                 # for demonstration purposes, adjust as needed\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    "    report_to='tensorboard',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "     \n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_vit,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "train_results = trainer.train()\n",
    "\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "metrics = trainer.evaluate(val_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use your fine-tuned model to predict\n",
    "## TODO: Load the image and the preprocessor\n",
    "from transformers import ViTImageProcessor\n",
    "\n",
    "img = Image.open(\"path/to/your/image.jpg\")\n",
    "processor = ViTImageProcessor.from_pretrained (\"path/to/your/Preprocessor.json\") # e.g. vit-swin-test-cifar-10/checkpoint-4500\n",
    "inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print (outputs)\n",
    "predicted_class_idx = torch.argmax(outputs.logits[0]).item()\n",
    "print (predicted_class_idx)\n",
    "print (id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example of fine-tuning of the Hugging Face model using the standard Pytorch loop\n",
    "### Task 3: create the validation set and change the training loop to compute the training and validation losses and accuracies after the epoch. Experiment with fine-tuning options of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\",\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "###########################################\n",
    "## TODO: Experiment with fine-tuning the model: \n",
    "## 1. Freeze all layers except the classifier\n",
    "## 2. Unfreeze the last few layers\n",
    "## 3. Unfreeze all layers\n",
    "###########################################\n",
    "# freeze all layers except the classifier\n",
    "print (model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=5e-5)\n",
    "\n",
    "# move model to GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "  #print(\"Epoch:\", epoch)\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  loss_in_epoch = 0\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "    # move batch to GPU\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "\n",
    "    loss, logits = outputs.loss, outputs.logits\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # metrics\n",
    "    total += batch[\"labels\"].shape[0]\n",
    "    predicted = logits.argmax(-1)\n",
    "    correct += (predicted == batch[\"labels\"]).sum().item()\n",
    "\n",
    "    accuracy = correct/total\n",
    "    loss_in_epoch += loss.item()\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "      print(f\"Loss after {idx} steps:\", loss.item())\n",
    "      print(f\"Accuracy after {idx} steps:\", accuracy)\n",
    "  print(f\"Loss in epoch {epoch}:\", loss_in_epoch/len(train_dataloader)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonaiLatestEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
