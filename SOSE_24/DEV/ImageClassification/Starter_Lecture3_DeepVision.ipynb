{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n",
    "\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This line detects if we have a gpu support on our system\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-class classification\n",
    "criterion_multi_class = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your train val test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import  SubsetRandomSampler  #for validation test\n",
    "\n",
    "#Define a transform to convert to images to tensor and normalize\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,),)]) #mean and std have to be sequences (e.g., tuples), \n",
    "                                                                      # therefore we should add a comma after the values\n",
    "\n",
    "#transform for train also includes augmentation now!\n",
    "#,transforms.RandomRotation(degrees=(0, 180)),\n",
    "transform_train = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,),)])                      \n",
    "#Load the data: train and test sets\n",
    "trainset = datasets.CIFAR10('~/Datasets/CF_data', download=True, train=True, transform=transform_train)\n",
    "validset = datasets.CIFAR10('~/Datasets/CF_data', download=False, train=True, transform=transform)\n",
    "testset = datasets.CIFAR10('~/Datasets/CF_data', download=True, train=False, transform=transform)\n",
    "\n",
    "#Preparing the validation test\n",
    "indices = list(range(len(trainset)))\n",
    "np.random.shuffle(indices)\n",
    "#to get 20% of the train set\n",
    "split = int(np.floor(0.2 * len(trainset)))\n",
    "print(len(indices[split:]))\n",
    "train_sample = SubsetRandomSampler(indices[split:])\n",
    "valid_sample = SubsetRandomSampler(indices[:split])\n",
    "\n",
    "#Data Loader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sample, batch_size=16)\n",
    "validloader = torch.utils.data.DataLoader(validset, sampler=valid_sample, batch_size=16)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #  defining accuracy function\n",
    "def accuracy(network, dataloader):\n",
    "      network.eval()\n",
    "      total_correct = 0\n",
    "      total_instances = 0\n",
    "      for images, labels in tqdm(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = torch.argmax(network(images), dim=1)\n",
    "        correct_predictions = sum(predictions==labels).item()\n",
    "        total_correct+=correct_predictions\n",
    "        total_instances+=len(images)\n",
    "      return round(total_correct/total_instances, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "cnn_model= SimpleCNN()\n",
    "cnn_model.to(device)\n",
    "summary(cnn_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "* Experiment with different Optimizers and learning rate schedulers\n",
    "* Plot the learning curves and analyze your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model(model, optimizer, scheduler, epochs):\n",
    "    writer = SummaryWriter()\n",
    "    model.to(device)\n",
    "\n",
    "    print(model)  \n",
    "    \n",
    "    ####\n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "            'training_loss_per_batch': [],\n",
    "            'validation_loss_per_batch': [],\n",
    "            'training_accuracy_per_epoch': [],\n",
    "            'validation_accuracy_per_epoch': []\n",
    "        }     \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "          print(f'Epoch {epoch+1}/{epochs}')\n",
    "          train_losses = []\n",
    "    \n",
    "          #  training\n",
    "          print('training...')\n",
    "          model.train()\n",
    "          for images, labels in tqdm(trainloader):\n",
    "            #  sending data to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            #  resetting gradients\n",
    "            optimizer.zero_grad()\n",
    "            #  making predictions\n",
    "            predictions = model(images)\n",
    "            #  computing loss\n",
    "            loss = criterion_multi_class(predictions, labels)\n",
    "            log_dict['training_loss_per_batch'].append(loss.item())\n",
    "            train_losses.append(loss.item())\n",
    "            #  computing gradients\n",
    "            loss.backward()\n",
    "            #  updating weights\n",
    "            optimizer.step()\n",
    "          with torch.no_grad():\n",
    "            print('deriving training accuracy...')\n",
    "            #  computing training accuracy\n",
    "            train_accuracy = accuracy(model, trainloader)\n",
    "            log_dict['training_accuracy_per_epoch'].append(train_accuracy)\n",
    "    \n",
    "          #  validation\n",
    "          print('validating...')\n",
    "          val_losses = []\n",
    "    \n",
    "          #  setting convnet to evaluation mode\n",
    "          model.eval()\n",
    "    \n",
    "          with torch.no_grad():\n",
    "            for images, labels in tqdm(validloader):\n",
    "              #  sending data to device\n",
    "              images, labels = images.to(device), labels.to(device)\n",
    "              #  making predictions\n",
    "              predictions = model(images)\n",
    "              #  computing loss\n",
    "              val_loss = criterion_multi_class(predictions, labels)\n",
    "              log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "              val_losses.append(val_loss.item())\n",
    "            #  computing accuracy\n",
    "            print('deriving validation accuracy...')\n",
    "            val_accuracy = accuracy(model, validloader)\n",
    "            log_dict['validation_accuracy_per_epoch'].append(val_accuracy)\n",
    "    \n",
    "          train_losses = np.array(train_losses).mean()\n",
    "          writer.add_scalar(\"Loss/train\", train_losses, epoch)\n",
    "          val_losses = np.array(val_losses).mean()\n",
    "    \n",
    "          print(f'training_loss: {round(train_losses, 4)}  training_accuracy: '+\n",
    "          f'{train_accuracy}  validation_loss: {round(val_losses, 4)} '+  \n",
    "          f'validation_accuracy: {val_accuracy}\\n')\n",
    "          \n",
    "          #Update scheduler\n",
    "          scheduler.step()\n",
    "    \n",
    "          \n",
    "    ####\n",
    "    #  saving model\n",
    "    # Get the current datetime\n",
    "    current_datetime = datetime.now()\n",
    "    \n",
    "    # Convert the datetime to a string\n",
    "    datetime_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    print(datetime_string)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'models/model_Shallow_FMNIST'+datetime_string+'.pth')\n",
    "    print('model saved')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "learning_rate = 0.01\n",
    "model = SimpleCNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "train_model(model, optimizer, scheduler, 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Batch Normalization\n",
    "* Add Batch Normalization layers to the network architecture\n",
    "* Train the updated network, try to increase the learning rate\n",
    "* Analyze the convergence and the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# TODO: Insert your code below\n",
    "# Insert BatchNorm2d layers in the CNN model\n",
    "###########################################\n",
    "class SimpleCNNwBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNwBN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "cnn_model1= SimpleCNNwBN()\n",
    "cnn_model1.to(device)\n",
    "summary(cnn_model1, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "learning_rate = 0.01\n",
    "model = SimpleCNNwBN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "train_model(model, optimizer, scheduler,20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Complexity of the model\n",
    "* Simplify your model and observe the underfitting behavior\n",
    "* Check, if your original model experiences the overfitting behavior. \n",
    "* Experiment with different approaches (Regularization, Dropout, Augmentation) to prevent overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimplerCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplerCNNModel, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64 * 16 * 16, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),  # Dropout layer with 50% dropout rate\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "learning_rate = 0.01\n",
    "model = SimplerCNNModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "train_model(model, optimizer, scheduler,20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
