{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (1.5.1)\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n",
    "\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This line detects if we have a gpu support on our system\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 100]               0\n",
      "            Linear-2                   [-1, 64]           6,464\n",
      "              ReLU-3                   [-1, 64]               0\n",
      "            Linear-4                   [-1, 32]           2,080\n",
      "              ReLU-5                   [-1, 32]               0\n",
      "            Linear-6                   [-1, 10]             330\n",
      "================================================================\n",
      "Total params: 8,874\n",
      "Trainable params: 8,874\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ShallowNN4Images(nn.Module):\n",
    "    def __init__(self, input_num, hidden_num, output_num):\n",
    "        super(ShallowNN4Images, self).__init__()\n",
    "        # we need to make a vector of input neurons from the image pixels\n",
    "        self.flatten = nn.Flatten()\n",
    "        hidden_num2 = (int)(hidden_num/2)\n",
    "        self.hidden1 = nn.Linear(input_num, hidden_num) # hidden layer\n",
    "        self.hidden2 = nn.Linear(hidden_num, hidden_num2) # hidden layer\n",
    "        self.output = nn.Linear(hidden_num2, output_num) # output layer\n",
    "        #self.sigmoid = nn.Sigmoid() # sigmoid activation function\n",
    "        self.relu = nn.ReLU() # relu activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # first hidden layer\n",
    "        x = self.hidden1(x)\n",
    "        #activation function\n",
    "        x = self.relu(x) \n",
    "        #second hidden layer\n",
    "        x = self.hidden2(x)\n",
    "        #activation function\n",
    "        x = self.relu(x) \n",
    "        #output layer\n",
    "        out = self.output(x)\n",
    "        return out\n",
    "\n",
    "input_num = 100 # 10x10 image\n",
    "hidden_num = 64\n",
    "output_num = 10 # The output should be the same as the number of classes\n",
    "\n",
    "model = ShallowNN4Images(input_num, hidden_num, output_num)\n",
    "model.to(device) # send our model to gpu if available else cpu. \n",
    "#print(model)\n",
    "print (summary(model, (1, 10, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Important Note: Pytorch does not need the network to have the last layer \n",
    " - as a sigmoid layer for BCEWithLogitsLoss (it's computed inside the loss function)\n",
    " - as a softmax layer for CrossEntropyLoss (it's computed inside the loss function) \n",
    "'''\n",
    "\n",
    "#binary classification\n",
    "criterion_binary_case = torch.nn.BCEWithLogitsLoss()\n",
    "#multi-class classification\n",
    "criterion_multi_class = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your train val test dataset\n",
    "#### Option 1: dataset is a part of pytorch\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import  SubsetRandomSampler  #for validation test\n",
    "\n",
    "#Define a transform to convert to images to tensor and normalize\n",
    "transform = transforms.Compose([transforms.Resize(size=(10,10)),transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,),)]) #mean and std have to be sequences (e.g., tuples), \n",
    "                                                                      # therefore we should add a comma after the values\n",
    "\n",
    "#transform for train also includes augmentation now!\n",
    "#,transforms.RandomRotation(degrees=(0, 180)),\n",
    "transform_train = transforms.Compose([transforms.Resize(size=(10,10)),transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,),)])                      \n",
    "#Load the data: train and test sets\n",
    "trainset = datasets.FashionMNIST('~/F_MNIST_data', download=True, train=True, transform=transform_train)\n",
    "validset = datasets.FashionMNIST('~/F_MNIST_data1', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('~/F_MNIST_data', download=True, train=False, transform=transform)\n",
    "\n",
    "#Preparing the validation test\n",
    "indices = list(range(len(trainset)))\n",
    "np.random.shuffle(indices)\n",
    "#to get 20% of the train set\n",
    "split = int(np.floor(0.2 * len(trainset)))\n",
    "print(len(indices[split:]))\n",
    "train_sample = SubsetRandomSampler(indices[split:])\n",
    "valid_sample = SubsetRandomSampler(indices[:split])\n",
    "\n",
    "#Data Loader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sample, batch_size=16)\n",
    "validloader = torch.utils.data.DataLoader(validset, sampler=valid_sample, batch_size=16)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7f77234fc5e0>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "print(dataiter)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "for idx in np.arange(16):\n",
    "  # xticks=[], yticks=[] is empty to print the images without any ticks around them\n",
    "  #np.sqeeze : Remove single-dimensional entries from the shape of an array.\n",
    "  ax = fig.add_subplot(4, 5, idx+1, xticks=[], yticks=[])\n",
    "  ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "   # .item() gets the value contained in a Tensor\n",
    "  ax.set_title(labels[idx].item())\n",
    "  fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: A CustomDatasetClass is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "#fashion mnist dataset is also available in csv format\n",
    "import pandas as pd\n",
    "train_csv = pd.read_csv(\"~/F_MNIST_data/FashionMNIST/csv/fashion-mnist_train.csv\")\n",
    "test_csv = pd.read_csv(\"~/F_MNIST_data/FashionMNIST/csv/fashion-mnist_test.csv\")\n",
    "\n",
    "print (len(train_csv), len(test_csv))\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n",
    "    def __init__(self, data, transform = None):\n",
    "        \"\"\"Method to initilaize variables.\"\"\" \n",
    "        self.fashion_MNIST = list(data.values)\n",
    "        self.transform = transform\n",
    "        label = []\n",
    "        image = []\n",
    "        for i in self.fashion_MNIST:\n",
    "             # first column is of labels.\n",
    "            label.append(i[0])\n",
    "            image.append(i[1:])\n",
    "        self.labels = np.asarray(label)\n",
    "        # Dimension of Images = 28 * 28 * 1. where height = width = 28 and color_channels = 1.\n",
    "        self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = self.images[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "train_set = FashionDataset(train_csv, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,),)]))\n",
    "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,),)]))\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_set, (50000, 10000))\n",
    "\n",
    "print (len(train_dataset), len(valid_dataset), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " #  defining accuracy function\n",
    "def accuracy(network, dataloader):\n",
    "      network.eval()\n",
    "      total_correct = 0\n",
    "      total_instances = 0\n",
    "      for images, labels in tqdm(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = torch.argmax(network(images), dim=1)\n",
    "        correct_predictions = sum(predictions==labels).item()\n",
    "        total_correct+=correct_predictions\n",
    "        total_instances+=len(images)\n",
    "      return round(total_correct/total_instances, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "ShallowNN4Images(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (hidden1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (hidden2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (output): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Epoch 1/1\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:19<00:00, 157.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriving training accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:15<00:00, 195.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 202.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriving validation accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 187.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.5686  training_accuracy: 0.797  validation_loss: 0.5759 validation_accuracy: 0.794\n",
      "\n",
      "2024-04-01_16-56-19\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "print(model)\n",
    "\n",
    "start_epoch = 0\n",
    "epochs = 1\n",
    "\n",
    "####\n",
    "#  creating log\n",
    "log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'training_accuracy_per_epoch': [],\n",
    "        'validation_accuracy_per_epoch': []\n",
    "    } \n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #  training\n",
    "      print('training...')\n",
    "      model.train()\n",
    "      for images, labels in tqdm(trainloader):\n",
    "        #  sending data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #  resetting gradients\n",
    "        optimizer.zero_grad()\n",
    "        #  making predictions\n",
    "        predictions = model(images)\n",
    "        #  computing loss\n",
    "        loss = criterion_multi_class(predictions, labels)\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "        train_losses.append(loss.item())\n",
    "        #  computing gradients\n",
    "        loss.backward()\n",
    "        #  updating weights\n",
    "        optimizer.step()\n",
    "      with torch.no_grad():\n",
    "        print('deriving training accuracy...')\n",
    "        #  computing training accuracy\n",
    "        train_accuracy = accuracy(model, trainloader)\n",
    "        log_dict['training_accuracy_per_epoch'].append(train_accuracy)\n",
    "\n",
    "      #  validation\n",
    "      print('validating...')\n",
    "      val_losses = []\n",
    "\n",
    "      #  setting convnet to evaluation mode\n",
    "      model.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for images, labels in tqdm(validloader):\n",
    "          #  sending data to device\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          #  making predictions\n",
    "          predictions = model(images)\n",
    "          #  computing loss\n",
    "          val_loss = criterion_multi_class(predictions, labels)\n",
    "          log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "          val_losses.append(val_loss.item())\n",
    "        #  computing accuracy\n",
    "        print('deriving validation accuracy...')\n",
    "        val_accuracy = accuracy(model, validloader)\n",
    "        log_dict['validation_accuracy_per_epoch'].append(val_accuracy)\n",
    "\n",
    "      train_losses = np.array(train_losses).mean()\n",
    "      writer.add_scalar(\"Loss/train\", train_losses, epoch)\n",
    "      val_losses = np.array(val_losses).mean()\n",
    "\n",
    "      print(f'training_loss: {round(train_losses, 4)}  training_accuracy: '+\n",
    "      f'{train_accuracy}  validation_loss: {round(val_losses, 4)} '+  \n",
    "      f'validation_accuracy: {val_accuracy}\\n')\n",
    "      \n",
    "####\n",
    "#  saving model\n",
    "# Get the current datetime\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Convert the datetime to a string\n",
    "datetime_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "print(datetime_string)\n",
    "\n",
    "torch.save(model.state_dict(), 'model_Shallow_FMNIST'+datetime_string+'.pth')\n",
    "print('model saved')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "              ReLU-2           [-1, 32, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          18,496\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-15            [-1, 256, 4, 4]               0\n",
      "           Linear-16                 [-1, 1024]       4,195,328\n",
      "             ReLU-17                 [-1, 1024]               0\n",
      "           Linear-18                  [-1, 512]         524,800\n",
      "             ReLU-19                  [-1, 512]               0\n",
      "           Linear-20                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 5,851,338\n",
      "Trainable params: 5,851,338\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.24\n",
      "Params size (MB): 22.32\n",
      "Estimated Total Size (MB): 25.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "cnn_model= SimpleCNN()\n",
    "cnn_model.to(device)\n",
    "summary(cnn_model, (3, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
