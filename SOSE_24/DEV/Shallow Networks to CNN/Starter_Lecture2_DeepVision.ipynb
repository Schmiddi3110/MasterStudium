{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:51:53.242156Z",
     "start_time": "2024-04-03T08:51:47.515555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: torchsummary in /home/fabian/.local/lib/python3.10/site-packages (1.5.1)\r\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n",
    "\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This line detects if we have a gpu support on our system\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:52:03.275121Z",
     "start_time": "2024-04-03T08:52:03.170231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 100]               0\n",
      "            Linear-2                   [-1, 64]           6,464\n",
      "              ReLU-3                   [-1, 64]               0\n",
      "            Linear-4                   [-1, 32]           2,080\n",
      "              ReLU-5                   [-1, 32]               0\n",
      "            Linear-6                   [-1, 10]             330\n",
      "================================================================\n",
      "Total params: 8,874\n",
      "Trainable params: 8,874\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ShallowNN4Images(nn.Module):\n",
    "    def __init__(self, input_num, hidden_num, output_num):\n",
    "        super(ShallowNN4Images, self).__init__()\n",
    "        # we need to make a vector of input neurons from the image pixels\n",
    "        self.flatten = nn.Flatten()\n",
    "        hidden_num2 = (int)(hidden_num/2)\n",
    "        self.hidden1 = nn.Linear(input_num, hidden_num) # hidden layer\n",
    "        self.hidden2 = nn.Linear(hidden_num, hidden_num2) # hidden layer\n",
    "        self.output = nn.Linear(hidden_num2, output_num) # output layer\n",
    "        #self.sigmoid = nn.Sigmoid() # sigmoid activation function\n",
    "        self.relu = nn.ReLU() # relu activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # first hidden layer\n",
    "        x = self.hidden1(x)\n",
    "        #activation function\n",
    "        x = self.relu(x) \n",
    "        #second hidden layer\n",
    "        x = self.hidden2(x)\n",
    "        #activation function\n",
    "        x = self.relu(x) \n",
    "        #output layer\n",
    "        out = self.output(x)\n",
    "        return out\n",
    "\n",
    "input_num = 100 # 10x10 image\n",
    "hidden_num = 64\n",
    "output_num = 10 # The output should be the same as the number of classes\n",
    "\n",
    "model = ShallowNN4Images(input_num, hidden_num, output_num)\n",
    "model.to(device) # send our model to gpu if available else cpu. \n",
    "#print(model)\n",
    "print (summary(model, (1, 10, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:52:28.607854Z",
     "start_time": "2024-04-03T08:52:28.602090Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Important Note: Pytorch does not need the network to have the last layer \n",
    " - as a sigmoid layer for BCEWithLogitsLoss (it's computed inside the loss function)\n",
    " - as a softmax layer for CrossEntropyLoss (it's computed inside the loss function) \n",
    "'''\n",
    "\n",
    "#binary classification\n",
    "criterion_binary_case = torch.nn.BCEWithLogitsLoss()\n",
    "#multi-class classification\n",
    "criterion_multi_class = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your train val test dataset\n",
    "### Option 1: dataset is a part of pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:00:33.648201Z",
     "start_time": "2024-04-03T08:59:58.419201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:11<00:00, 2295156.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 479817.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:02<00:00, 1536381.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 10790743.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:14<00:00, 1848920.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data1/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 921497.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data1/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:02<00:00, 1774244.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data1/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 12157813.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/fabian/F_MNIST_data1/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/fabian/F_MNIST_data1/FashionMNIST/raw\n",
      "\n",
      "48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import  SubsetRandomSampler  #for validation test\n",
    "\n",
    "#Define a transform to convert to images to tensor and normalize\n",
    "transform = transforms.Compose([transforms.Resize(size=(10,10)),transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,),)]) #mean and std have to be sequences (e.g., tuples), \n",
    "                                                                      # therefore we should add a comma after the values\n",
    "\n",
    "#transform for train also includes augmentation now!\n",
    "#,transforms.RandomRotation(degrees=(0, 180)),\n",
    "transform_train = transforms.Compose([transforms.Resize(size=(10,10)),transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,),)])                      \n",
    "#Load the data: train and test sets\n",
    "trainset = datasets.FashionMNIST('~/F_MNIST_data', download=True, train=True, transform=transform_train)\n",
    "validset = datasets.FashionMNIST('~/F_MNIST_data1', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('~/F_MNIST_data', download=True, train=False, transform=transform)\n",
    "\n",
    "#Preparing the validation test\n",
    "indices = list(range(len(trainset)))\n",
    "np.random.shuffle(indices)\n",
    "#to get 20% of the train set\n",
    "split = int(np.floor(0.2 * len(trainset)))\n",
    "print(len(indices[split:]))\n",
    "train_sample = SubsetRandomSampler(indices[split:])\n",
    "valid_sample = SubsetRandomSampler(indices[:split])\n",
    "\n",
    "#Data Loader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sample, batch_size=16)\n",
    "validloader = torch.utils.data.DataLoader(validset, sampler=valid_sample, batch_size=16)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:00:43.608357Z",
     "start_time": "2024-04-03T09:00:41.476274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7e7a056244c0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13113/507692184.py:17: UserWarning: The figure layout has changed to tight\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x500 with 16 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "print(dataiter)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "for idx in np.arange(16):\n",
    "  # xticks=[], yticks=[] is empty to print the images without any ticks around them\n",
    "  #np.sqeeze : Remove single-dimensional entries from the shape of an array.\n",
    "  ax = fig.add_subplot(4, 5, idx+1, xticks=[], yticks=[])\n",
    "  ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "   # .item() gets the value contained in a Tensor\n",
    "  ax.set_title(labels[idx].item())\n",
    "  fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: A CustomDatasetClass is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:00:47.072118Z",
     "start_time": "2024-04-03T09:00:46.968165Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/fabian/F_MNIST_data/FashionMNIST/csv/fashion-mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#fashion mnist dataset is also available in csv format\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m train_csv \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m~/F_MNIST_data/FashionMNIST/csv/fashion-mnist_train.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m test_csv \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m~/F_MNIST_data/FashionMNIST/csv/fashion-mnist_test.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m (\u001B[38;5;28mlen\u001B[39m(train_csv), \u001B[38;5;28mlen\u001B[39m(test_csv))\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/fabian/F_MNIST_data/FashionMNIST/csv/fashion-mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "#fashion mnist dataset is also available in csv format\n",
    "import pandas as pd\n",
    "train_csv = pd.read_csv(\"~/F_MNIST_data/FashionMNIST/csv/fashion-mnist_train.csv\")\n",
    "test_csv = pd.read_csv(\"~/F_MNIST_data/FashionMNIST/csv/fashion-mnist_test.csv\")\n",
    "\n",
    "print (len(train_csv), len(test_csv))\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n",
    "    def __init__(self, data, transform = None):\n",
    "        \"\"\"Method to initilaize variables.\"\"\" \n",
    "        self.fashion_MNIST = list(data.values)\n",
    "        self.transform = transform\n",
    "        label = []\n",
    "        image = []\n",
    "        for i in self.fashion_MNIST:\n",
    "             # first column is of labels.\n",
    "            label.append(i[0])\n",
    "            image.append(i[1:])\n",
    "        self.labels = np.asarray(label)\n",
    "        # Dimension of Images = 28 * 28 * 1. where height = width = 28 and color_channels = 1.\n",
    "        self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = self.images[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "train_set = FashionDataset(train_csv, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,),)]))\n",
    "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,),)]))\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_set, (50000, 10000))\n",
    "\n",
    "print (len(train_dataset), len(valid_dataset), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " #  defining accuracy function\n",
    "def accuracy(network, dataloader):\n",
    "      network.eval()\n",
    "      total_correct = 0\n",
    "      total_instances = 0\n",
    "      for images, labels in tqdm(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = torch.argmax(network(images), dim=1)\n",
    "        correct_predictions = sum(predictions==labels).item()\n",
    "        total_correct+=correct_predictions\n",
    "        total_instances+=len(images)\n",
    "      return round(total_correct/total_instances, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/tiva/anaconda3/envs/pyTest2/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "ShallowNN4Images(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (hidden1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (hidden2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (output): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Epoch 1/1\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:19<00:00, 157.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriving training accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:15<00:00, 195.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 202.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriving validation accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:03<00:00, 187.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.5686  training_accuracy: 0.797  validation_loss: 0.5759 validation_accuracy: 0.794\n",
      "\n",
      "2024-04-01_16-56-19\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "print(model)\n",
    "\n",
    "start_epoch = 0\n",
    "epochs = 1\n",
    "\n",
    "####\n",
    "#  creating log\n",
    "log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'training_accuracy_per_epoch': [],\n",
    "        'validation_accuracy_per_epoch': []\n",
    "    } \n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #  training\n",
    "      print('training...')\n",
    "      model.train()\n",
    "      for images, labels in tqdm(trainloader):\n",
    "        #  sending data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #  resetting gradients\n",
    "        optimizer.zero_grad()\n",
    "        #  making predictions\n",
    "        predictions = model(images)\n",
    "        #  computing loss\n",
    "        loss = criterion_multi_class(predictions, labels)\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "        train_losses.append(loss.item())\n",
    "        #  computing gradients\n",
    "        loss.backward()\n",
    "        #  updating weights\n",
    "        optimizer.step()\n",
    "      with torch.no_grad():\n",
    "        print('deriving training accuracy...')\n",
    "        #  computing training accuracy\n",
    "        train_accuracy = accuracy(model, trainloader)\n",
    "        log_dict['training_accuracy_per_epoch'].append(train_accuracy)\n",
    "\n",
    "      #  validation\n",
    "      print('validating...')\n",
    "      val_losses = []\n",
    "\n",
    "      #  setting convnet to evaluation mode\n",
    "      model.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for images, labels in tqdm(validloader):\n",
    "          #  sending data to device\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          #  making predictions\n",
    "          predictions = model(images)\n",
    "          #  computing loss\n",
    "          val_loss = criterion_multi_class(predictions, labels)\n",
    "          log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "          val_losses.append(val_loss.item())\n",
    "        #  computing accuracy\n",
    "        print('deriving validation accuracy...')\n",
    "        val_accuracy = accuracy(model, validloader)\n",
    "        log_dict['validation_accuracy_per_epoch'].append(val_accuracy)\n",
    "\n",
    "      train_losses = np.array(train_losses).mean()\n",
    "      writer.add_scalar(\"Loss/train\", train_losses, epoch)\n",
    "      val_losses = np.array(val_losses).mean()\n",
    "\n",
    "      print(f'training_loss: {round(train_losses, 4)}  training_accuracy: '+\n",
    "      f'{train_accuracy}  validation_loss: {round(val_losses, 4)} '+  \n",
    "      f'validation_accuracy: {val_accuracy}\\n')\n",
    "      \n",
    "####\n",
    "#  saving model\n",
    "# Get the current datetime\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Convert the datetime to a string\n",
    "datetime_string = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "print(datetime_string)\n",
    "\n",
    "torch.save(model.state_dict(), 'model_Shallow_FMNIST'+datetime_string+'.pth')\n",
    "print('model saved')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "              ReLU-2           [-1, 32, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          18,496\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-5           [-1, 64, 16, 16]               0\n",
      "            Conv2d-6          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-10            [-1, 128, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]         295,168\n",
      "             ReLU-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 256, 8, 8]         590,080\n",
      "             ReLU-14            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-15            [-1, 256, 4, 4]               0\n",
      "           Linear-16                 [-1, 1024]       4,195,328\n",
      "             ReLU-17                 [-1, 1024]               0\n",
      "           Linear-18                  [-1, 512]         524,800\n",
      "             ReLU-19                  [-1, 512]               0\n",
      "           Linear-20                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 5,851,338\n",
      "Trainable params: 5,851,338\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.24\n",
      "Params size (MB): 22.32\n",
      "Estimated Total Size (MB): 25.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "cnn_model= SimpleCNN()\n",
    "cnn_model.to(device)\n",
    "summary(cnn_model, (3, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
