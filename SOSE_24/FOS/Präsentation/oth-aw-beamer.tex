\documentclass[aspectratio=169, lecture, amberg]{OTHAWbeamer}
\let\Tiny=\tiny  
\usepackage{ngerman}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{etex}
\usepackage{biblatex}
\usepackage{csquotes}
\usepackage{comment}
\usepackage{amsmath}
\addbibresource{references.bib}

\title[Forschungsseminar]{One-Step Image Translation with Text-to-Image Models}
\subtitle{Forschungsseminar}
\author[Schmidt]{Fabian Schmidt}
\place{OTH Amberg-Weiden}
\date{\today}


\email{f.schmidt3@oth-aw.de}

\begin{document}
\maketitle

% ---------- Begin Präsentation ----------
\frame{
\frametitle{Table of Contents}
\begin{enumerate}
    \item Introduction
    \item Related Work
    \item Terminology
    \item Method
    \item Experiments - Paired Image Translation
    \item Discussion and Limitations
    \item Live Demo
\end{enumerate}
\tableofcontents
}
% ---------- Begin Section Introduction ----------
% ---------- Problems with Diffusion Models ----------
\begin{frame}
\frametitle{Introduction}
\framesubtitle{Problems with Diffusion Models}

\begin{columns}
    \column{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{images/GANs_Diffusion_Autoencoders.png}

    \column{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/Generation-with-Diffusion-Models-ezgif.com-webp-to-jpg-converter.jpg}
  \end{columns}  
  \tiny{\footnotemark \url{https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/}}
  \tiny{\footnotemark \url{https://miro.medium.com/v2/resize:fit:720/format:webp/1*RDPhd2dvmHE4UrAP-QHb9w.png}}
\end{frame}

% ---------- Proposed solutions ----------
\begin{frame}
\frametitle{Introduction}
\framesubtitle{Proposed solutions}
\begin{itemize}
    \item One-step image-to-image translation method for paired and unpaired settings
    \item Reduce number of inference steps to 1
    \item Trainable without image pairs
    \item Adapt pre-trained text-conditional one-step diffusion model to new domains via adversarial learning
\end{itemize}
\end{frame}

% ---------- Begin Section Related Work ----------
% ---------- Image-to-Image translation ----------
\begin{frame}
\frametitle{Related Work}
\framesubtitle{Image-to-Image translation}
Paired Image Translation
\begin{itemize}
    \item e.g. GLIGEN \cite{li2023gligen}, T2I-Adapter \cite{mou2023t2i}, ControlNet \cite{zhang2023adding}
    \item requires large number of training pairs
    \item slow inference
\end{itemize}
Unpaired Image Translation
\begin{itemize}
    \item GAN- or diffusion-based methods \cite{cyclediffusion} \cite{su2022dual} \cite{sasaki2021unitddpm}
    \item require training from scratch on new domains   
\end{itemize}
\end{frame}

% ---------- Text-to-Image models ----------
\begin{frame}
\frametitle{Related Work}
\framesubtitle{Text-to-Image models}
\begin{itemize}
    \item Large-scale text-conditioned models have enhanced image quality and diversity by training on vast datasets \cite{schuhmann2022laion5b} \cite{kakaobrain2022coyo-700m}
    \item Zero-shot methods for editing real images use pre-trained text-to-image models, such as SDEdit \cite{meng2022sdedit}
    \item Prompt-to-Prompt techniques manipulate or preserve features in cross-attention and self-attention layers during image editing.
    \item Some approaches fine-tune networks or text embeddings for input images before editing or employ precise inversion methods.
    \item Despite impressive results, these methods face challenges in complex scenes with multiple objects.
    \item Our work complements these methods by incorporating paired or unpaired data from new domains/tasks.    
\end{itemize}
\end{frame}

% ---------- One-step generative models ----------
\begin{frame}
\frametitle{Related Work}
\framesubtitle{One-step generative models}

\end{frame}

% ---------- Begin Section Terminology ----------
% ---------- Generative Adversarial Networks(GAN) ----------
\begin{frame}
\frametitle{Terminology}
\framesubtitle{Generative Adversarial Networks(GAN)}
\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{images/blog_gan.png}
    \caption{GAN training process}
\end{figure}
\tiny{\footnotemark \url{http://www.lherranz.org/2018/08/07/imagetranslation/}}
\end{frame}
\note{}

% ---------- Generative Adversarial Networks(GAN) ----------
\begin{comment}
\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Generative Adversarial Networks(GAN)}
    \begin{block}{Generator loss}
        \begin{equation}
            \min_G \mathcal{L}_G = \mathbb{E}_{z \sim p_z(z)} [\log(1 - D(G(z)))]
        \end{equation}
    \end{block}
    \begin{block}{Discriminator loss}
        \begin{equation}
            \max_D \mathcal{L}_D = \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log(1 - D(G(z)))]
        \end{equation}
    \end{block}
    \tiny\footnotemark \url{Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Neural Information
    Processing Systems (NeurIPS) (2014)}
\end{frame}
\end{comment}
% ---------- CycleGAN ----------
\begin{frame}
\frametitle{Terminology}
\framesubtitle{CycleGAN}
\begin{figure}
    \centering
    \includegraphics[width=0.78\linewidth]{images/blog_cyclegan_h2z2h-768x333.png}
    \caption{CycleGAN Architecture}
\end{figure}
\tiny{\footnotemark \url{http://www.lherranz.org/2018/08/07/imagetranslation/}}
\end{frame}

\begin{comment}
    
    % ---------- CycleGAN ----------
    \begin{frame}
    \frametitle{Terminology}
    \framesubtitle{CycleGAN}
    \begin{block}{Adversarial loss for G and F}
        \begin{align}
            \mathcal{L}_{\text{GAN}}(F, D_X, Y, X) = \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D_X(x)] + \mathbb{E}_{y \sim p_{\text{data}}(y)} [\log(1 - D_X(F(y)))] \\
            \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) = \mathbb{E}_{y \sim p_{\text{data}}(y)} [\log D_Y(y)] + \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log(1 - D_Y(G(x)))]
        \end{align}
    \end{block}
    
    \begin{block}{Cycle consistency loss}
        \begin{equation}
            \mathcal{L}_{\text{cycle}}(G, F) = \mathbb{E}_{x \sim p_{\text{data}}(x)} [\lVert F(G(x)) - x \rVert_1] + \mathbb{E}_{y \sim p_{\text{data}}(y)} [\lVert G(F(y)) - y \rVert_1]
        \end{equation}
    \end{block}
\end{frame}
\note{\begin{itemize}
    \item Adversarial loss: Standard GAN loss. Vergleich zwei Folien früher    
    \item Cycle consistency loss: Wenn ein Bild von Domain X in Domain Y übersetzt wird und dann wieder zurück, sollte das Ergebnis dem Original entsprechen. Mit L1 Norm (mean absoulte Error)
    }
    % ---------- CycleGAN ----------
    \begin{frame}
    \frametitle{Terminology}
    \framesubtitle{CycleGAN}
    \begin{block}{Identity loss}
        \begin{align}
            \mathcal{L}_{\text{identity}}(G, F, X, Y) & = \mathbb{E}_{y \sim p_{\text{data}}(y)} [\lVert G(y) - y \rVert_1] \\
            \mathcal{L}_{\text{identity}}(F, G, Y, X) & = \mathbb{E}_{x \sim p_{\text{data}}(x)} [\lVert F(x) - x \rVert_1]
        \end{align}
    \end{block}
    
    \begin{block}{Cycle loss}
        \begin{equation}
            \begin{split}
                &\mathcal{L}(G, F, D_X, D_Y) = \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X) + \\
                &\lambda \mathcal{L}_{\text{cycle}}(G, F) + \lambda_{\text{id}} \left( \mathcal{L}_{\text{identity}}(G, F, X, Y) + \mathcal{L}_{\text{identity}}(F, G, Y, X) \right)
            \end{split}    
        \end{equation}
    \end{block}
    \tiny{\footnotemark \url{https://arxiv.org/pdf/1703.10593.pdf}}
\end{frame}
\note{\begin{itemize}
    \item Identity loss: Wenn ein Bild von Domain X in Domain Y übersetzt wird, sollte das Ergebnis dem Original entsprechen
    \item Cycle loss: gewichtete Kombination aus Adversarial loss, Cycle consistency loss und Identity loss
    }
\end{comment}
    
    
% ---------- UNet and Skip Connections ----------
\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{UNet and Skip Connections}
    \begin{figure}
        \centering
        \includegraphics[width=0.6\linewidth]{images/Group14.jpg}
        \caption{Architecture}
    \end{figure}
\end{frame}
\note{
    \begin{itemize}
        \item UNet: Convolutional Neural Network, das für Bildsegmentierung verwendet wird
        \begin{itemize}
            \item Encoder Pfad: Mehrere Blöcke von convolutional layers mit ReLU activation und max pooling. Reduziert die Dimensionalität des Inputs
            \item Decoder Pfad: Mehrere Blöcke von convolutional layers mit RelU activation und upconvolution. Erhöht die Dimensionalität des Inputs. Außerdem Concatenation mit entsprechenden Encoder Pfad
            \item Skip Connections: Verbindung zwischen Encoder und Decoder Pfad. Hilft details zu erhalten, die im Encoder verloren gehen würden
        \end{itemize}
    \end{itemize}
}

% ---------- LoRA Weights ----------
\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{LoRA Weights}
    \begin{figure}
        \centering
        \includegraphics[width=0.4\linewidth]{images/Bildschirmfoto vom 2024-04-15 10-21-59.png}
        \caption{\textbf{Lo}w \textbf{R}ank \textbf{A}daption}
    \end{figure}
\end{frame}

% ---------- Paired vs unpaired data ----------
\begin{frame}
\frametitle{Terminology}
\framesubtitle{Paired vs unpaired data}
\begin{columns}
    \column{0.5\textwidth}
    \centering    
    \begin{figure}
        \includegraphics[width=0.75\textwidth]{images/blog_unpairedimagetranslation2.png}
        \caption{Unpaired Data}
    \end{figure}

    \column{0.5\textwidth}    
    \centering    
    \begin{figure}        
        \includegraphics[width=0.75\textwidth]{images/blog_pairedimagetranslation.png}
        \caption{Paired Data}
    \end{figure}
  \end{columns}
\end{frame}
\note{
    \begin{itemize}
        \item Paired data: Jedes Bild in Domain X hat ein korrespondierendes Bild in Domain Y
        \item Unpaired data: Es gibt keine direkte Zuordnung zwischen den Bildern in Domain X und Domain Y
    \end{itemize}

}
% ---------- Begin Section Method ----------
% ---------- Adding Conditioning Input ----------
\begin{frame}
\frametitle{Method}
\framesubtitle{Adding Conditioning Input}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/Bildschirmfoto vom 2024-04-14 10-57-39.png}
    \caption{Conflicts between noise and conditional input}
\end{figure}
\end{frame}

% ---------- Preserving Input Details ----------
\begin{frame}
\frametitle{Method}
\framesubtitle{Preserving Input Details}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/Bildschirmfoto vom 2024-04-14 11-06-40.png}
    \caption{Skip Connections help retain details}
\end{figure}
\end{frame}

% ---------- Preserving Input Details ----------
\begin{frame}
\frametitle{Method}
\framesubtitle{Preserving Input Details}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/method.jpg}
    \caption{Model Architecture}
\end{figure}
\end{frame}

% ---------- Unpaired Training ----------
\begin{frame}
\frametitle{Method}
\framesubtitle{Unpaired Training}
Goal: Convert images from $\mathcal{X} \subset \mathbb{R} ^{H x W x 3}$ to $\mathcal{Y} \subset \mathbb{R} ^{H x W x 3}$ \newline
given an unpaired dataset $\mathcal{X} = \{x \in \mathcal{X} \}$ and $\mathcal{Y} = \{y \in \mathcal{Y} \}$ \newline
using one network G and two translations $G(x, c_y): \mathcal{X} \rightarrow \mathcal{Y}$ and $G(y, c_x): \mathcal{Y} \rightarrow \mathcal{X}$.

\end{frame}

\begin{frame}
\frametitle{Method}
\framesubtitle{Unpaired Training}
\begin{block}{Cycle consistency with perecptual loss}
    \begin{equation}
        \mathcal{L}_{\text{cycle}}(G, F) = \mathbb{E}_x [ \mathcal{L}_\text{rec} (G(G(x,c_Y), c_X), x) ] + \mathbb{E}_y [ \mathcal{L}_\text{rec} (G(G(y,c_X), c_Y), y) ]
    \end{equation}
\end{block}
with $\mathcal{L}_{\text{rec}}$ as combination of L1 and LPIPS \cite{zhang2018unreasonable}
\begin{block}{Adversarial loss}
    \begin{align}
        \mathcal{L}_{\text{GAN}} &= \mathbb{E}_{y} [\log D_Y(y)] + \mathbb{E}_{x} [\log(1 - D_Y(G(x,c_Y)))] \\
        &+ \mathbb{E}_{x} [\log D_X(x)] + \mathbb{E}_{Y} [\log(1 - D_X(G(y,c_X)))]
    \end{align}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Method}
\framesubtitle{Unpaired Training}
\begin{block}{Identity regularization loss}
    \begin{equation}
        \mathcal{L} _{\text{idt}} = \mathbb{E} _y [ \mathcal{L}_{\text{rec}}(G(y,c_Y),y)] + \mathbb{E}_x [ \mathcal{L}_{\text{rec}}(G(x,c_X),x)]
    \end{equation}
\end{block}

\begin{block}{Full objective}
    \begin{equation}
       \arg \underset{G}{\min} \mathcal{L}_{\text{cycle}} + \lambda _{\text{idt}} \mathcal{L}_{\text{idt}} + \lambda_{\text{GAN}}\mathcal{L}_{\text{GAN}}
    \end{equation}
\end{block}
    

\end{frame}


% ---------- Extensions ----------
\begin{frame}
\frametitle{Method}
\framesubtitle{Extensions - Paired Training}
\begin{itemize}
    \item Adaptation of network G to paired setting, like edge-to-image or sketch-to-image, called pix2pix-Turbo
    \item new translation function $G(x,c): X \rightarrow Y$ where $X$ is source domain, $Y$ target domain and $c$ conditioning input
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Method}
    \framesubtitle{Extensions - Generating diverse output}
    Introduction of interpolation coefficient $\gamma$ \newline 
    Three changes to the Architecture (1/3):
    \begin {itemize}
        \item Generator function $G(x,z,\gamma)$ combines noise z and encoder output like so: $\gamma G_{\text{enc}}(x) + (1 - \gamma) z$
        \item Output as U-Net input
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Method}
    \framesubtitle{Extensions - Generating diverse output}
    Three changes to the Architecture (2/3):
    \begin {itemize}
        \item Scale LoRA weights and skip connections according to $\theta = \theta_0 + \gamma \Delta \theta$  
        \item where $\theta_0$ and $\Delta \theta$ denote the original weights and new weights.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Method}
    \framesubtitle{Extensions - Generating diverse output}
    Three changes to the Architecture (3/3):
    \begin {itemize}
        \item Scale reconstruction loss according to $\gamma$: $\mathcal{L}_{\text{diverse}} = \mathcal{L}_{x,y,z,\gamma} \gamma\mathcal{L}_{\text{rec}}(G(x,z,\gamma),y)]$
        \item $\gamma = 0$ corresponds to default stochastic behavior of pretrained model, in this cas reconstruction loss is not enforced
        \item $\gamma = 1$ corresponds to deterministic translation from previoues seections
    \end{itemize}
\end{frame}

% ---------- Begin Sections Experiments - Paired Image Translation ----------
% ---------- Experiments - Paired Image Translation ----------
\begin{frame}
\frametitle{Experiments - Paired Image Translation}
\framesubtitle{Baselines}
Training details:
\begin{itemize}
    \item 330MB of trainable parameters for unpaired models(LoRA weights, zero-conv layer, first conv layer of U-Net)
    \item Adam Optimizer with learning rate: 1e-6, batch size:8, $\lambda _{\text{idt}} = 1$, $\lambda _{\text{GAN}} = 0.5$
\end{itemize}
Datasets:
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        Task & Images Source & Images Target & Dataset \\
        \hline
        Horse $\leftrightarrow$ Zebra & 939 & 1177 & ImageNet \cite{5206848}\\
        Winter $\leftrightarrow$ Summer & 854 & 1273 & Flickr \cite{zhu2020unpaired} \\
        Day $\leftrightarrow$ Night & Day subset & Night subset & BDD100k \cite{yu2020bdd100k}\\
        Clear $\leftrightarrow$ Foggy & 12454 & 572 from & BDD100k and DENSE \cite{bijelic2020seeing}
    \end{tabular}       
\end{table}
\end{frame}

\begin{frame}
    \frametitle{Experiments - Paired Image Translation}
    \framesubtitle{Baselines}
    Evaluation Protocol:
    \begin{itemize}
        \item match data distribution of target domain $\rightarrow$ FID \cite{heusel2018gans}
        \item preserve input image structure in translated output $\rightarrow$ DINO \cite{tumanyan2022splicing}
        \item Inference runtime using a single NVIDIA RTX A6000 GPU
        \item human preceptual study
    \end{itemize}
    
    
\end{frame}
% ---------- Comparison to Unpaired Methods ----------

\begin{frame}
    \frametitle{Experiments - Paired Image Translation}
    \framesubtitle{Comparison to Unpaired Methods}
    \begin{figure}
        \centering
        \includegraphics[width=0.85\linewidth]{images/horse_zebra.png}
        
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiments - Paired Image Translation}
    \framesubtitle{Comparison to Unpaired Methods}
    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{images/horse_zebra_table.png}
        
    \end{figure}
\end{frame}

\begin{frame}
\frametitle{Experiments - Paired Image Translation}
\framesubtitle{Comparison to Unpaired Methods}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/day_night.png}
    
\end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiments - Paired Image Translation}
    \framesubtitle{Comparison to Unpaired Methods}
    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{images/day_night_table.png}
        
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiments - Paired Image Translation}
    \framesubtitle{Comparison to Unpaired Methods}
    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{images/human-pref.png}
        
    \end{figure}
\end{frame}


% ---------- Ablation Study ----------
\begin{frame}
\frametitle{Experiments - Paired Image Translation}
\framesubtitle{Ablation Study}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/ablation_horse_zebra.png}
    
\end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiments - Paired Image Translation}
    \framesubtitle{Ablation Study}
    \begin{figure}
        \centering
        \includegraphics[width=0.55\linewidth]{images/ablation_images.png}
        
    \end{figure}
    \end{frame}
% ---------- Extensions ----------
\begin{frame}
\frametitle{Experiments - Unpaired Image Translation}
\framesubtitle{Training Details}
\begin{block}{Loss function}
    \begin{align}
        \arg \underset{G}{\min} \mathcal{L}_{\text{rec}} + \lambda _{\text{clip}} \mathcal{L}_{\text{CLIP}} + \lambda_{\text{GAN}}\mathcal{L}_{\text{GAN}}
    \end{align}
\end{block}
with $\mathcal{L}_{\text{rec}}$ = L2-Norm + LPIPS, $\lambda_{\text{clip}} = 4$ and $\lambda_{\text{GAN}} = 0.4$
\end{frame}

\begin{frame}
    \frametitle{Experiments - Unpaired Image Translation}
    \framesubtitle{Training Details}
        Edge-to-Image:
        \begin{itemize}
            \item Canny Edge Detector with random threshold
            \item Adam Optimizer with learning rate: 1e-5, batch size: 40, Steps: 7500
        \end{itemize}
        Sketch-to-Image:
        \begin{itemize}
            \item Synthetic sketches with multiple augmentations
            \item Initialized with Edge-to-Image model and fine-tuned for 5000 steps with same Optimizer
        \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Experiments - Unpaired Image Translation}
    \framesubtitle{Comparison to Unpaired Methods}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{images/unpaired_comp1.png}
        
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Experiments - Unpaired Image Translation}
    \framesubtitle{Comparison to Unpaired Methods}
    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{images/unpaired_comp2.png}
            
    \end{figure}
\end{frame}

% ---------- Begin Section Discussion ----------
% ---------- Discussion ----------
\begin{frame}
\frametitle{Discussion and Limitations}
\framesubtitle{Discussion}
\begin{itemize}
    \item one-step pre-trained models can serve as a backbone model for many image synthesis tasks
    \item Adapting the models can be achieved through GAN objectives without multi-step diffusion training
    \item model training requires a small number of additional trainable parameters
\end{itemize}
\end{frame}

% ---------- Limitations ----------
\begin{frame}
\frametitle{Discussion and Limitations}
\framesubtitle{Limitations}
\begin{itemize}
    \item cannot specify strength of guidance as SD-Turbo does not use classifier-free guidance
    \item does not support negative prompt
    \item training is memory intensive
\end{itemize}

\end{frame}

% ---------- End ----------
\begin{frame}
\frametitle{The End}
\begin{center}
\scalebox{2}{Questions?}
\end{center}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{References}
\printbibliography
\end{frame}
\end{document}