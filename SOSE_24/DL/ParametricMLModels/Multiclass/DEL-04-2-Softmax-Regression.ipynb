{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "portable-honolulu",
   "metadata": {},
   "source": [
    "## DEL-02 Programming Excercise - Softmax Regression\n",
    "### (created by Prof. Dr.-Ing. Christian Bergler & Prof. Dr. Fabian Brunner)\n",
    "\n",
    "Documentation: **Python-Bibliothek Pandas** - https://pandas.pydata.org/docs/\n",
    "\n",
    "Documentation: **Numpy** - https://numpy.org/doc/\n",
    "\n",
    "Documentation: **Sklearn** - https://scikit-learn.org/stable/index.html\n",
    "\n",
    "Documentation: **Matplotlib** - Documentation: https://matplotlib.org/stable/index.html\n",
    "\n",
    "Documentation: **Matplotlib** - Graphics Gallery: https://matplotlib.org/2.0.2/gallery.html\n",
    "\n",
    "Additional Documentation: **Python Tutorial** - https://docs.python.org/3/tutorial/\n",
    "\n",
    "Additional Documentation: **Matthes Eric, \"Python crash course: A hands-on, project-based introduction to programming\"**, ISBN: 978-1-59327-603-4, ©2023 no starch press  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-abraham",
   "metadata": {},
   "source": [
    "### Softmax-Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-vitamin",
   "metadata": {},
   "source": [
    "- The model function for the `softmax regression` is made up of the `affine-linear transformation`\n",
    "\\begin{align*}\n",
    "z_1 &= w_{11}x_1+ w_{21} x_2+\\ldots+w_{p1} x_p+b_1~,\\\\\n",
    "z_2 &= w_{12}x_1+w_{22} x_2+ \\ldots+w_{p2} x_p+b_2~,\\\\\n",
    "\\vdots & \\hspace{4cm}\\vdots \\\\\n",
    "z_n &= w_{1n}x_1+w_{2n} x_2+\\ldots +w_{pn} x_p +b_n\n",
    "\\end{align*}\n",
    "with $n$ number of (output) neurons and $p$ number of (input) neurons from the previous layer $l-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-petroleum",
   "metadata": {},
   "source": [
    "- Followed by the subsequent application of the `softmax function`\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\vec{y}}=\\mathrm{softmax}(\\vec{z})\n",
    "\\end{align*}\n",
    "$$\n",
    "with respect to the (output) feature vector $\\vec{z}$: $$\\vec{z}=(z_1,\\ldots,z_n)^T$$\n",
    "using:\n",
    "\\begin{align*}\n",
    "\\vec{W}=\\left(\\begin{array}{rrrr}\n",
    "w_{11}&w_{12}&\\ldots &w_{1n}\\\\\n",
    "w_{21}&w_{22}&\\ldots&w_{2n}\\\\\n",
    "\\vdots &\\vdots &\\ddots&\\vdots\\\\\n",
    "w_{p1}&w_{p2}&\\ldots& w_{pn}\n",
    "\\end{array}\\right)\\in\\mathbb{R}^{p\\times n}~,\\qquad \n",
    "\\vec{b}=\\left(\\begin{array}{c} b_1\\\\ b_2\\\\ \\vdots \\\\ b_n\\end{array}\\right)\\in\\mathbb{R}^n~\n",
    "\\end{align*}\n",
    "which represents the vectorized form:\n",
    "\\begin{align*}\n",
    "\\vec{z}&=\\vec{W}^T\\vec{x}+\\vec{b}\\\\\n",
    "\\hat{\\vec{y}}&=\\mathrm{softmax}(\\vec{z})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-basketball",
   "metadata": {},
   "source": [
    "- When vectorizing over a batch $\\vec{X}\\in\\mathbb{R}^{m\\times p}$ (where $m$ denotes the number of samples and $p$ the number of features), the representation is as follows\n",
    "Bei Vektorisierung über ein Batch $\\vec{X}\\in\\mathbb{R}^{m\\times p}$ (wobei $m$ die Anzahl der Samples und $p$ die Anzahl der Features bezeichnen) lautet die Darstellung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-growing",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{Z}&=\\vec{X}\\vec{W}+\\vec{b}\\\\\n",
    "\\hat{\\vec{Y}}&=\\mathrm{softmax}(\\vec{Z})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-scroll",
   "metadata": {},
   "source": [
    "### Task DEL-02-1 (Shapes and Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-lecture",
   "metadata": {},
   "source": [
    "- a) What is the dimension of the matrix $\\hat{\\vec{Y}}$? How should the rows of the matrix be interpreted?\n",
    "- b) Assume that all weights $\\vec{W}$ and $\\vec{b}$ have the value zero. What result/prediction does the model then provide for any input vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-fence",
   "metadata": {},
   "source": [
    "### Task DEL-02-2 (Numerical Stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-auction",
   "metadata": {},
   "source": [
    "- a) Consider the following (vectorized) implementation of the `softmax function`:\n",
    "    - Explain this implementation! In particular, explain why ```keepdims=True``` is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pointed-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def softmax(Z):\n",
    "    Z_exp = np.exp(Z)\n",
    "    Z_exp_sum = Z_exp.sum(axis=1, keepdims=True)\n",
    "    return Z_exp/Z_exp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-disposition",
   "metadata": {},
   "source": [
    "- b) Show that for a given vector $\\vec{z}=(z_1,\\ldots,z_n)^T$ and any number $\\alpha\\in\\mathbb{R}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{softmax}(\\vec{z})=\\mathrm{softmax}(\\vec{z}-\\alpha\\cdot (1,\\ldots,1)^T)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-dublin",
   "metadata": {},
   "source": [
    "- c) What problems can arise in the numerical evaluation of the `softmax function`? How can the property shown in subtask b) be utilised to evaluate the `softmax function` in a numerically stable way?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-bunch",
   "metadata": {},
   "source": [
    "### Task DEL-02-3 (Model Training with Softmax Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-intersection",
   "metadata": {},
   "source": [
    "- When training a `softmax regression` model, the unknown weights $\\vec{W}$ and $\\vec{b}$ are determined by minimizing the `cross-entropy loss function`\n",
    "$$\n",
    "L(\\vec{W},\\vec{b})=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{j=1}^n y^{(i)}_j\\cdot \\log(\\hat{y}^{(i)}_j)~\\quad\\text{mit}~~\\hat{y}^{(i)}_j=f_{\\vec{W},\\vec{b}}(\\vec{x}^{(i)})_j\n",
    "$$\n",
    "Here, $y_j^{(i)}$ is the $j$-th component of the $i$-th (vector-valued, obtained by one hot encoding) label and $\\hat{y}_j^{(i)}$ is the $j$-th component of the probability distribution predicted by the model for the $i$-th sample of the batch.\n",
    "\n",
    "- a) In which case does $L(\\vec{W},\\vec{b})=0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-variation",
   "metadata": {},
   "source": [
    "- b) Let $L^{(i)}=-\\sum_{j=1}^n y_j^{(i)}\\log(\\hat{y}_j^{(i)})$ be the contribution of the $i$th samples to the loss function. Using the chain rule, show that \n",
    "$$\\frac{\\partial L^{(i)}}{\\partial o_j^{(i)}}= \\hat{y}_j^{(i)}-y_j^{(i)}$$\n",
    "is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-hacker",
   "metadata": {},
   "source": [
    "- c) Using the result from b), derive expressions for the partial derivatives of $L$ according to the weights:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\frac{\\partial L}{\\partial w_{ij}}~,\\quad i=1,\\ldots,p,~ j=1,\\ldots,n~,\\\\\n",
    "&\\frac{\\partial L}{\\partial b_j}~,\\quad ~j=1,\\ldots,n~.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-reading",
   "metadata": {},
   "source": [
    "- <b>Result to verify:\n",
    "  \n",
    "    </b> If the partial derivatives are summarised in matrices $dW$ and $dB$ (with the same dimensions as $\\vec{W}$ and $\\vec{b}$), they are obtained in vectorized form by\n",
    "    $$dW=\\frac{1}{m}\\vec{X}^T(\\hat{\\vec{Y}}-\\vec{Y})~,\\qquad dB=\\frac{1}{m}(1,\\ldots,1)\\cdot(\\hat{\\vec{Y}}-\\vec{Y})$$\n",
    "    You can use this representation to implement the gradient method in the following task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-lotus",
   "metadata": {},
   "source": [
    "### Task DEL-02-4 (Implementation Softmax Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-status",
   "metadata": {},
   "source": [
    "- Complete the following implementation of the MySoftmaxRegression class, which realises the Softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "animal-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data \n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "identical-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Vectorized implementation of softmax function\n",
    "    \"\"\"\n",
    "    Z_exp = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    partition = Z_exp.sum(axis=1, keepdims=True)\n",
    "    return Z_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ruled-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySoftmaxRegression:\n",
    "    \n",
    "    def __init__(self, lr = 1.0, num_iter = 100):\n",
    "        self.learning_rate = lr #learning rate\n",
    "        self.num_iter = num_iter #number of iterations for gradient descent\n",
    "        self.W = None #weight matrix\n",
    "        self.b = None #bias vector\n",
    "        \n",
    "        \n",
    "    def net(self, X):\n",
    "        \"\"\"\n",
    "        Model function to predict scores for a batch of samples (for logistic regression, this function was called \"predict_proba\")\n",
    "        \n",
    "        :param X: batch of training data of dimension n_samples x n_features\n",
    "        :type X: numpy array\n",
    "        :return: array containing the predicted scores for all samples of the batch X (dimension: n_samples x n_classes)\n",
    "        :rtype: numpy array\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Model training using gradient descent optimization algorithm\n",
    "        \n",
    "        :param X: batch of training data of dimension n_samples x n_features\n",
    "        :type X: numpy array\n",
    "        :param y: target values corresponding to records in X \n",
    "        :type y: numpy array\n",
    "        :return: List containing the values of the loss function after each iteration of Gradient descent\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes based on the largest predicted class probability\n",
    "        \n",
    "        :param X: batch of data to be scored\n",
    "        :type X: numpy array\n",
    "        :param threshold: decision criterion, where the prediction probability of the model needs to be >= threshold to assign the binary (true) class =1 \n",
    "        :type threshold: float\n",
    "        :return: predicted classes for the records in X\n",
    "        :rtype: numpy array\n",
    "        \"\"\" \n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95299673-b15c-4030-89ed-13e65f012f38",
   "metadata": {},
   "source": [
    "- Call the constructor of the class `MySoftmaxRegression` to create an object `my_soft_regressor` and train the algorithm in connection with the training data of the `Iris Dataset` (see above)\n",
    "- Use a learning rate `lr=`$0.05$ and a number of iteration `num_iter`=$10000$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "typical-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce3d66-b86c-40aa-9a2f-50d641c4d074",
   "metadata": {},
   "source": [
    "- The `preprocessing` package of the `sklear` module holds a class `StandardScaler`in order to standardize features by substracting the mean and scaling to unit variance via $$z = \\frac{x-\\mu}{s}$$ with $\\mu =$mean, $s=$stdv\n",
    "- Call the `fit_transform` function of the `sc` object in order to build the standardized data corpus\n",
    "- Train the model by calling the `train` function of the `my_soft_regressor` ojbect together with the standardized training data and respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "positive-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa14cf9-d3ed-4e65-9979-a14cc50babc2",
   "metadata": {},
   "source": [
    "- Using `matplotlib` in order to plot and visualize the temporal development regarding the values of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "peaceful-heater",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Training loss')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO\n",
    "\n",
    "plt.xlabel('Anahl Iterationen')\n",
    "plt.ylabel('Training loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16c34e-b926-4d73-9bef-7c33f9e6e9cd",
   "metadata": {},
   "source": [
    "- Classification of the training dataset (Note: For the sake of simplicity, model validation by splitting the initial data corpus into training, validation, and test data paritions was not performed in this case. The aim of the task was primarily to introduce and present how to design a `softmax regression classifier`. In practice, of course, such partitioning is carried out in advance\n",
    "- Call the `predict` function of the `my_soft_regressor` object with respect to the standardized training data corpus\n",
    "- How to handle a threshold operation also in the case of `softmax` regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "distinct-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
