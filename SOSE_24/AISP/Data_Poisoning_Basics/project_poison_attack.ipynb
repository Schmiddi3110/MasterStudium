{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.utils import load_cifar10\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # mps does not work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "\n",
    "# import the model, training routines you need from your victim's code...\n",
    "\n",
    "\n",
    "print(\"Model and data preparation done.\")\n",
    "\n",
    "model_art = PyTorchClassifier() # TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TARGET IMAGES \n",
    "# ATTACK GOAL\n",
    "# - select a source class ID (0...9) and a target class id \n",
    "# - select a bunch of images belonging to the source class and try to get the victim's model to classify them as target class.target_images\n",
    "# - try several source/target combinations  \n",
    "# - use Gradient Matching Attack\n",
    "\n",
    "SOURCE = \n",
    "TARGET = \n",
    "\n",
    "\n",
    "\n",
    "x_trigger = \n",
    "y_trigger = \n",
    "\n",
    "\n",
    "epsilson = \n",
    "percent_poison = 0.01\n",
    "attack = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE YOUR ATTACK\n",
    "# - assume you can put 1% (percent_poison=0.01) of poisoned images into the victim's training data. The remainder of the training data is unchanged.\n",
    "# - train a model to see how successful your attack will be then\n",
    "# - how many of your selected target images get correctly misclassified (according to your attack)?\n",
    "# - can you optimize the attack to increase your success?\n",
    "# - would you be more successful with 2% poison images? How about 0.1%?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
