{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abc4177",
   "metadata": {},
   "source": [
    "# N-GRAM - Probabilistic Language Modeling\n",
    "\n",
    "- Let us build our first language model ...\n",
    "- **Important**: for all of the tasks below please make use of the provided methods given by the `nltk` Python module\n",
    "- This is a step-by-step coding example to get from raw text information to the final language model\n",
    "- Have look at the NLTK-LM documentation: https://www.nltk.org/api/nltk.lm.html\n",
    "\n",
    "## Load and Extract the Data\n",
    "\n",
    "- Use the provided text document including a series of different Tweets from Donald Trupm (`Donald-Trump-Tweets.csv`)\n",
    "- Use the `Pandas` Python library to read the CSV-file as a `pandas dataframe` object\n",
    "- Filter the column which contains the text information (`Tweet_Text`), leading to a Table with 2 columns -- index and text information\n",
    "- Verify also for invalid entries and filter them in advance (`notnull`function of the `pandas dataframe` object)\n",
    "- Visualize the resulting and filtered `Pandas` Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89f9a3e-b163-48c4-b75f-92aa8c117cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Media_Type</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_Url</th>\n",
       "      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>15:26:37</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>text</td>\n",
       "      <td>photo</td>\n",
       "      <td>ThankAVet</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>127213</td>\n",
       "      <td>41112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>13:33:35</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>141527</td>\n",
       "      <td>28654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>11:14:20</td>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>183729</td>\n",
       "      <td>50039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:19:44</td>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>214001</td>\n",
       "      <td>67010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:10:46</td>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>178499</td>\n",
       "      <td>36688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>13:10:00</td>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>953</td>\n",
       "      <td>431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:18:31</td>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1175</td>\n",
       "      <td>1086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:10:17</td>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1494</td>\n",
       "      <td>930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>9:44:07</td>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1800</td>\n",
       "      <td>1738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>0:21:25</td>\n",
       "      <td>RT @marklevinshow: Trump: Rove is a clown and ...</td>\n",
       "      <td>link</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.210000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>962</td>\n",
       "      <td>689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7375 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time                                         Tweet_Text  \\\n",
       "0     16-11-11  15:26:37  Today we express our deepest gratitude to all ...   \n",
       "1     16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...   \n",
       "2     16-11-11  11:14:20  Love the fact that the small groups of protest...   \n",
       "3     16-11-11   2:19:44  Just had a very open and successful presidenti...   \n",
       "4     16-11-11   2:10:46  A fantastic day in D.C. Met with President Oba...   \n",
       "...        ...       ...                                                ...   \n",
       "7370  15-07-16  13:10:00  I loved firing goofball atheist Penn @pennjill...   \n",
       "7371  15-07-16  10:18:31  I hear @pennjillette show on Broadway is terri...   \n",
       "7372  15-07-16  10:10:17  Irrelevant clown @KarlRove sweats and shakes n...   \n",
       "7373  15-07-16   9:44:07  \"@HoustonWelder: Donald Trump is one of the se...   \n",
       "7374  15-07-16   0:21:25  RT @marklevinshow: Trump: Rove is a clown and ...   \n",
       "\n",
       "      Type Media_Type   Hashtags      Tweet_Id  \\\n",
       "0     text      photo  ThankAVet  7.970000e+17   \n",
       "1     text        NaN        NaN  7.970000e+17   \n",
       "2     text        NaN        NaN  7.970000e+17   \n",
       "3     text        NaN        NaN  7.970000e+17   \n",
       "4     text        NaN        NaN  7.970000e+17   \n",
       "...    ...        ...        ...           ...   \n",
       "7370  text        NaN        NaN  6.220000e+17   \n",
       "7371  text        NaN        NaN  6.220000e+17   \n",
       "7372  text        NaN        NaN  6.220000e+17   \n",
       "7373  text        NaN        NaN  6.220000e+17   \n",
       "7374  link        NaN        NaN  6.210000e+17   \n",
       "\n",
       "                                              Tweet_Url  \\\n",
       "0     https://twitter.com/realDonaldTrump/status/797...   \n",
       "1     https://twitter.com/realDonaldTrump/status/797...   \n",
       "2     https://twitter.com/realDonaldTrump/status/797...   \n",
       "3     https://twitter.com/realDonaldTrump/status/796...   \n",
       "4     https://twitter.com/realDonaldTrump/status/796...   \n",
       "...                                                 ...   \n",
       "7370  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7371  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7372  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7373  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7374  https://twitter.com/realDonaldTrump/status/621...   \n",
       "\n",
       "      twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  \\\n",
       "0                                        127213     41112          NaN   \n",
       "1                                        141527     28654          NaN   \n",
       "2                                        183729     50039          NaN   \n",
       "3                                        214001     67010          NaN   \n",
       "4                                        178499     36688          NaN   \n",
       "...                                         ...       ...          ...   \n",
       "7370                                        953       431          NaN   \n",
       "7371                                       1175      1086          NaN   \n",
       "7372                                       1494       930          NaN   \n",
       "7373                                       1800      1738          NaN   \n",
       "7374                                        962       689          NaN   \n",
       "\n",
       "      Unnamed: 11  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "7370          NaN  \n",
       "7371          NaN  \n",
       "7372          NaN  \n",
       "7373          NaN  \n",
       "7374          NaN  \n",
       "\n",
       "[7375 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "trump_data = pandas.read_csv('Donald-Trump-Tweets.csv')\n",
    "trump_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f4e970-b5b2-4461-bf22-8905d38622e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Media_Type</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_Url</th>\n",
       "      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>15:26:37</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>text</td>\n",
       "      <td>photo</td>\n",
       "      <td>ThankAVet</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>127213</td>\n",
       "      <td>41112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>13:33:35</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>141527</td>\n",
       "      <td>28654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>11:14:20</td>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>183729</td>\n",
       "      <td>50039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:19:44</td>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>214001</td>\n",
       "      <td>67010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:10:46</td>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>178499</td>\n",
       "      <td>36688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>13:10:00</td>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>953</td>\n",
       "      <td>431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:18:31</td>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1175</td>\n",
       "      <td>1086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:10:17</td>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1494</td>\n",
       "      <td>930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>9:44:07</td>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1800</td>\n",
       "      <td>1738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>0:21:25</td>\n",
       "      <td>RT @marklevinshow: Trump: Rove is a clown and ...</td>\n",
       "      <td>link</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.210000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>962</td>\n",
       "      <td>689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7375 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time                                         Tweet_Text  \\\n",
       "0     16-11-11  15:26:37  Today we express our deepest gratitude to all ...   \n",
       "1     16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...   \n",
       "2     16-11-11  11:14:20  Love the fact that the small groups of protest...   \n",
       "3     16-11-11   2:19:44  Just had a very open and successful presidenti...   \n",
       "4     16-11-11   2:10:46  A fantastic day in D.C. Met with President Oba...   \n",
       "...        ...       ...                                                ...   \n",
       "7370  15-07-16  13:10:00  I loved firing goofball atheist Penn @pennjill...   \n",
       "7371  15-07-16  10:18:31  I hear @pennjillette show on Broadway is terri...   \n",
       "7372  15-07-16  10:10:17  Irrelevant clown @KarlRove sweats and shakes n...   \n",
       "7373  15-07-16   9:44:07  \"@HoustonWelder: Donald Trump is one of the se...   \n",
       "7374  15-07-16   0:21:25  RT @marklevinshow: Trump: Rove is a clown and ...   \n",
       "\n",
       "      Type Media_Type   Hashtags      Tweet_Id  \\\n",
       "0     text      photo  ThankAVet  7.970000e+17   \n",
       "1     text        NaN        NaN  7.970000e+17   \n",
       "2     text        NaN        NaN  7.970000e+17   \n",
       "3     text        NaN        NaN  7.970000e+17   \n",
       "4     text        NaN        NaN  7.970000e+17   \n",
       "...    ...        ...        ...           ...   \n",
       "7370  text        NaN        NaN  6.220000e+17   \n",
       "7371  text        NaN        NaN  6.220000e+17   \n",
       "7372  text        NaN        NaN  6.220000e+17   \n",
       "7373  text        NaN        NaN  6.220000e+17   \n",
       "7374  link        NaN        NaN  6.210000e+17   \n",
       "\n",
       "                                              Tweet_Url  \\\n",
       "0     https://twitter.com/realDonaldTrump/status/797...   \n",
       "1     https://twitter.com/realDonaldTrump/status/797...   \n",
       "2     https://twitter.com/realDonaldTrump/status/797...   \n",
       "3     https://twitter.com/realDonaldTrump/status/796...   \n",
       "4     https://twitter.com/realDonaldTrump/status/796...   \n",
       "...                                                 ...   \n",
       "7370  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7371  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7372  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7373  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7374  https://twitter.com/realDonaldTrump/status/621...   \n",
       "\n",
       "      twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  \\\n",
       "0                                        127213     41112          NaN   \n",
       "1                                        141527     28654          NaN   \n",
       "2                                        183729     50039          NaN   \n",
       "3                                        214001     67010          NaN   \n",
       "4                                        178499     36688          NaN   \n",
       "...                                         ...       ...          ...   \n",
       "7370                                        953       431          NaN   \n",
       "7371                                       1175      1086          NaN   \n",
       "7372                                       1494       930          NaN   \n",
       "7373                                       1800      1738          NaN   \n",
       "7374                                        962       689          NaN   \n",
       "\n",
       "      Unnamed: 11  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "7370          NaN  \n",
       "7371          NaN  \n",
       "7372          NaN  \n",
       "7373          NaN  \n",
       "7374          NaN  \n",
       "\n",
       "[7375 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_trump_tweets = trump_data[trump_data['Tweet_Text'].notnull()]\n",
    "trump_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a573b9-dc55-442e-9534-22abb495182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Media_Type</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_Url</th>\n",
       "      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>15:26:37</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>text</td>\n",
       "      <td>photo</td>\n",
       "      <td>ThankAVet</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>127213</td>\n",
       "      <td>41112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>13:33:35</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>141527</td>\n",
       "      <td>28654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>11:14:20</td>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>183729</td>\n",
       "      <td>50039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:19:44</td>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>214001</td>\n",
       "      <td>67010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:10:46</td>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>178499</td>\n",
       "      <td>36688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>13:10:00</td>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>953</td>\n",
       "      <td>431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:18:31</td>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1175</td>\n",
       "      <td>1086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:10:17</td>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1494</td>\n",
       "      <td>930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>9:44:07</td>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1800</td>\n",
       "      <td>1738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>0:21:25</td>\n",
       "      <td>RT @marklevinshow: Trump: Rove is a clown and ...</td>\n",
       "      <td>link</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.210000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>962</td>\n",
       "      <td>689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7375 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time                                         Tweet_Text  \\\n",
       "0     16-11-11  15:26:37  Today we express our deepest gratitude to all ...   \n",
       "1     16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...   \n",
       "2     16-11-11  11:14:20  Love the fact that the small groups of protest...   \n",
       "3     16-11-11   2:19:44  Just had a very open and successful presidenti...   \n",
       "4     16-11-11   2:10:46  A fantastic day in D.C. Met with President Oba...   \n",
       "...        ...       ...                                                ...   \n",
       "7370  15-07-16  13:10:00  I loved firing goofball atheist Penn @pennjill...   \n",
       "7371  15-07-16  10:18:31  I hear @pennjillette show on Broadway is terri...   \n",
       "7372  15-07-16  10:10:17  Irrelevant clown @KarlRove sweats and shakes n...   \n",
       "7373  15-07-16   9:44:07  \"@HoustonWelder: Donald Trump is one of the se...   \n",
       "7374  15-07-16   0:21:25  RT @marklevinshow: Trump: Rove is a clown and ...   \n",
       "\n",
       "      Type Media_Type   Hashtags      Tweet_Id  \\\n",
       "0     text      photo  ThankAVet  7.970000e+17   \n",
       "1     text        NaN        NaN  7.970000e+17   \n",
       "2     text        NaN        NaN  7.970000e+17   \n",
       "3     text        NaN        NaN  7.970000e+17   \n",
       "4     text        NaN        NaN  7.970000e+17   \n",
       "...    ...        ...        ...           ...   \n",
       "7370  text        NaN        NaN  6.220000e+17   \n",
       "7371  text        NaN        NaN  6.220000e+17   \n",
       "7372  text        NaN        NaN  6.220000e+17   \n",
       "7373  text        NaN        NaN  6.220000e+17   \n",
       "7374  link        NaN        NaN  6.210000e+17   \n",
       "\n",
       "                                              Tweet_Url  \\\n",
       "0     https://twitter.com/realDonaldTrump/status/797...   \n",
       "1     https://twitter.com/realDonaldTrump/status/797...   \n",
       "2     https://twitter.com/realDonaldTrump/status/797...   \n",
       "3     https://twitter.com/realDonaldTrump/status/796...   \n",
       "4     https://twitter.com/realDonaldTrump/status/796...   \n",
       "...                                                 ...   \n",
       "7370  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7371  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7372  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7373  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7374  https://twitter.com/realDonaldTrump/status/621...   \n",
       "\n",
       "      twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  \\\n",
       "0                                        127213     41112          NaN   \n",
       "1                                        141527     28654          NaN   \n",
       "2                                        183729     50039          NaN   \n",
       "3                                        214001     67010          NaN   \n",
       "4                                        178499     36688          NaN   \n",
       "...                                         ...       ...          ...   \n",
       "7370                                        953       431          NaN   \n",
       "7371                                       1175      1086          NaN   \n",
       "7372                                       1494       930          NaN   \n",
       "7373                                       1800      1738          NaN   \n",
       "7374                                        962       689          NaN   \n",
       "\n",
       "      Unnamed: 11  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "7370          NaN  \n",
       "7371          NaN  \n",
       "7372          NaN  \n",
       "7373          NaN  \n",
       "7374          NaN  \n",
       "\n",
       "[7375 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets = trump_data['Tweet_Text'].to_list()\n",
    "trump_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c5a9a-8308-4a3e-95c9-99485cc3826d",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "- Clean and prepare (e.g. regex, nltk functionalities, etc.) the entire text information, in order to gurantee a robust sentence tokenization (e.g. how to handle hyperlinks, hashtags, punctuation marks, non-alphanumeric patterns, twitter references via @, upper/lower case, ...?)\n",
    "- Perform the entire preprocessing in the same way for all three data partitions to ensure comparability\n",
    "- Split the entire text information into a `training, validation, and unseen test dataset` (take the `first 6,375 lines for training`, the following `500 for validation`, and the remaining `500 for testing`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657172fc-c022-4b7f-8e54-c7e2935cb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "sent_patt = re.compile('(?<!\\.|\\!|\\?|\\:|\\;|\\s)\\w[.,:;!?]\\s+')\n",
    "multi_sym = re.compile(r'[!.,?=-]{2,}')\n",
    "time = re.compile(r'([0-2][0-9]:[0-5][0-9]([pm]|[am])*)|([0-2]*[0-9]*:*[0-5][0-9]([pm]|[am])+)|([0-9][0-9]*([pm]|[am])+)')\n",
    "date = re.compile(r'([0-3]*[0-9]\\/[0-9]*\\/[0-9]+)')\n",
    "\n",
    "for idx in range(len(trump_tweets)):\n",
    "    tweet = trump_tweets[idx]\n",
    "    \n",
    "    tweet = \" \"+tweet.lower()+\" \"\n",
    "    \n",
    "    tweet = tweet.replace(\"\\n\", \" \").replace(\"\\\"\", \"\").replace(\"“\", \"\").replace(\"”\", \"\").replace(\"|\", \" \").replace(\"`\", \" \").replace(\"'\", \" \").replace(\":_\", \" \").replace(\"_\", \" \").replace(\" rt \", \" retweet \").replace(\" mrs. \", \" mrs \").replace(\" ms. \", \" ms \").replace(\" mr. \", \" mr \").replace(\" dr. \", \" dr \").replace(\" prof. \", \" prof \"). replace(\" dr.-ing. \", \" dr.-ing \")  \n",
    "    tweet = re.sub(r'http\\S+', ' hrefl ', tweet) #links\n",
    "    tweet = re.sub(r'#\\S+', ' twhash ', tweet) #hashtag\n",
    "    tweet = re.sub(r'@\\S+', ' usacc ', tweet) #useraccount\n",
    "    \n",
    "    all_sym = multi_sym.finditer(tweet)\n",
    "    all_time = time.finditer(tweet)\n",
    "    all_date = date.finditer(tweet)\n",
    "    \n",
    "    for m in all_sym:\n",
    "        tweet = tweet.replace(m.group(), ' '+m.group()[0]+' ', 1)\n",
    "    for m in all_time:\n",
    "        tweet = tweet.replace(m.group(), ' tiform ', 1)\n",
    "    for m in all_date:\n",
    "        tweet = tweet.replace(m.group(), ' dtform ', 1)\n",
    "    tweet = \" \"+tweet+\" \"\n",
    "    all_pts = sent_patt.finditer(tweet)\n",
    "    for m in all_pts:\n",
    "        tweet = tweet.replace(m.group(), m.group()[0]+' '+m.group()[1]+' ', 1)\n",
    "\n",
    "    tweet = re.sub(\"\\s\\s+\", \" \", tweet).lower()\n",
    "    trump_tweets[idx] = tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97544409-f336-40c6-8813-ceb34d59bf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 6375\n",
      "Val Size: 500\n",
      "Test Size: 500\n"
     ]
    }
   ],
   "source": [
    "train = trump_tweets[0:6375]\n",
    "val = trump_tweets[6375:6875]\n",
    "test = trump_tweets[6875:]\n",
    "\n",
    "print(\"Train Size:\", len(train))\n",
    "print(\"Val Size:\", len(val))\n",
    "print(\"Test Size:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff701ce-0bd5-4e8e-8f4f-5ae9fcd67465",
   "metadata": {},
   "source": [
    "## Sentence Tokenization and Padding\n",
    "- Join all the tweet text information and perform sentence tokenization\n",
    "- Report the number of sentences\n",
    "- Use the list of detected and individual sentences and either consistently remove any type of punctuation marks, or leave them in the original corpus and treat them as individual words  \n",
    "- Integrate the required sentence start (`<s>`) and sentence end (`</s>`) for each sentence in the list (`N-Gram order = 3`)\n",
    "- Convert the list of sentences (list of strings) into a nested list, describing a list of sentences, while each sentence is represented as list of words (word vector $\\vec{w}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5bd032-cfcf-420e-b7a6-7ea28d8c6490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Training= 113740\n",
      "Number Sentences in Training= 12623\n",
      "Average Number Words per Sentence in Training= 9.010536322585756\n",
      "\n",
      "Words in Validation= 8831\n",
      "Number Sentences in Validation= 954\n",
      "Average Number Words per Sentence in Validation= 9.256813417190775\n",
      "\n",
      "Words in Test= 8687\n",
      "Number Sentences in Test= 918\n",
      "Average Number Words per Sentence in Test= 9.462962962962964\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.lm.preprocessing import pad_both_ends \n",
    "\n",
    "def sentenize(partition_list):\n",
    "    #Training\n",
    "    num_words_partition = 0\n",
    "    sentences_partition = []\n",
    "    for partition_tweet in partition_list:\n",
    "        tweet_proc = partition_tweet.replace(\" !\", \" .\").replace(\" ?\", \" .\")\n",
    "        sent_part = tweet_proc.split(\" .\")\n",
    "        sent_part = list(filter(None, sent_part))\n",
    "        for sent in sent_part:\n",
    "            words = sent.strip().split(\" \")\n",
    "            sentences_partition.append(words)\n",
    "            num_words_partition += len(words)\n",
    "            \n",
    "    return sentences_partition, num_words_partition\n",
    "            \n",
    "#Training\n",
    "sentences_train, num_words_train = sentenize(train)\n",
    "\n",
    "print(\"Words in Training=\", num_words_train)\n",
    "print(\"Number Sentences in Training=\", len(sentences_train))\n",
    "print(\"Average Number Words per Sentence in Training=\", num_words_train/len(sentences_train))\n",
    "print()\n",
    "\n",
    "#Validation\n",
    "sentences_val, num_words_val = sentenize(val)\n",
    "\n",
    "print(\"Words in Validation=\", num_words_val)\n",
    "print(\"Number Sentences in Validation=\", len(sentences_val))\n",
    "print(\"Average Number Words per Sentence in Validation=\", num_words_val/len(sentences_val))\n",
    "print()\n",
    "\n",
    "#Test\n",
    "sentences_test, num_words_test = sentenize(test)\n",
    "\n",
    "print(\"Words in Test=\", num_words_test)\n",
    "print(\"Number Sentences in Test=\", len(sentences_test))\n",
    "print(\"Average Number Words per Sentence in Test=\", num_words_test/len(sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7db7707-c1bd-4f76-9762-32a274d587de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(sentences_dpart):\n",
    "    padded_dpart_sentences = []\n",
    "    for dpart_sentence in sentences_dpart:\n",
    "        padded_sentence = list(pad_both_ends(dpart_sentence, 3))\n",
    "        padded_dpart_sentences.append(padded_sentence)\n",
    "    return padded_dpart_sentences\n",
    "    \n",
    "padded_train_tweets = pad_sentence(sentences_train)\n",
    "padded_val_tweets = pad_sentence(sentences_val)\n",
    "padded_test_tweets = pad_sentence(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce5a14-08cb-4907-94d0-e0a6e9e48db5",
   "metadata": {},
   "source": [
    "## Generate Vocabulary\n",
    "\n",
    "- Use the entire padded sentence information (nested list), in order to provide one single list object with all the words (use the `flatten` function from `nltk.lm.preprocessing`)\n",
    "- Create the vocabulary for our language model, however each word, which does not show up more than `N times (N=1, N=2, ...)` (what happens if `N > 1`?) within the entire corpus, should not be part of the vocabulary and mapped to the `<UNK>` category/tag (use the `unk_cutoff` option)\n",
    "- Use the `Vocabulary` object of `nltk.lm` to realize the requirements\n",
    "- Report the size of your entire vocabulary (`|V|`) as well as the number of unique elements, togehter with the top-N most-frequent elements (without considering sentence start `<s>` and `</s>`) - What can you observe regarding the type of words which are the most frequent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ecd4d6-f365-49b3-ac0e-59712e49db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "train_words = list(flatten(padded_train_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca85811-ff66-4aa5-b1ee-5d48328d26e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the Vocabulary= 8548\n",
      "Total number of words in the Vocabulary= 164232\n"
     ]
    }
   ],
   "source": [
    "train_vocabulary = Vocabulary(train_words, unk_cutoff=1)\n",
    "print(\"Number of unique words in the Vocabulary=\", len(train_vocabulary))\n",
    "number_words_vocabulary = sum(train_vocabulary.counts.values())\n",
    "print(\"Total number of words in the Vocabulary=\", number_words_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4708dc55-5e27-4c58-9093-e978fd4d45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 25246), ('</s>', 25246), ('usacc', 6198), ('the', 3343), (',', 3123), ('hrefl', 2830), ('twhash', 2820), ('to', 2136), ('in', 1685), ('a', 1665), ('and', 1619), ('is', 1527), ('you', 1525), ('i', 1518), ('of', 1352), ('on', 1247), ('for', 1186), ('will', 1170), ('be', 946), ('great', 915), ('trump', 911), ('thank', 827), (':', 769), ('-', 671), ('at', 650), ('that', 642), ('we', 606), ('it', 582), ('with', 580), ('are', 550), (';', 544), ('hillary', 519), ('&amp', 517), ('me', 512), ('have', 484), ('he', 482), ('my', 465), ('so', 448), ('just', 444), ('not', 439), ('all', 438), ('this', 428), ('was', 424), ('america', 417), ('by', 377), ('people', 375), ('new', 363), ('has', 360), ('our', 348), ('out', 339), ('from', 311), ('your', 305), ('make', 302), ('clinton', 296), ('very', 294), ('tiform', 294), ('they', 288), ('poll', 284), ('no', 283), ('again', 282), ('about', 277), ('his', 277), ('.', 277), ('retweet', 272), ('now', 271), ('who', 270), ('get', 268), ('as', 263), ('donald', 249), ('big', 240), ('do', 236), ('president', 233), ('vote', 233), ('she', 225), ('more', 223), ('crooked', 222), ('like', 219), ('cruz', 217), ('but', 216), ('am', 214), ('her', 212), ('tonight', 211), ('one', 200), ('time', 199), ('what', 197), ('going', 195), ('up', 191), ('if', 191), ('only', 188), ('can', 187), ('join', 186), ('would', 183), ('debate', 182), ('last', 180), ('an', 170), ('many', 168), ('today', 166), ('said', 165), ('good', 159), ('watch', 158), ('why', 158), ('much', 157), ('country', 156), ('win', 156), ('night', 155), ('never', 155), ('back', 154), ('how', 154), ('enjoy', 153), ('when', 152), ('been', 151), ('should', 150), ('him', 149), ('than', 144), ('bad', 144), ('iowa', 141), ('tomorrow', 140), ('rubio', 140), ('media', 139), ('ted', 136), ('love', 135), ('see', 135), ('had', 133), ('want', 133), ('interviewed', 132), ('or', 131), ('dont', 130), ('support', 130), ('wow', 129), ('polls', 127), ('campaign', 127), ('nice', 126), ('us', 125), ('were', 125)]\n"
     ]
    }
   ],
   "source": [
    "most_common = train_vocabulary.counts.most_common(n=134)\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212c516-a1f7-4bb2-b196-5210c17f9e78",
   "metadata": {},
   "source": [
    "## Compute N-Grams\n",
    "\n",
    "- Use the padded sentence information and compute for each sentence, represented as word vector $\\vec{w}$ (list of words), all the N-Gram patterns, using the `everygrams` function from `nltk` (total N-Gram information should include `Unigram, Bigram, and Trigram` - `N=3` )\n",
    "- Analyze the output (`generator object`) for each sentence and convert it to a list object\n",
    "- Appending all the sentence-wise list output with all the N-Gram information to another list, representing your final training data\n",
    "- Compute the counts for all the Unigrams, Bigrams, and Trigrams (use the `Counter` module from `collections` together with the list-converted output of `everygrams`)\n",
    "- Visualize the `top-N` most-frequent Unigrams, Bigrams, and Trigrams (use `matplotlib` barplot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3104cfe9-5ad2-40e3-bd25-7c2a1b5457d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import everygrams\n",
    "\n",
    "def compute_n_grams(padded_datapart_sentences, minN, maxN):\n",
    "    ngram_counts_dpart = None\n",
    "    everygram_padded_dpart_tweets = []\n",
    "    for padded_dpart_tweets in padded_datapart_sentences:\n",
    "        everygram_dpart = list(everygrams(padded_dpart_tweets, min_len=minN, max_len=maxN))\n",
    "        everygram_padded_dpart_tweets.append(everygram_dpart)\n",
    "        if ngram_counts_dpart is None:\n",
    "            ngram_counts_dpart = Counter(everygram_dpart)\n",
    "        else:\n",
    "            ngram_counts_dpart += Counter(everygram_dpart)\n",
    "    \n",
    "    return ngram_counts_dpart, everygram_padded_dpart_tweets\n",
    "\n",
    "ngram_counts_train, everygram_padded_train_tweets = compute_n_grams(padded_train_tweets, 1, 3)\n",
    "ngram_counts_val, everygram_padded_val_tweets = compute_n_grams(padded_val_tweets, 1, 3)\n",
    "ngram_counts_test, everygram_padded_test_tweets = compute_n_grams(padded_test_tweets, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3e403-2cfe-44ed-8938-5643f1278c4d",
   "metadata": {},
   "source": [
    "## Build N-Gram Language Model\n",
    "\n",
    "- Create a N-Gram language model using the `MLE` (Maximum-Likelihood-Estimator) module from `nltk.lm`\n",
    "- Train the model calling the `fit` function, which requires two mandatory arguments - the (nested) sentence-related list with all N-Grams per sentence (see result of Section `Compute N-Grams`), in addition to the vocabulary (see result of Section `Generate Vocabulary`)\n",
    "- Analyze your trained model, using different functionalities, e.g. `counts`, `scores`, `logscore`, `vocab`, together with different words-sequences, also including words which are not in the vocabulary, in order to make sure that the model outputs are valid\n",
    "- Manually evaluate/verify probabilities for `Unigram`, `Bigram`, and `Trigram` (in case of the `MLE` model) for the following expression: `\"make america great\"` - Does it match with the `model.counts` and `model.score` values? What happens in the special case of the bigram probability for `<s> <s>` and `</s> </s>` ? Are those counts and score values the same - explain what you are observing?\n",
    "- There exist also more sophisticated language models, such as `KneserNeyInterpolated`, `Laplace`, `Lidstone`, `WittenBellInterpolated`, `AbsoluteDiscountingInterpolated` (see https://www.nltk.org/api/nltk.lm.html)\n",
    "- Train also a `KneserNeyInterpolated` language model. What do you encounter when computing probabilities, compared to the `MLE` version (key word: smoothing)\n",
    "- **Homework:** also have a look at the other LM alternatives (mentioned before), as well as compare results! Have also a look at the different `smoothing` options provided by the `nltk.lm` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d2ea2d-ba1d-465a-9e61-05d1ba08b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE, KneserNeyInterpolated, Laplace, Lidstone, AbsoluteDiscountingInterpolated, WittenBellInterpolated\n",
    "model = MLE(3) #other LM options are possible here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7af51e2-5127-41c4-8419-2aacb33a0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(everygram_padded_train_tweets, train_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "979bf55d-fae7-44c8-aaa2-07623eb346dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of unique words is the vocabulary= <Vocabulary with cutoff=1 unk_label='<UNK>' and 8549 items>\n",
      "Total number of N-Grams= <NgramCounter with 3 ngram orders and 454827 ngrams>\n",
      "--- Check if the sentence is within the vocabulary, otherwise replace by <UNK> ---\n",
      "('<UNK>', 'is', 'in', 'america', 'also', 'a', 'common', 'thing', 'god', 'bless', 'america')\n",
      "\n",
      "Number of occurrences of the word \"make\" within the training dataset= 302\n",
      "Number of occurrences of the word \"america\" within the training dataset= 417\n",
      "Number of occurrences of the word \"great\" within the training dataset= 915\n",
      "\n",
      "Number of occurrences of the word \"<s>\" within the training dataset= 25246\n",
      "Number of occurrences of the word \"</s>\" within the training dataset= 25246\n",
      "\n",
      "Number of occurrences of the word \"make america\" within the training dataset= 214\n",
      "Number of occurrences of the word \"america great\" within the training dataset= 179\n",
      "Number of occurrences of the word \"make america great\" within the training dataset= 175\n",
      "\n",
      "Number of occurrences of the word \"<s> <s>\" within the training dataset= 12623\n",
      "Number of occurrences of the word \"</s> </s>\" within the training dataset= 12623\n",
      "\n",
      "Total number of words in the training dataset (|V|)= 164232\n",
      "\n",
      "Probability of observing the word \"make\" in the training data= 0.0018388620975205807\n",
      "Probability of observing the word \"america\" in the training data= 0.002539091041940669\n",
      "Probability of observing the word \"great\" in the training data= 0.005571386818646792\n",
      "\n",
      "Probability of observing the bigram \"make america\" in the training data= 0.7086092715231788\n",
      "Probability of observing the bigram \"america great\" in the training data= 0.4292565947242206\n",
      "\n",
      "Probability of observing the trigram \"make america great\" in the training data= 0.8177570093457944\n",
      "\n",
      "Probability of observing the bigram \"<s> <s>\" in the training data= 0.5\n",
      "Probability of observing the bigram \"</s> </s>\" in the training data= 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of unique words is the vocabulary=\", model.vocab)\n",
    "print(\"Total number of N-Grams=\", model.counts)\n",
    "print(\"--- Check if the sentence is within the vocabulary, otherwise replace by <UNK> ---\")\n",
    "print(model.vocab.lookup('nlp is in america also a common thing god bless america'.split()))\n",
    "print()\n",
    "print(\"Number of occurrences of the word \\\"make\\\" within the training dataset=\", model.counts[\"make\"])\n",
    "print(\"Number of occurrences of the word \\\"america\\\" within the training dataset=\", model.counts[\"america\"])\n",
    "print(\"Number of occurrences of the word \\\"great\\\" within the training dataset=\", model.counts[\"great\"])\n",
    "print()\n",
    "print(\"Number of occurrences of the word \\\"<s>\\\" within the training dataset=\", model.counts[\"<s>\"])\n",
    "print(\"Number of occurrences of the word \\\"</s>\\\" within the training dataset=\", model.counts[\"</s>\"])\n",
    "print()\n",
    "print(\"Number of occurrences of the word \\\"make america\\\" within the training dataset=\", model.counts[['make']]['america'])\n",
    "print(\"Number of occurrences of the word \\\"america great\\\" within the training dataset=\", model.counts[['america']]['great'])\n",
    "print(\"Number of occurrences of the word \\\"make america great\\\" within the training dataset=\", model.counts[['make', 'america']]['great'])\n",
    "print()\n",
    "print(\"Number of occurrences of the word \\\"<s> <s>\\\" within the training dataset=\", model.counts[['<s>']]['<s>'])\n",
    "print(\"Number of occurrences of the word \\\"</s> </s>\\\" within the training dataset=\", model.counts[['</s>']]['</s>'])\n",
    "print()\n",
    "print(\"Total number of words in the training dataset (|V|)=\", number_words_vocabulary)\n",
    "print()\n",
    "print(\"Probability of observing the word \\\"make\\\" in the training data=\", model.score(\"make\"))\n",
    "print(\"Probability of observing the word \\\"america\\\" in the training data=\", model.score(\"america\"))\n",
    "print(\"Probability of observing the word \\\"great\\\" in the training data=\", model.score(\"great\"))\n",
    "print()\n",
    "print(\"Probability of observing the bigram \\\"make america\\\" in the training data=\", model.score('america', ['make']))\n",
    "print(\"Probability of observing the bigram \\\"america great\\\" in the training data=\", model.score('great', ['america']))\n",
    "print()\n",
    "print(\"Probability of observing the trigram \\\"make america great\\\" in the training data=\", model.score('great', 'make america'.split()))\n",
    "print()\n",
    "print(\"Probability of observing the bigram \\\"<s> <s>\\\" in the training data=\", model.score('<s>', ('<s>',)))\n",
    "print(\"Probability of observing the bigram \\\"</s> </s>\\\" in the training data=\", model.score('</s>', ('</s>',)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b23f48-54ad-4b2a-ac67-7726efa125ce",
   "metadata": {},
   "source": [
    "## Evaluation of the Language Model \n",
    "\n",
    "- First apply the trained language model and compute the `perplexity` for the `unigram \"america\"`. Compare if the perplexity is really the inverse probability which has been calculated before via `model.score`. Moreover take the sentence `<s> <s> make america great again </s> </s>` and convert all the bigrams within a list `[('<s>', '<s>'), ('<s>', 'make'), ...]`, which is then used as input for the `perplexity` computation. \n",
    "- Second, use the trained language model together with your `validation set` and compute the `perplexity` for each sentence in the validation set using the `everygram` output (compute total perplexity and averaged across all sentences)\n",
    "- In case the performance is not very promising and/or you are observing a lot of `inf` values (division by zero in the perplexity equation) try to have a look at your pipeline and try to further optimize the text preprocessing and parametric setup (reduce the complexity of the vocabualry, e.g. increase `unk_cutoff`, use categorical approaches (e.g. `NER`), change the size of `N`, use a LM with an integrated smoothing concept (e.g. `Laplace`, etc.), besides looking for any other strange behaviors\n",
    "- Once the performance on the validation corpus is satisfying, verify your model also on the final and unseen `test set` (via `perplexity` and same approach) - Large deviations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27cc95df-bc48-4478-9935-4ee821e30de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393.84172661870537\n",
      "3.826439252653616\n",
      "\n",
      "Perplexity Bigram= 2.0\n",
      "Perplexity Bigram= 345.83561643835645\n",
      "Perplexity Bigram= 1.411214953271028\n",
      "Perplexity Bigram= 2.3296089385474863\n",
      "Perplexity Bigram= 4.420289855072464\n",
      "Perplexity Bigram= 1.194915254237288\n",
      "Perplexity Bigram= 1.0\n",
      "\n",
      "Averaged Bigram-Perplexity: 51.17023506278353\n"
     ]
    }
   ],
   "source": [
    "unigram = [(\"america\",)]\n",
    "bigrams  = [('<s>', '<s>'), ('<s>', 'make'), ('make', 'america'), ('america', 'great'), ('great', 'again'), ('again', '</s>'), ('</s>', '</s>')]\n",
    "\n",
    "print(model.perplexity(unigram))\n",
    "print(model.perplexity(bigrams))\n",
    "print()\n",
    "\n",
    "bigram = 0\n",
    "for bi in bigrams:\n",
    "    print(\"Perplexity Bigram=\",model.perplexity([bi]))\n",
    "    bigram += model.perplexity([bi])\n",
    "print()\n",
    "print(\"Averaged Bigram-Perplexity:\", bigram/len(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6c52460-0139-4082-be95-d1991d6caf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------VALIDATION SET--------------------------\n",
      "Number of known/non-zero Unigrams/Probabilities= 12184\n",
      "Number of known/non-zero Bigrams/Probabilities= 8536\n",
      "Number of known/non-zero Trigrams/Probabilities= 4624\n",
      "\n",
      "Perplexity Unigram-Level (Total-Uni=12647, Valid-Uni=12184)= 6413.056553918828\n",
      "Perplexity Bigram-Level (Total-Bi=11693, Valid-Bi=8536)= 264.1471346939859\n",
      "Perplexity Trigram-Level (Total-Tri=10739, Valid-Tri=4624)= 143.12515433141965\n",
      "\n",
      "\n",
      "--------------------TEST SET--------------------------\n",
      "Number of known/non-zero Unigrams/Probabilities= 11862\n",
      "Number of known/non-zero Bigrams/Probabilities= 8073\n",
      "Number of known/non-zero Trigrams/Probabilities= 4249\n",
      "\n",
      "Perplexity Unigram-Level (Total-Uni=12359, Valid-Uni=11862)= 6289.107093400637\n",
      "Perplexity Bigram-Level (Total-Bi=11441, Valid-Bi=8073)= 298.82240277636356\n",
      "Perplexity Trigram-Level (Total-Tri=10523, Valid-Tri=4249)= 172.91214269608065\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_perplexity(everygram_datadist):\n",
    "    total_uni = 0\n",
    "    total_bi = 0\n",
    "    total_tri = 0\n",
    "    \n",
    "    val_uni = 0\n",
    "    val_bi = 0\n",
    "    val_tri = 0\n",
    "    \n",
    "    unigram = 0\n",
    "    bigram = 0\n",
    "    trigram = 0  \n",
    "    \n",
    "    for grams in everygram_datadist:\n",
    "        \n",
    "        for idx in range(len(grams)):\n",
    "            \n",
    "            perplex = model.perplexity([grams[idx]])\n",
    "            \n",
    "            if len(grams[idx]) == 1:\n",
    "                total_uni += 1\n",
    "                if not str(perplex).lower() == \"inf\":\n",
    "                    unigram += perplex\n",
    "                    val_uni += 1\n",
    "            elif len(grams[idx]) == 2:\n",
    "                total_bi += 1\n",
    "                if not str(perplex).lower() == \"inf\":\n",
    "                    bigram += perplex\n",
    "                    val_bi += 1\n",
    "            elif len(grams[idx]) == 3:\n",
    "                total_tri += 1\n",
    "                if not str(perplex).lower() == \"inf\":\n",
    "                    trigram += perplex\n",
    "                    val_tri += 1\n",
    "                    \n",
    "    return unigram, bigram, trigram, val_uni, val_bi, val_tri, total_uni, total_bi, total_tri\n",
    "\n",
    "v_unigram, v_bigram, v_trigram, v_val_uni, v_val_bi, v_val_tri, v_total_uni, v_total_bi, v_total_tri = get_perplexity(everygram_padded_val_tweets)\n",
    "t_unigram, t_bigram, t_trigram, t_val_uni, t_val_bi, t_val_tri, t_total_uni, t_total_bi, t_total_tri = get_perplexity(everygram_padded_test_tweets)\n",
    "\n",
    "print(\"--------------------VALIDATION SET--------------------------\")\n",
    "print(\"Number of known/non-zero Unigrams/Probabilities=\", v_val_uni)\n",
    "print(\"Number of known/non-zero Bigrams/Probabilities=\", v_val_bi)\n",
    "print(\"Number of known/non-zero Trigrams/Probabilities=\", v_val_tri)\n",
    "print()\n",
    "print(\"Perplexity Unigram-Level (Total-Uni=\"+str(v_total_uni)+\", Valid-Uni=\"+str(v_val_uni)+\")=\", v_unigram/v_val_uni)\n",
    "print(\"Perplexity Bigram-Level (Total-Bi=\"+str(v_total_bi)+\", Valid-Bi=\"+str(v_val_bi)+\")=\", v_bigram/v_val_bi)\n",
    "print(\"Perplexity Trigram-Level (Total-Tri=\"+str(v_total_tri)+\", Valid-Tri=\"+str(v_val_tri)+\")=\", v_trigram/v_val_tri)\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"--------------------TEST SET--------------------------\")\n",
    "print(\"Number of known/non-zero Unigrams/Probabilities=\", t_val_uni)\n",
    "print(\"Number of known/non-zero Bigrams/Probabilities=\", t_val_bi)\n",
    "print(\"Number of known/non-zero Trigrams/Probabilities=\", t_val_tri)\n",
    "print()\n",
    "print(\"Perplexity Unigram-Level (Total-Uni=\"+str(t_total_uni)+\", Valid-Uni=\"+str(t_val_uni)+\")=\", t_unigram/t_val_uni)\n",
    "print(\"Perplexity Bigram-Level (Total-Bi=\"+str(t_total_bi)+\", Valid-Bi=\"+str(t_val_bi)+\")=\", t_bigram/t_val_bi)\n",
    "print(\"Perplexity Trigram-Level (Total-Tri=\"+str(t_total_tri)+\", Valid-Tri=\"+str(t_val_tri)+\")=\", t_trigram/t_val_tri)\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1028d-b3a4-44e4-b1dc-17d9197c840d",
   "metadata": {},
   "source": [
    "## Artificial Text Generation\n",
    "- Use the `generate` functionality together with the `MLE` language model alternative to artificially produce Trump-Tweets via a user-defined number of words   \n",
    "- You might face a lot of sentence start `<s>` and sentence end `</s>` tokens. Optimize the generated output, by ignoring and eliminating these tokens and produce sentences of at least 15 words during generation (without `<s>` and `</s>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd86a13-f962-4b31-aed3-31f59dac5e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'make']\n",
      "['usacc', 'were', 'ready', 'to', 'make', 'america', 'great', 'again', '</s>', '</s>']\n",
      "['<s>', 'usacc', 'steam', ':', 'as', 'a', 'paragon', 'of', 'virtue', 'just', 'shows', 'how', 'weak', 'and', 'open-and']\n"
     ]
    }
   ],
   "source": [
    "generated_text_example_1 = model.generate(3, random_seed=4)\n",
    "generated_text_example_2 = model.generate(10, random_seed=23)\n",
    "generated_text_example_3 = model.generate(15, random_seed=154)\n",
    "\n",
    "print(generated_text_example_1)\n",
    "print(generated_text_example_2)\n",
    "print(generated_text_example_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1dff871-4a85-448f-a5d5-bbaef3761ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "generated_result = []\n",
    "while True:\n",
    "    rand_seed = random.randint(1, 1000)\n",
    "    init_gen = model.generate(40, random_seed=rand_seed)\n",
    "    for token in init_gen:\n",
    "        if token == '<s>' or token == '</s>':\n",
    "            continue\n",
    "        generated_result.append(token)\n",
    "\n",
    "    if len(generated_result) <= 15:\n",
    "        generated_result = []\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f91fe3f3-b13f-47c9-a1e4-d9bb3f55e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'the', 'fantastic', 'ratings', 'last', 'weekend', ',', 'usacc', 'of', 'course', 'there', 'is', 'nobody', 'more', 'against', 'obamacare', 'than', 'me']\n"
     ]
    }
   ],
   "source": [
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38484c0-b994-4f90-9d6d-5a47fd7a7c46",
   "metadata": {},
   "source": [
    "## Save and Store your final Model\n",
    "- Store the trained and final language model by pickling the output using the `dill` Python module\n",
    "- Call the `dump` function to write the model to a specific output path\n",
    "- Call the `load` method to load your model, based on a given input path (verfiy the model after loading!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bde955f-a4e8-43e5-81eb-cde003ffa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('My-NGram-LM.pkl', 'wb') as fout:\n",
    "    pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dff75c87-635e-45e1-8591-2d2457144558",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('My-NGram-LM.pkl', 'rb') as fin:\n",
    "    model_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddbdf77a-ed0a-4d0b-a7e8-1b289dc3bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 8549 items>\n",
      "<NgramCounter with 3 ngram orders and 454827 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab)\n",
    "print(model.counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
