{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8bb3d0",
   "metadata": {},
   "source": [
    "# Pytorch Example Problem\n",
    "\n",
    "- We will build our first regression problem on the California housing dataset. The dataset contains the house prizes (median within a block) in California from the 1990 census together with some parameters of the housing block. The machine learning task will be to predict the median house value based on the other parameters, which thus serve as features\n",
    "\n",
    "- Before we dive into `Pytorch` - here are some helpful tutorials:\n",
    "    - `Introduction`: https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html, https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "    - `Training`: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html, https://pytorch.org/tutorials/beginner/pytorch_with_examples.html, https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "    - `Datasets, Dataloaders`: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    - `Layer`:  https://pytorch.org/docs/stable/nn.html\n",
    "    - `Loss`: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "    - `Optimizers`: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "- California Housing Dataset: Information concerning the demography (income, population, house occupancy) in the respective districts, together with the location of the districts (latitude, longitude), and additional general kn regarding the house in the districts (number of rooms and bedrooms, age of the house). These statistics are based on a specific district, why they are reported as average or median values.\n",
    "  \n",
    "    - Use the `housing.csv` file (available here: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html or https://www.kaggle.com/datasets/camnugent/california-housing-prices?select=housing.csv)\n",
    "    - See also the short description(s) (here: https://www.kaggle.com/datasets/camnugent/california-housing-prices?select=housing.csv and https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
    "\n",
    "- Get familiar with `Pytorch datasets` and `dataloaders`: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "- Do some exploratory data analysis (`descriptive statistics`) on the dataset (`histogram` of every column, `correlation`, `scatter-matrix`)\n",
    "\n",
    "- Prepare your dataset for machine learning. You will define a `Neural Network` in `Pytorch`, please create appropriate `PyTorch` `datasets` and `dataloaders`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a52a77",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "- Read in `housing.csv` via `pandas` and ignore/remove the last column (`ocean_proximity`) for further processing\n",
    "- Create `histogram` information for each column\n",
    "- Check for `NaN, None, NaT` values in the entire data corpus and remove data entries which are affected\n",
    "- Compute pairwise correlation of columns via `corr()`\n",
    "- Create a `scatter-matrix` - Interpret the results! What can be observed, e.g. population size and number of bedrooms - Interpret the results!\n",
    "- Create individual `column`-specific `scatter` plots (`x-axis` = number of data samples, `y-axis` = value range) - Interpret the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10501810808049d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('housing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4987fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b57077",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Paarweise Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4822cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=['Population', 'TotalBedrooms', 'TotalRooms', 'MedianIncome', 'MedianHouseValue']\n",
    "pd.plotting.scatter_matrix(df[selected_columns], figsize=(12,10))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eba9bfc7-c501-454e-93a3-4e763ed6c020",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "- **Major Goal:** predicting house values (`MedianHouseValue`) based on all the other given `variables/features` (`Longitude, Latitude, HousingMedianAge, TotalRooms, TotalBedrooms, Population, Households, MedianIncome`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc330416-f74e-4b29-8627-2bce5f3062cc",
   "metadata": {},
   "source": [
    "### Prepare Data: Create Datasets and Dataloaders\n",
    "\n",
    "- Split your data into `training`, `validation`, and `test` via the `PyTorch` function `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9195d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#ground truth vs. features\n",
    "ycol = 'MedianHouseValue'\n",
    "X = df.drop(columns=[ycol])\n",
    "y = df[ycol]\n",
    "\n",
    "#train - test\n",
    "Xtr_, Xte, ytr_, yte = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "#train - validation\n",
    "Xtr, Xval, ytr, yval = train_test_split(Xtr_, ytr_, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea50ea-0ce8-42ae-97df-d2c0459dfb69",
   "metadata": {},
   "source": [
    "- Build your own `Custom Dataset` (inherit `from torch.utils.data import Dataset`)\n",
    "- Provide an option (`scalers`) to normalize your data (`max val=1` and `min val=0`) - Use `MiinMaxScaler` of `sklearn.preprocessing`\n",
    "- Provide also two additional options including `transform` and `target_transform`, representing a list of sequentially applied functions in order to compute different types of user-specific transformations (e.g. `transform = [functionA(paraA,...), functionB(paraB,...), functionB(paraC,...), etc.]`)\n",
    "- Implement/Overwrite the system call methods (`magic methods`) of `__len__` and `__getitem__` - Which operations trigger an underlying function call of those methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d909303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256eeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, X, y, scalers=None, scale=True, transform=None, target_transform=None):\n",
    "        if scale:\n",
    "            if scalers is None:\n",
    "                self.mmX = MinMaxScaler()\n",
    "                self.mmy = MinMaxScaler()\n",
    "                _X = self.mmX.fit_transform(X.values)\n",
    "                _y = self.mmy.fit_transform(y.values.reshape(-1, 1))\n",
    "            else:\n",
    "                self.mmX = scalers[0]\n",
    "                self.mmy = scalers[1]\n",
    "                _X = self.mmX.transform(X.values)\n",
    "                _y = self.mmy.transform(y.values.reshape(-1, 1))\n",
    "        else:\n",
    "            # no scaling\n",
    "            _X = X.values\n",
    "            _y = y.values\n",
    "            \n",
    "        self.X = torch.from_numpy(_X).to(torch.float32)\n",
    "        self.y = torch.from_numpy(_y).to(torch.float32)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.X[idx,:]\n",
    "        target = self.y[idx]\n",
    "        \n",
    "        return features, target\n",
    "    \n",
    "    def get_scalers(self):\n",
    "        return self.mmX, self.mmy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da1cea-4fec-4461-a014-8a01053c01a9",
   "metadata": {},
   "source": [
    "- Instantiate your `training, validation, and testing` dataset by calling the constructor of `MyCustomDataset` (`scale=True`, `scalers=None` only for `training` dataset)\n",
    "- Get and store both scalers `min/max` of the `training set` in two variables (for later processing) and use them for the `scalers` option of your validation and test set\n",
    "- Report the size of a single input feature vector `x` as well as output feature vector `y`\n",
    "- Report the overall size of your `training, validation, and test dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyCustomDataset(Xtr,ytr, scalers=None, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_min_scaler, tr_max_scaler = dataset.get_scalers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6511f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, target = dataset.__getitem__(0)\n",
    "print(f\"Feature: {feature}\")\n",
    "print(f\"Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Anzahl Trainingsdaten: {len(Xtr)}\")\n",
    "print(f\"Anzahl Testdaten: {len(Xte)}\")\n",
    "print(f\"Anzahl Validierungsdaten: {len(Xval)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "503b6a03d4aa34dd"
  },
  {
   "cell_type": "markdown",
   "id": "b8a69e8e-7ee0-4293-84d3-d523c2b4dfc9",
   "metadata": {},
   "source": [
    "- Use the partition-specific `datasets` (`MyCustomDataset`) to build the corresponding `dataloaders` via `from torch.utils.data import DataLoader`\n",
    "- Choose a `batch-size` of `16` for `training, validation, and test`, together with the `shuffle=True` option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = MyCustomDataset(Xtr, ytr)\n",
    "validation_dataset = MyCustomDataset(Xval, yval)\n",
    "test_dataset = MyCustomDataset(Xte, yte)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e8f93-8232-4511-9e8e-820e631a9436",
   "metadata": {},
   "source": [
    "## Baseline System - Random Forest Regressor\n",
    "\n",
    "- Create a `RandomForestRegressor` using `sklearn.ensemble` (`n_estimators=100`) - Check the `Python` documentation what is addressed by the parameter `n_estimators`!\n",
    "- Train the `RandomForestRegressor` using your `training dataset` (seperate it first into `input feature list` and `ground truth list`)\n",
    "- Validate/Test the `RandomForestRegressor` using your `validation dataset` and `test dataset` (seperate it first into `input feature list` and `ground truth list`) - compute `inverse transform` before!\n",
    "- Careful: use the `min-max-scaler` to compute an `inverse_transform` of the `0/1-min-max` normalized values back to its original values before computing the final target metrics. Why re-transform?\n",
    "- Report the `r2_score` and `root mean squarred error` (`RMSE`) from the `sklearn_metrics` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73887b-93e8-4d05-8db2-84f9dbaff1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "#train\n",
    "XX, yy = train_dataset[:]\n",
    "\n",
    "rf.fit(np.array(XX), np.array(yy).ravel())\n",
    "\n",
    "#predict\n",
    "XXv, yyv = validation_dataset[:]\n",
    "XXt, yyt = test_dataset[:]\n",
    "\n",
    "yval_pred_rf = rf.predict(XXv.numpy())\n",
    "ytest_pred_rf = rf.predict(XXt.numpy())\n",
    "\n",
    "mmx, mmy = train_dataset.get_scalers()\n",
    "#evaluate val\n",
    "ypredv_rf = (mmy.inverse_transform(yval_pred_rf.reshape(-1, 1)).tolist())\n",
    "ypredv_rf = np.array(ypredv_rf).ravel()             \n",
    "\n",
    "ytruev_rf = (mmy.inverse_transform(yyv.numpy().reshape(-1, 1)).tolist())\n",
    "ytruev_rf = np.array(ytruev_rf).ravel()\n",
    "\n",
    "#evaluate test\n",
    "ypredt_rf = (mmy.inverse_transform(ytest_pred_rf.reshape(-1, 1)).tolist())\n",
    "ypredt_rf = np.array(ypredt_rf).ravel()             \n",
    "\n",
    "ytruet_rf = (mmy.inverse_transform(yyt.numpy().reshape(-1, 1)).tolist())\n",
    "ytruet_rf = np.array(ytruet_rf).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"Baseline System Random Forest Classifier (Validation Set):\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"R2 Random Forest\", r2_score(ytruev_rf, ypredv_rf))\n",
    "print (\"RMSE Random Forest\", mean_squared_error(ytruev_rf, ypredv_rf, squared=False))\n",
    "print()\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"Baseline System Random Forest Classifier (Test Set):\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"R2 Random Forest\", r2_score(ytruet_rf, ypredt_rf))\n",
    "print (\"RMSE Random Forest\", mean_squared_error(ytruet_rf, ypredt_rf, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6d506",
   "metadata": {},
   "source": [
    "## Neural Network Design\n",
    "\n",
    "- Design a neural network for this regression task. Familiarize with the key concepts and how these are implemented in Pytorch (see links for documentation and tutorials at the very beginning):\n",
    "    - Network and layers\n",
    "    - Nonlinearity (ReLU)\n",
    "    - Training and validation datasets\n",
    "    - Optimizer\n",
    "    - Loss function \n",
    "\n",
    "- Build a `CHNetwork` which inherits from `nn.Module` of `torch`\n",
    "- Defined and implement the `__init__` (constructor) and the `forward path`\n",
    "- Create a `MLP` (Multi-Layer-Perceptron) with an `input layer`, two `hidden layer`, and an `output layer` with the following dimensional relations/number of neurons: `8 to 512`, `512 to 64`, and `64 to 1`\n",
    "- After every of those `fully-connected` layer (except the last one, which projects the hidden representation to the final output neuron) a `ReLU` activation function is applied\n",
    "- Return the final and overall `feature output x` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e89688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CHNetwork(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CHNetwork, self).__init__()\n",
    "\n",
    "        hidden_size = [512, 64]\n",
    "\n",
    "        self.linear01 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.linear02 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear03 = nn.Linear(hidden_size[1], output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.relu(self.linear01(input))\n",
    "        x = torch.relu(self.linear02(x))\n",
    "        x = self.linear03(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaab438-c85d-4b81-a4d4-4046eb670656",
   "metadata": {},
   "source": [
    "## Training Routine Design\n",
    "\n",
    "- Implement an entire network training routine, including all of the required parametric options, such as:\n",
    "    - `_net` - Neural network model\n",
    "    - `_optimizer_fn`- Type of Optimizer\n",
    "    - `optim_params` - Parameter of Optimizer (learning rate, beta values, ...)\n",
    "    - `_scheduler_fn` - Scheduler for learning rate\n",
    "    - `scheduler_params` - Parameter of Scheduler (learning rate decay gamma, step-size for learning rate patience, verbose, ...)  \n",
    "    - `_loss_fn` - Loss Function\n",
    "    - `dataloader_tr` - Dataloader for training set\n",
    "    - `dataloader_val` - Dataloader for validation set\n",
    "    - `epochs`- Maximum number of training epochs\n",
    "    - `apply_scheduler` - Use learning rate scheduling\n",
    "    - `snapshot_inverval` - Number of epochs \n",
    "    - `snap_path` - Path where to store snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aea9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Routine to train the neural network.\n",
    "Args:\n",
    "    - _net - Neural network model\n",
    "    - _optimizer_fn- Type of Optimizer (torch.optim function)\n",
    "    - optim_params - Parameter of Optimizer (learning rate, beta values, ...)\n",
    "    - _scheduler_fn - Scheduler for learning rate\n",
    "    - scheduler_params - Parameter of Scheduler (learning rate decay gamma, step-size for learning rate patience, verbose, ...)  \n",
    "    - _loss_fn - Loss Function (torch.nn loss)\"cuda\"\n",
    "    - dataloader_tr - Dataloader for training set\n",
    "    - dataloader_val - Dataloader for validation set\n",
    "    - epochs - Maximum number of training epochs\n",
    "    - apply_scheduler (default: True) - Use learning rate scheduling\n",
    "    - snapshot_inverval (default: 300) - Number of epochs \n",
    "    - snap_path (default \"./\") - Path where to store snapshots\n",
    "\"\"\"\n",
    "\n",
    "def training(_net, \n",
    "             _optimizer_fn, \n",
    "             optim_params, \n",
    "             _scheduler_fn,\n",
    "             scheduler_params,\n",
    "             _loss_fn, \n",
    "             dataloader_tr, \n",
    "             dataloader_val, \n",
    "             epochs,\n",
    "             apply_scheduler=True,\n",
    "             snapshot_interval=300,\n",
    "             snap_path = \"./\"):\n",
    " \n",
    "    _optimizer = _optimizer_fn(_net.parameters(), **optim_params)\n",
    "    _scheduler = _scheduler_fn(_optimizer, **scheduler_params)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        _net.train()\n",
    "        running_tr_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(dataloader_tr):\n",
    "\n",
    "            _optimizer.zero_grad()\n",
    "            \n",
    "            inputs, targets = data\n",
    "            \n",
    "            yhats = _net(inputs)\n",
    "            \n",
    "            loss = _loss_fn(yhats, targets)\n",
    "            loss.backward()\n",
    "            _optimizer.step()\n",
    "            \n",
    "            if apply_scheduler:\n",
    "                _scheduler.step()\n",
    "            \n",
    "            running_tr_loss += loss.detach().numpy()\n",
    "            \n",
    "        running_tr_loss /= (i+1)\n",
    "\n",
    "        train_losses.append(running_tr_loss)\n",
    "        print(\"Training loss: \", running_tr_loss)\n",
    "        \n",
    "        running_val_loss = 0        \n",
    "        _net.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i, vdata in enumerate(dataloader_val):\n",
    "                vinputs, vtargets = vdata\n",
    "                vloss = _loss_fn(_net(vinputs), vtargets)\n",
    "                running_val_loss += vloss.detach().numpy()\n",
    "            \n",
    "            running_val_loss /= (i+1)\n",
    "            val_losses.append(running_val_loss)\n",
    "\n",
    "            print(\"Validation loss: \", running_val_loss)\n",
    "            \n",
    "        if epoch % snapshot_interval == 0:\n",
    "            filename = \"snapshot_%s\" % (int(epoch/snapshot_interval))\n",
    "            torch.save(_net.state_dict().copy(), os.path.join(snap_path, filename))\n",
    "\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4133d2-308e-4f94-a395-8098bba159d8",
   "metadata": {},
   "source": [
    "- Train your neural network and evaluate its performance on the validation set by calling the designed training routine `training`\n",
    "- Get the `number of input features` (size of the input layer)\n",
    "- Create a new model (`CHNetwork`) with the respective `input` and `output features size`\n",
    "- Create a loss function (optimization criterion), using `torch.nn`. Use the existing `MSELoss` with the `reduction` option `mean`\n",
    "- Create a `SGD` optimizer from `torch.optim`\n",
    "- Create a `Exponential Learning Rate Scheduler` using `torch.optim`\n",
    "- Define the `parameters` as a dictionary for the `optimizer` (initial `learning rate` of `0.1`) and `scheduler` (`gamma` of `0.99999` and `verbose` not activated)\n",
    "- Set the `maximum` number of `training epochs` to `500` (try also different values!)\n",
    "- Call the `training` method (training routine) with all the defined parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c77637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input features\n",
    "number_of_features = X.values.shape[1]\n",
    "\n",
    "#network\n",
    "my_net_def = CHNetwork(number_of_features, 1)\n",
    "\n",
    "#loss function\n",
    "my_loss_fn = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "#optimizer\n",
    "my_optimizer_fn = torch.optim.SGD\n",
    "\n",
    "#scheduler\n",
    "my_scheduler_fn = torch.optim.lr_scheduler.ExponentialLR\n",
    "\n",
    "#params\n",
    "optim_params = {\n",
    "        'lr': 0.1\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "        'gamma': 0.99999,\n",
    "        'verbose': False,\n",
    "}\n",
    "\n",
    "#training epochs maximum\n",
    "epochs_max = 500\n",
    "\n",
    "train_losses_ref, val_losses_ref = training(my_net_def, \n",
    "                                          my_optimizer_fn, \n",
    "                                          optim_params, \n",
    "                                          my_scheduler_fn,\n",
    "                                          scheduler_params,\n",
    "                                          my_loss_fn, \n",
    "                                          train_dataloader, \n",
    "                                          validation_dataloader, \n",
    "                                          epochs_max,\n",
    "                                          apply_scheduler=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069946e-5e54-4fe3-8d27-e37bd8d40395",
   "metadata": {},
   "source": [
    "## Evaluation and Interpretation\n",
    "\n",
    "- Define a function `get_metrics` using the model together with a given data partition to compute and return the `R2` and `RMSE`\n",
    "- Predict the `MedianHouseValue` for each sample (`feature vector x`) in the data partition (accumulate the `model output (prediction)` and `ground truth` for the entire dataset)\n",
    "- Careful: compute the `inverse_transform` using the `min-max-scaler` before accumlating `prediction` and `ground truth`\n",
    "- Compute/Return `R2` and `RMSE` based on the inverse transformed min-max prediction `ypred` and ground truth `ytrue`\n",
    "- Interpret the individual metrics - What is a `R2-Score` and `RMSE` and how the metric should change in order to improve the model quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88aaa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(net, validation_dataloader):\n",
    "    ytrue = []\n",
    "    ypred = []\n",
    "    net.eval()\n",
    "    for i, vdata in enumerate(validation_dataloader):\n",
    "        vinputs, vtargets = vdata\n",
    "        preds = net(vinputs)\n",
    "\n",
    "        ytrue += (mmy.inverse_transform(vtargets).tolist())\n",
    "        ypred += (mmy.inverse_transform(preds.detach().numpy()).tolist())\n",
    "\n",
    "    ytrue = np.array(ytrue).ravel()\n",
    "    ypred = np.array(ypred).ravel()\n",
    "\n",
    "    r2 = r2_score(ytrue, ypred)\n",
    "    rmse = mean_squared_error(ytrue, ypred, squared=False)\n",
    "\n",
    "    return r2, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f56a2",
   "metadata": {},
   "source": [
    "- Plot the training loss (`line plot`) using the final and overall `train loss` across all `epochs` from your training routine (`training`)\n",
    "- Plot the validation loss (`line plot`) in exactly the same way (using the `validation loss`)\n",
    "- Plot the validation loss (`line plot`) by computing the average across `n=10` windowed validation values (smoothing) via `pandas Series rolling`\n",
    "- Call the `get_metrics` function, using your trained model, in order to compute the `R2` and `RMSE` metrics (validaton and test). Compare it with the `Random Forest Regressor` result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_losses_ref)\n",
    "ax.set_title('Training loss over Epochs')\n",
    "ax.set_xlabel('Epoche')\n",
    "ax.set_ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a74b15-44d7-497c-8b2a-601167e91dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots()\n",
    "ax.plot(val_losses_ref)\n",
    "ax.set_title('Training loss over Epochs')\n",
    "ax.set_xlabel('Epoche')\n",
    "ax.set_ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a pandas Series from the validation loss values\n",
    "validation_loss_series = pd.Series(val_losses_ref)\n",
    "\n",
    "# Compute the rolling mean with a window size of n=10\n",
    "rolling_mean = validation_loss_series.rolling(window=10).mean()\n",
    "\n",
    "# Plot the original validation loss and the smoothed curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(validation_loss_series, label='Validation Loss', alpha=0.2)\n",
    "plt.plot(rolling_mean, label='Smoothed Validation Loss (n=10)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Validation Loss with Smoothing')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20103586ff9054ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r2, rmse = get_metrics(my_net_def, validation_dataloader)\n",
    "print(f'R2 auf Validation Dataset: {r2}')\n",
    "print(f'RMSE auf Validation Dataset: {rmse}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3e569a6afc78b88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r2, rmse = get_metrics(my_net_def, test_dataloader)\n",
    "print(f'R2 auf Validation Dataset: {r2}')\n",
    "print(f'RMSE auf Validation Dataset: {rmse}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76e3f49afb1d141f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print (\"R2 Random Forest\", r2_score(ytruet_rf, ypredt_rf))\n",
    "print (\"RMSE Random Forest\", mean_squared_error(ytruet_rf, ypredt_rf, squared=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "566328133ae7ba91"
  },
  {
   "cell_type": "markdown",
   "id": "e6ca9832-32a5-47da-b707-419b128bc589",
   "metadata": {},
   "source": [
    "## Neural Network Improvements and Fine-Tuning:\n",
    "Try to improve your initial neural network performance by adding additional techniques and perform manual hyperparameter tuning:\n",
    "\n",
    "- Try different layer architectures (deeper networks)\n",
    "- Different Optimizers (e.g. `SGD` with `momentum`,  `RMSProp`, `Adam`)\n",
    "- Early Stopping\n",
    "- Dropout\n",
    "\n",
    "**Important:** Once you have realized the additional code extension you can use the above training routine (`training`) to start a new training of your network, together with all the changes. Afterwards you can use the same plotting and metric computation (`get_metrics`) for all the different network alternatives/options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d1018d-9cbd-46a6-ac75-ffcf116cb413",
   "metadata": {},
   "source": [
    "### Deeper Nerual Network Architecture\n",
    "\n",
    "Change your initial `CHNetwork` implementation (deeper network and more parameters) to realize the following architectural design:\n",
    "- Model Design: `Input Layer`, `3x Hidden Layer`, `Output Layer` (use `nn.Linear` ase in the example above)\n",
    "- Layer Shapes: `8 to 512`, `512 to 256`, `256 to 128`, `128 to 64`, `64 to 1`\n",
    "- Activation Functions: `relu` activation after each layer, except the final output layer\n",
    "- Implement the `__init__` and `forward` function (see example above)\n",
    "- Return `x`\n",
    "- Train your model again and report the individual loss plots plus the `R2` and `RMSE`\n",
    "- **Additional Remark**: of course you can also try different architectural designs and activation functions - play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf531fbe-0f2f-46e0-8eff-425e5dc1029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CHNetwork(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CHNetwork, self).__init__()\n",
    "        hidden_size = [512, 256,128, 64]\n",
    "        \n",
    "        self.linear01 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.linear02 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.linear03 = nn.Linear(hidden_size[1], hidden_size[2])\n",
    "        self.linear04 = nn.Linear(hidden_size[2], hidden_size[3])\n",
    "        self.linear05 = nn.Linear(hidden_size[3], output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.relu(self.linear01(input))\n",
    "        x = torch.relu(self.linear02(x))\n",
    "        x = torch.relu(self.linear03(x))\n",
    "        x = torch.relu(self.linear04(x))\n",
    "        x = self.linear05(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f585db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input features\n",
    "number_of_features = X.values.shape[1]\n",
    "\n",
    "#network\n",
    "my_net_def2 = CHNetwork(number_of_features, 1)\n",
    "\n",
    "#loss function\n",
    "my_loss_fn = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "#optimizer\n",
    "my_optimizer_fn = torch.optim.SGD\n",
    "\n",
    "#scheduler\n",
    "my_scheduler_fn = torch.optim.lr_scheduler.ExponentialLR\n",
    "\n",
    "#params\n",
    "optim_params = {\n",
    "        'lr': 0.1\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "        'gamma': 0.99999,\n",
    "        'verbose': False,\n",
    "}\n",
    "\n",
    "#training epochs maximum\n",
    "epochs_max = 500\n",
    "\n",
    "train_losses_ref, val_losses_ref = training(my_net_def, \n",
    "                                          my_optimizer_fn, \n",
    "                                          optim_params, \n",
    "                                          my_scheduler_fn,\n",
    "                                          scheduler_params,\n",
    "                                          my_loss_fn, \n",
    "                                          train_dataloader, \n",
    "                                          validation_dataloader, \n",
    "                                          epochs_max,\n",
    "                                          apply_scheduler=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa8897d-74e7-4cc1-84c5-554f1879124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_losses_ref)\n",
    "ax.set_title('Training loss over Epochs')\n",
    "ax.set_xlabel('Epoche')\n",
    "ax.set_ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a pandas Series from the validation loss values\n",
    "validation_loss_series = pd.Series(val_losses_ref)\n",
    "\n",
    "# Compute the rolling mean with a window size of n=10\n",
    "rolling_mean = validation_loss_series.rolling(window=10).mean()\n",
    "\n",
    "# Plot the original validation loss and the smoothed curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(validation_loss_series, label='Validation Loss', alpha=0.2)\n",
    "plt.plot(rolling_mean, label='Smoothed Validation Loss (n=10)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Validation Loss with Smoothing')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c3c3258d5b8168"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.show()\n",
    "r2, rmse = get_metrics(my_net_def, validation_dataloader)\n",
    "print(f'R2 auf Validation Dataset: {r2}')\n",
    "print(f'RMSE auf Validation Dataset: {rmse}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b961db9028b922"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r2, rmse = get_metrics(my_net_def, test_dataloader)\n",
    "print(f'R2 auf Validation Dataset: {r2}')\n",
    "print(f'RMSE auf Validation Dataset: {rmse}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b42022192102f065"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d8672-0ca3-4929-98d7-5f1c36205ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a0b8917",
   "metadata": {},
   "source": [
    "### Different Optimizer\n",
    "\n",
    "- Different Optimizers, e.g. `SGD` with `Momentum`,  `RMSProp`, `Adam` and/or hyperparameter options (e.g. `momentum`, etc.)\n",
    "- Hyperparamter tuning: in practice iterate across the full range of meaningful momentum values to identify the best fitting one\n",
    "- Using the `SGD` optimizer with momentum\n",
    "- Train your model again and report the individual loss plots plus the `R2` and `RMSE`\n",
    "- **Additional Remark**: of course you can also try different opimizers and/or parametric variations - play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec54a08-efc4-4251-aee2-04fc0e625e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input features\n",
    "number_of_features = X.values.shape[1]\n",
    "\n",
    "#network\n",
    "my_net_def3 = CHNetwork(number_of_features, 1)\n",
    "\n",
    "#loss function\n",
    "my_loss_fn = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "#optimizer\n",
    "my_optimizer_fn = torch.optim.SGD\n",
    "\n",
    "#scheduler\n",
    "my_scheduler_fn = torch.optim.lr_scheduler.ExponentialLR\n",
    "\n",
    "#params\n",
    "optim_params = {\n",
    "        'lr': 0.1\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "        'gamma': 0.99999,\n",
    "        'verbose': False,\n",
    "}\n",
    "\n",
    "#training epochs maximum\n",
    "epochs_max = 500\n",
    "\n",
    "train_losses_ref, val_losses_ref = training(my_net_def, \n",
    "                                          my_optimizer_fn, \n",
    "                                          optim_params, \n",
    "                                          my_scheduler_fn,\n",
    "                                          scheduler_params,\n",
    "                                          my_loss_fn, \n",
    "                                          train_dataloader, \n",
    "                                          validation_dataloader, \n",
    "                                          epochs_max,\n",
    "                                          apply_scheduler=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f0034-a5a8-4ea7-8682-dbc5a89ff4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3418a8c",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "- Get yourself familiar with `early stopping` in neural networks\n",
    "  \n",
    "Build your own `EarlyStoppingCriterion` by implementing the code skeleton below:\n",
    "- The constructor gets three different argurments as input:\n",
    "    - `patience` - the number of epochs after the model stops training in case there was no improvement on the validation loss\n",
    "    - `mode` - distinguish between two possible types: `min` (minimization of your target metric, e.g. loss) and `max` (maximizing your target metric, e.g. accuracy)\n",
    "    - `min_delta (default: 0)` - provide the option for a `min_delta` (minimum value) the new/updated target metric (loss, accurarcy, f1-score, etc.) must have improved at least to consider it as an improvement\n",
    "- Implement the `step` function, which takes as input the current/updated score of your target metric `cur_score` and return a boolean (`True`or `False`), in case the ealry stopping criterion is activated (no improvment for more than `patience`, your choase number of epochs, e.g. 15 epochs) or not (improvements of your metrics are still made). Reset your internal counter, whenever the metric gets improved (start from `0` and count until your `patience` criterion)\n",
    "- Last step: integrate the `early stopping criterion` into your trainings routine (`training`), by calling the `step` Function, together with the `validation score` and stop the training when the `early stopping` is `True`\n",
    "- In addition: further extend the given training routine (`training`) so that during training the current best state of the model (`_net.state_dict()`), i.e. the best result on the validation set, is memorized with the corresponding parameters of the model and is stored once the early stopping criterion is active or the `max number of epochs` is reached (Idea: save your best model on the validation set as snapshot)\n",
    "- Train your model again and report the individual loss plots plus the `R2` and `RMSE`\n",
    "- **Additional Remark**: of course you can also try different architectural designs, dropout strategies, and activation functions - play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7fa35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class EarlyStoppingCriterion(object):\n",
    "\n",
    "    def __init__(self, patience, mode, min_delta=0.0):\n",
    "        assert patience >= 0\n",
    "        assert mode in {\"min\", \"max\"}\n",
    "        assert min_delta >= 0.0\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self._count = 0\n",
    "        self.best_score = None\n",
    "        self.is_improved = None\n",
    "\n",
    "    def step(self, cur_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = cur_score\n",
    "            return False, self.best_score \n",
    "        else:\n",
    "            if self.mode == \"max\":\n",
    "                self.is_improved = cur_score >= self.best_score + self.min_delta\n",
    "            else:\n",
    "                self.is_improved = cur_score <= self.best_score - self.min_delta\n",
    "\n",
    "            if self.is_improved:\n",
    "                self._count = 0\n",
    "                self.best_score = cur_score\n",
    "            else:\n",
    "                self._count += 1\n",
    "\n",
    "            return self._count > self.patience, self.best_score\n",
    "\n",
    "\"\"\"\n",
    "Routine to train the neural network.\n",
    "Args:\n",
    "    - _net - Neural network model\n",
    "    - _optimizer_fn- Type of Optimizer (torch.optim function)\n",
    "    - optim_params - Parameter of Optimizer (learning rate, beta values, ...)\n",
    "    - _scheduler_fn - Scheduler for learning rate\n",
    "    - scheduler_params - Parameter of Scheduler (learning rate decay gamma, step-size for learning rate patience, verbose, ...)  \n",
    "    - _loss_fn - Loss Function (torch.nn loss)\n",
    "    - dataloader_tr - Dataloader for training set\n",
    "    - dataloader_val - Dataloader for validation set\n",
    "    - epochs - Maximum number of training epochs\n",
    "    - apply_scheduler (default: True) - Use learning rate scheduling\n",
    "    - snapshot_inverval (default: 300) - Number of epochs \n",
    "    - snap_path (default \"./\") - Path where to store snapshots\n",
    "\"\"\"\n",
    "\n",
    "def training(_net, \n",
    "             _optimizer_fn, \n",
    "             optim_params, \n",
    "             _scheduler_fn,\n",
    "             scheduler_params,\n",
    "             _loss_fn, \n",
    "             dataloader_tr, \n",
    "             dataloader_val, \n",
    "             epochs,\n",
    "             mode=\"min\",\n",
    "             patience=10,\n",
    "             apply_scheduler=True,\n",
    "             snapshot_interval=300,\n",
    "             snap_path = \"./\"):\n",
    " \n",
    "    _optimizer = _optimizer_fn(_net.parameters(), **optim_params)\n",
    "    _scheduler = _scheduler_fn(_optimizer, **scheduler_params)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_model = _net.state_dict().copy()\n",
    "    best_metric = 0.0 if mode == \"max\" else float(\"inf\")\n",
    "    \n",
    "    early_stopping = EarlyStoppingCriterion(mode=mode, patience=patience)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        _net.train()\n",
    "        running_tr_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(dataloader_tr):\n",
    "\n",
    "            _optimizer.zero_grad()\n",
    "            \n",
    "            inputs, targets = data\n",
    "            \n",
    "            yhats = _net(inputs)\n",
    "            \n",
    "            loss = _loss_fn(yhats, targets)\n",
    "            loss.backward()\n",
    "            _optimizer.step()\n",
    "            \n",
    "            if apply_scheduler:\n",
    "                _scheduler.step()\n",
    "            \n",
    "            running_tr_loss += loss.detach().numpy()\n",
    "            \n",
    "        running_tr_loss /= (i+1)\n",
    "\n",
    "        train_losses.append(running_tr_loss)\n",
    "        print(\"Training loss: \", running_tr_loss)\n",
    "        \n",
    "        running_val_loss = 0        \n",
    "        _net.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i, vdata in enumerate(dataloader_val):\n",
    "                vinputs, vtargets = vdata\n",
    "                vloss = _loss_fn(_net(vinputs), vtargets)\n",
    "                running_val_loss += vloss.detach().numpy()\n",
    "            \n",
    "            running_val_loss /= (i+1)\n",
    "            val_losses.append(running_val_loss)\n",
    "            \n",
    "            print(\"Validation loss: \", running_val_loss)\n",
    "            print(\"Epoch: \", epoch)\n",
    "            \n",
    "            early_stop_valid, cur_val_score = early_stopping.step(running_val_loss)\n",
    "\n",
    "            if (mode == \"min\" and cur_val_score < best_metric) or (mode == \"max\" and cur_val_score > best_metric):\n",
    "                best_metric = cur_val_score\n",
    "                best_model = _net.state_dict().copy()\n",
    "                best_epoch = epoch\n",
    "                 \n",
    "            if early_stop_valid:\n",
    "                print(\"Early Stopping at validation epoch: \", epoch)\n",
    "                break\n",
    "                \n",
    "        if epoch % snapshot_interval == 0:\n",
    "            filename = \"snapshot_%s\" % (int(epoch/snapshot_interval))\n",
    "            torch.save(best_model, os.path.join(snap_path, filename))\n",
    "\n",
    "    print(\"Best Epoch:\", best_epoch, \"Best Validation Meric:\", best_metric)\n",
    "    filename = \"snapshot_%s_best_model\" % (int(best_epoch))\n",
    "    torch.save(best_model, os.path.join(snap_path, filename))\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133fe14-6e0c-4e43-8368-a05189cc3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input features\n",
    "number_of_features = X.values.shape[1]\n",
    "\n",
    "#network\n",
    "my_net_deeper_optimizer_early = CHNetwork(number_of_features, 1)\n",
    "\n",
    "#loss function\n",
    "my_loss_fn = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "#optimizer\n",
    "my_optimizer = torch.optim.SGD\n",
    "\n",
    "#scheduler\n",
    "my_scheduler_fn = torch.optim.lr_scheduler.ExponentialLR\n",
    "\n",
    "#params\n",
    "optim_params = {\n",
    "        'lr': 0.1,\n",
    "        'momentum' : 0.5,\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "        'gamma': 0.99999,\n",
    "        'verbose': False,\n",
    "}\n",
    "\n",
    "#training epochs maximum\n",
    "epochs_max = 400\n",
    "\n",
    "train_losses_ref_deeper_momentum_early, val_losses_ref_deeper_momentum_early = training(my_net_deeper_optimizer_early, \n",
    "                                          my_optimizer_fn, \n",
    "                                          optim_params, \n",
    "                                          my_scheduler_fn,\n",
    "                                          scheduler_params,\n",
    "                                          my_loss_fn, \n",
    "                                          train_dataloader, \n",
    "                                          validation_dataloader, \n",
    "                                          epochs_max,\n",
    "                                          mode=\"min\",\n",
    "                                          patience=30,\n",
    "                                          apply_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d57de-ca12-4374-8ce3-fad02ba579bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without momentum and deeper network\n",
    "#plt.plot(train_losses_ref_deeper, color='orange')\n",
    "#plt.plot(val_losses_ref_deeper, color='blue')\n",
    "#plt.plot(pd.Series(val_losses_ref_deeper).rolling(10).mean(), color='green')\n",
    "#\n",
    "##with momentum and deeper network\n",
    "#plt.plot(train_losses_ref_deeper_momentum, color='black')\n",
    "#plt.plot(val_losses_ref_deeper_momentum, color='orchid')\n",
    "#plt.plot(pd.Series(val_losses_ref_deeper_momentum).rolling(10).mean(), color='deeppink')\n",
    "\n",
    "#with momentum and deeper network and early stopping\n",
    "plt.plot(train_losses_ref_deeper_momentum_early, color='yellow')\n",
    "plt.plot(val_losses_ref_deeper_momentum_early, color='brown')\n",
    "plt.plot(pd.Series(val_losses_ref_deeper_momentum_early).rolling(10).mean(), color='cyan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad19013-2c85-484d-b688-8a1afa332617",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2v, rmsev = get_metrics(my_net_deeper_optimizer_early, validation_dataloader)\n",
    "r2t, rmset = get_metrics(my_net_deeper_optimizer_early, test_dataloader)\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"Neural Network Classifier - Deeper, Momentum, Early Stopping - (Validation Set):\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"R2 Neural Network Classifier\", r2v)\n",
    "print (\"RMSE Neural Network Classifier\", rmsev)\n",
    "print()\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"Neural Network Classifier - Deeper, Momentum, Early Stopping - (Test Set):\")\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print (\"R2 Neural Network Classifier\", r2t)\n",
    "print (\"RMSE Neural Network Classifier\", rmset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cd838",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "- Get yourself familiar with `dropout` in neural networks: see https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "\n",
    "Change your initial `CHNetwork` implementation to also integrate dropout (same network depth, but more parameters) and realize the following architectural design:\n",
    "- Model Design: `Dropout Layer`, `Input Layer`, `Dropout Layer before every of the 3x Hidden Layer`, `Output Layer`\n",
    "- Specific `dropout_rate_input` for the `input layer` and `dropout_rate` for all the remaining `hidden layer` \n",
    "- Layer Shapes: `8 to 1024`, `1024 to 1024`, `1024 to 512`, `512 to 64`, `64 to 1`\n",
    "- Activation Functions: `relu` activation after each `linear layer` (fully-connected), except the final output layer\n",
    "- Implement the `__init__` and `forward` function (see example above)\n",
    "- Return `x`\n",
    "- Train your model again and report the individual loss plots plus the `R2` and `RMSE`\n",
    "- **Additional Remark**: of course you can also try different architectural designs, dropout strategies, and activation functions - play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CHNetworkDropout(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, dropout_rate=0.5, dropout_rate_input=0):\n",
    "        super(CHNetworkDropout, self).__init__()\n",
    "\n",
    "        hidden_size = [1024, 1024, 512, 64]\n",
    "        self.dropout_inp = nn.Dropout(dropout_rate_input) # typically own dropout rate for input \n",
    "        self.linear01 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.dropout01 = nn.Dropout(dropout_rate) \n",
    "        self.linear02 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.dropout02 = nn.Dropout(dropout_rate)\n",
    "        self.linear03 = nn.Linear(hidden_size[1], hidden_size[2])\n",
    "        self.dropout03 = nn.Dropout(dropout_rate)\n",
    "        self.linear04 = nn.Linear(hidden_size[2], hidden_size[3])\n",
    "        self.linear05 = nn.Linear(hidden_size[3], output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        x = self.dropout_inp(x)\n",
    "        x = torch.relu(self.linear01(x))\n",
    "        x = self.dropout01(x)\n",
    "        x = torch.relu(self.linear02(x))\n",
    "        x = self.dropout02(x)\n",
    "        x = torch.relu(self.linear03(x))\n",
    "        x = self.dropout03(x)\n",
    "        x = torch.relu(self.linear04(x))\n",
    "        x = self.linear05(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c55717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### number of input features\n",
    "number_of_features = X.values.shape[1]\n",
    "\n",
    "#network\n",
    "my_net_deeper_optimizer_early_drop = CHNetworkDropout(number_of_features, 1, dropout_rate=0.1)\n",
    "\n",
    "#loss function\n",
    "my_loss_fn = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "#optimizer\n",
    "my_optimizer = torch.optim.SGD\n",
    "\n",
    "#scheduler\n",
    "my_scheduler_fn = torch.optim.lr_scheduler.ExponentialLR\n",
    "\n",
    "#params\n",
    "optim_params = {\n",
    "        'lr': 0.1,\n",
    "        'momentum' : 0.5,\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "        'gamma': 0.99999,\n",
    "        'verbose': False,\n",
    "}\n",
    "\n",
    "#training epochs maximum\n",
    "epochs_max = 400\n",
    "\n",
    "train_losses_ref_deeper_momentum_early_drop, val_losses_ref_deeper_momentum_early_drop = training(my_net_deeper_optimizer_early_drop, \n",
    "                                          my_optimizer_fn, \n",
    "                                          optim_params, \n",
    "                                          my_scheduler_fn,\n",
    "                                          scheduler_params,\n",
    "                                          my_loss_fn, \n",
    "                                          train_dataloader, \n",
    "                                          validation_dataloader, \n",
    "                                          epochs_max,\n",
    "                                          mode=\"min\",\n",
    "                                          patience=30,\n",
    "                                          apply_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without momentum and deeper network\n",
    "#plt.plot(train_losses_ref_deeper, color='orange')\n",
    "#plt.plot(val_losses_ref_deeper, color='blue')\n",
    "#plt.plot(pd.Series(val_losses_ref_deeper).rolling(10).mean(), color='green')\n",
    "#\n",
    "##with momentum and deeper network\n",
    "#plt.plot(train_losses_ref_deeper_momentum, color='black')\n",
    "#plt.plot(val_losses_ref_deeper_momentum, color='orchid')\n",
    "#plt.plot(pd.Series(val_losses_ref_deeper_momentum).rolling(10).mean(), color='deeppink')\n",
    "\n",
    "#with momentum and deeper network and early stopping\n",
    "plt.plot(train_losses_ref_deeper_momentum_early, color='yellow')\n",
    "plt.plot(val_losses_ref_deeper_momentum_early, color='brown')\n",
    "plt.plot(pd.Series(val_losses_ref_deeper_momentum_early).rolling(10).mean(), color='cyan')\n",
    "\n",
    "#with momentum and deeper network and early stopping and dropout\n",
    "plt.plot(train_losses_ref_deeper_momentum_early_drop, color='purple')\n",
    "plt.plot(val_losses_ref_deeper_momentum_early_drop, color='olive')\n",
    "plt.plot(pd.Series(val_losses_ref_deeper_momentum_early_drop).rolling(10).mean(), color='limegreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed6dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
