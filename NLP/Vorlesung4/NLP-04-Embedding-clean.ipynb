{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abc4177",
   "metadata": {},
   "source": [
    "# Word Embeddings - Word2Vec and GloVe\n",
    "\n",
    "- Let us build our first word embeddings ...\n",
    "- **Important**: for all of the tasks below please make use of the provided methods given by the `gensim` and `nltk` Python module\n",
    "- This is a  step-by-step coding example how to get from word representation to the corresponding word embeddings in order to use them for various downstream ML/DL-driven learning algroithms\n",
    "- Have look at the `GENSIM` and `NLTK-LM` documentation: https://radimrehurek.com/gensim/ and https://www.nltk.org/api/nltk.lm.html\n",
    "\n",
    "## Load Pre-Trained Word2Vec and GloVe Embeddings\n",
    "\n",
    "- There exist a portfolio of existing word embeddings, trained on the basis of different text corpora and various underlying algorithmic concepts, such as `Word2Vec` or `GloVe`. Look up the `Python` method, which reports all the different models and training corpora which are available for download (Hint: check the dictonary `gensim.downloader.info()` for `models`) \n",
    "- Use the `api` module to `load` the pre-trained `Word2Vec` and `GloVe` model, trained on `google-news-300` and `twitter-200` (in case you face a Jupyter-internal `Warning: IOPub message rate exceeded`, you can either ignore it or get around it via starting Jupyter with the option `--NotebookApp.iopub_data_rate_limit=1.0e10`. No worries, the download itself, but also loading each of the two models, will take some time\n",
    "- You will use both pre-loaded `Word2Vec` and `GloVe` models (for comparison), but also your own trained `Word2Vec` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89f9a3e-b163-48c4-b75f-92aa8c117cd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:08:52.405874975Z",
     "start_time": "2023-11-09T10:08:52.361270200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader\n",
    "import gensim.downloader as api\n",
    "\n",
    "from matplotlib import cm\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f4e970-b5b2-4461-bf22-8905d38622e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:12:00.657969267Z",
     "start_time": "2023-11-09T10:08:53.171809635Z"
    }
   },
   "outputs": [],
   "source": [
    "google300 = api.load(\"word2vec-google-news-300\")\n",
    "twitter200 = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008fb5d-a0c6-43ce-a707-5cebca128534",
   "metadata": {},
   "source": [
    "## Loading, Extracting, Preprocessing, and Tokenizing (Sentences) of the Data Corpus for Downstream Model Training\n",
    "\n",
    "- Use the same text document (`Donald-Trump-Tweets.csv`) as in the `N-GRAM` exercise, providing a series of different Tweets from Donald Trump \n",
    "- Prepare the entire data corpus including various Trump-Tweets, using a similar procedure as within the `N-GRAM` exercise, in terms of `loading, extracting, preprocessing, and tokenizing (sentences)` the data material\n",
    "    - (1) `Pandas` Python library to read the `CSV`-file, filter the column which contains the text information (`Tweet_Text`), and check for empty/invalid elements\n",
    "    - (2)  Clean and prepare (e.g. `regex`, `nltk` functionalities, etc.) the entire text information, to ensure a robust sentence tokenization (categorize (`NER`) patterns like hyperlinks, elements addressed via hashtags, dates, timestamps, multiple whitespaces, upper/lower case, etc.)\n",
    "    - (3) Take the entire text information as one large `training` corpus (all the 7,375 lines) for downstream training of your word embedding model (later in the Jupyter notebook)\n",
    "- `Sentence tokenization:` consider the fact that a single tweet often consists of multiple sentences, which has to be taken into account while building the sentences (`num_of_tweets << num_of_sentences`). Sentences should be stored (as for the `NGRAM` exercise) in a `nested list`, e.g. `[[\"my\", \"first\", \"sentences\"], [\"my\", \"second\", \"sentence\"]]` (each sentence is represented as list of words (word vector $\\vec{w}$))\n",
    "- In terms of word embeddings, `stop words` often lead to undesired phenomena, due to a severe word-specific class imbalance. A `stop word` is very often `context word c_i` word and consequently often surrounded by topic-wise totally different words, which makes it hard to learn and derive a suitable vector representation. Therefor, in order to enhance the quality of the learned word embeddings, remove the `stop words` from the sentence list (make use of `from nltk.corpus import stopwords`, but if needed first download the pool of `stop words` via `nltk.download(\"stopwords\")`).\n",
    "- The identification of sentence beginning/end requires punctuation marks. However, when it comes to building the word embeddings, punctuation marks or any kind of remaining non-alphanumeric patterns (e.g. %, &, $, etc.) should be removed. Since you have your sentences already at this stage this is not a problem!\n",
    "- How about padding the individual sentences with the start (`<s>`) and sentence end (`</s>`) tokens (as for `N-GRAM`)? Useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5660102-6eb3-4efb-bfa6-6a5c5de7462c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:12:03.245323086Z",
     "start_time": "2023-11-09T10:12:00.744514954Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ../NLTK_Data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ../NLTK_Data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Sätze aus Tweets: 10283\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.corpus import stopwords\n",
    "data = pd.read_csv('../Vorlesung3/Donald-Trump-Tweets.csv').filter(['Tweet_Text']).dropna().drop_duplicates()\n",
    "\n",
    "text = data.values.tolist()\n",
    "\n",
    "for i in range(len(text)):\n",
    "    text[i] = re.sub('([#][\\w_-]+)', 'Hashtag', str(text[i]))\n",
    "    text[i] = re.sub('([@][\\w_-]+)', 'Mention', str(text[i]))\n",
    "    text[i] = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S', 'Link', str(text[i]))\n",
    "    \n",
    "allTweets = \" \".join(text)\n",
    "allTweets = re.sub(\"(\\[\\\\'\\.|\\[\\\\'\\\"|\\[\\\\')\", '', allTweets)\n",
    "allTweets = re.sub(\"(\\\\'])\", '', allTweets)\n",
    "\n",
    "\n",
    "nltk.download('punkt', download_dir='../NLTK_Data')\n",
    "nltk.download('stopwords', download_dir='../NLTK_Data')\n",
    "nltk.data.path.append('../NLTK_Data')\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(allTweets.lower())\n",
    "print('Anzahl Sätze aus Tweets: {}'.format(sentences.__len__()))\n",
    "\n",
    "#remove punctuation\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = re.sub('(!|\\.|,|-|\\&amp;|\\?|\\]|\\\\+n|:|%|\\$|&|;|\")', '', str(sentences[i]))\n",
    "    \n",
    "#Add BoS and EoS tags\n",
    "sentence_word = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.tokenize.word_tokenize(sentences[i])\n",
    "    filtered_words = [w for w in words if not w.lower() in stop_words]\n",
    "    padded = list(pad_both_ends(filtered_words, n =3))\n",
    "    sentence_word.append([\" \".join(padded), padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a0712b-00d2-4047-86a5-b0bb18035981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:20:46.348736585Z",
     "start_time": "2023-11-09T10:20:46.293356887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittliche Anzahl Wörter in einem Satz (ohne stopwords): 7.497811922590683\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "for i in range(len(sentence_word)):\n",
    "    total_words += (sentence_word[i][1].__len__() - 4) #substract BoS EoS token\n",
    "\n",
    "print(\"Durchschnittliche Anzahl Wörter in einem Satz (ohne stopwords): {}\".format(total_words/sentences.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a89f73-8a6f-4d99-9ddd-8af124af5d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85ce5a14-08cb-4907-94d0-e0a6e9e48db5",
   "metadata": {},
   "source": [
    "## Apply Pre-Trained Word2Vec and GloVe models\n",
    "\n",
    "- Use the pre-loaded `Word2Vec` and `GloVe` model and report the shape of a single word embedding vector for both of the models, as well as the overall size of the vocabulary. Are there differences in the vector embedding and vocabulary size and if so, how large is the difference? \n",
    "- Use both models throughout the exercise (same functionalities) in order to compute the cosine similarity $cos(\\theta)$, by applying the `similarity` function for the following words pairs (you can also try others, of course):\n",
    "    - `Pikachu` vs. `Pokemon`\n",
    "    - `Soccer` vs. `Football`\n",
    "    - `Messi` vs. `Barcelona`\n",
    "    - `Pele` vs. `Ronaldo`\n",
    "    - `Orca` vs. `Whale`\n",
    "    - `Prof.` vs. `Dr.`\n",
    "- Report the `N=10` context words which present the highest similarity in terms of a given target word (descending order). Do not implement your own function, but rather make use of an existing function, provided by the model. Use the following target words: `student, trump, boxing, hospital, matrix & python` (you can also try others, of course)\n",
    "- Look up and use the `doesnt_match` function - what is the underlying purpose?\n",
    "- Compute various `analogies` via `positive`and `negative` word pairs (e.g. `positive=[woman king]`, `negative=[man]`, leading to `queen`) - use the same function as for computing the most similar context words (Hint: look for an existing function of the model!)\n",
    "- Get the `dense vector` (word vector) from the trained word embedding matrix via `get_vector`, as well as the `index`for a specific word via `get_index`, both for the word `europe`\n",
    "- Define a function `get_word_embeddings(words)` which `returns word clouds/clusters` for each given word in a `wordlist`, including the `10` closest (highest similarity) `context word embeddings` around a given list of `target words`. The dictionary output named `word cloud` has the following structure: `target word : {context_word_1 : ndarray_embed_vec, (...) , context_word_N : ndarray_embed_vec, target_word : ndarray_embed_vec}`\n",
    "\n",
    "```\n",
    "def get_word_embeddings(model, wordlist, topn):\n",
    "    w_emb = []\n",
    "    w_cloud = {}\n",
    "    for t_w in words:\n",
    "        #TODO - use \"most_similar\" function with target word \"t_w\"\n",
    "        #TODO - get embedding for context word \"c_w\"\n",
    "        #TODO - store everything (t_w, list of c_w embeddings) in a dictionary w_cloud (key = t_w, value = list of c_w)\n",
    "    return w_cloud \n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ecd4d6-f365-49b3-ac0e-59712e49db91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:23:18.147429939Z",
     "start_time": "2023-11-09T10:23:18.095448383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe von word2vec-google-news-300: 300; Größe von glove-twitter-200: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Größe von word2vec-google-news-300: {}; Größe von glove-twitter-200: {}\".format(google300.vector_size, twitter200.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pikachu und Pokemon sind sich 0.6372727751731873 ähnlich\n",
      "Soccer und Football sind sich 0.7229189872741699 ähnlich\n",
      "Messi und Barcelona sind sich 0.6065428256988525 ähnlich\n",
      "Pele und Ronaldo sind sich 0.5526662468910217 ähnlich\n",
      "Orca und Whale sind sich 0.5591481924057007 ähnlich\n",
      "Prof. und Dr. sind sich 0.7368013262748718 ähnlich\n",
      "GloVe\n",
      "Pikachu ist nicht im Datensatz enthalten\n",
      "Soccer ist nicht im Datensatz enthalten\n",
      "Messi ist nicht im Datensatz enthalten\n",
      "Pele ist nicht im Datensatz enthalten\n",
      "Orca ist nicht im Datensatz enthalten\n",
      "Prof. ist nicht im Datensatz enthalten\n"
     ]
    }
   ],
   "source": [
    "test_words=[\n",
    "    ['Pikachu', 'Pokemon'],\n",
    "    ['Soccer','Football'],\n",
    "    ['Messi','Barcelona'],\n",
    "    ['Pele','Ronaldo'],\n",
    "    ['Orca','Whale'],\n",
    "    ['Prof.','Dr.'],\n",
    "]\n",
    "\n",
    "for test_word in test_words:    \n",
    "    print('{} und {} sind sich {} ähnlich'.format(test_word[0], test_word[1],google300.similarity(test_word[0], test_word[1])))\n",
    "    \n",
    "print('GloVe')\n",
    "for test_word in test_words:   \n",
    "    try:\n",
    "        print('{} und {} sind sich {} ähnlich'.format(test_word[0], test_word[1],twitter200.similarity(test_word[0], test_word[1])))\n",
    "    except:\n",
    "        print(\"{} ist nicht im Datensatz enthalten\".format(test_word[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T10:33:48.193946500Z",
     "start_time": "2023-11-09T10:33:48.149563557Z"
    }
   },
   "id": "39553fb83b30a0aa"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ähnlichsten Worte zu student sind: ['students', 'Student', 'teacher', 'stu_dent', 'faculty', 'school', 'undergraduate', 'university', 'undergraduates', 'semester']\n",
      "10 ähnlichsten Worte zu trump sind: ['trumps', 'trumping', 'supersede', 'trumped', 'supercede', 'prevail', 'outweigh', 'trample', 'overshadow', 'dictate']\n",
      "10 ähnlichsten Worte zu boxing sind: ['amateur_boxing', 'boxers', 'Boxing', 'boxer', 'mixed_martial_arts', 'prizefighting', 'light_heavyweight', 'welterweight', 'MMA', 'middleweight']\n",
      "10 ähnlichsten Worte zu hospital sind: ['Hospital', 'hopsital', 'hosptial', 'hospitals', 'intensive_care', 'Hosptial', 'Hopsital', 'hopital', 'Intensive_Care_Unit', 'Medical_Center']\n",
      "10 ähnlichsten Worte zu matrix sind: ['intramolecular', 'Si_substrate', 'covalent_bond', 'matrices', 'antiparallel', 'chromatograms', 'antibody_antigen', 'gaussian', 'matrixes', 'polyhedral']\n",
      "10 ähnlichsten Worte zu python sind: ['pythons', 'Burmese_python', 'snake', 'crocodile', 'boa_constrictor', 'alligator', 'reptile', 'albino_python', 'croc', 'lizard']\n"
     ]
    }
   ],
   "source": [
    "target_words=['student','trump','boxing','hospital','matrix','python']\n",
    "\n",
    "for target_word in target_words:\n",
    "    topn = google300.most_similar(target_word, topn=10)\n",
    "    print('10 ähnlichsten Worte zu {} sind: {}'.format(target_word, [item[0] for item in topn]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T10:50:18.783495005Z",
     "start_time": "2023-11-09T10:50:16.458147637Z"
    }
   },
   "id": "a0db04799e26d966"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "'Pikachu'"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google300.doesnt_match(['Dark Magician', 'Exodia', 'Blue-Eyes white dragon', 'Pikachu'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T12:27:20.884998164Z",
     "start_time": "2023-11-09T12:27:20.765179945Z"
    }
   },
   "id": "88ef36a7498d13bf"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "[('kings', 0.7138045430183411),\n ('queen', 0.6510956883430481),\n ('monarch', 0.6413194537162781),\n ('crown_prince', 0.6204220056533813),\n ('prince', 0.6159993410110474),\n ('sultan', 0.5864824056625366),\n ('ruler', 0.5797567367553711),\n ('princes', 0.5646552443504333),\n ('Prince_Paras', 0.5432944297790527),\n ('throne', 0.5422105193138123)]"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google300.most_similar(positive=['king'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T12:32:29.503420151Z",
     "start_time": "2023-11-09T12:32:29.124514200Z"
    }
   },
   "id": "6082713baa765029"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.54296875e-01, -8.00781250e-02,  1.13281250e-01,  4.70703125e-01,\n       -4.15039062e-02,  4.56542969e-02, -2.03125000e-01, -3.57421875e-01,\n       -5.90820312e-02,  3.12500000e-02, -1.74804688e-01, -1.82617188e-01,\n       -4.19921875e-02, -2.61718750e-01,  7.47070312e-02,  2.27539062e-01,\n        1.42578125e-01,  3.37890625e-01, -1.39160156e-02,  1.47705078e-02,\n        1.71875000e-01, -1.94335938e-01,  2.50000000e-01,  7.23266602e-03,\n       -1.15722656e-01,  4.82421875e-01, -3.78906250e-01,  1.52343750e-01,\n       -1.80664062e-01,  1.41601562e-01,  1.55639648e-02,  3.32031250e-01,\n       -1.04980469e-01, -1.32812500e-01, -1.25976562e-01,  4.66308594e-02,\n       -3.16406250e-01, -4.24804688e-02,  3.55468750e-01,  1.95312500e-01,\n       -6.73828125e-02,  6.44531250e-02,  1.33789062e-01,  3.03955078e-02,\n        2.31445312e-01,  7.32421875e-02,  5.76171875e-02, -2.40478516e-02,\n       -3.16406250e-01,  5.41992188e-02, -1.49414062e-01, -4.95605469e-02,\n        1.53320312e-01,  4.47265625e-01, -1.11816406e-01, -1.25000000e-01,\n       -2.61718750e-01,  4.34570312e-02, -3.19824219e-02, -4.33593750e-01,\n       -2.03125000e-01,  3.66210938e-02, -1.98242188e-01, -2.47070312e-01,\n       -2.24609375e-01, -5.58593750e-01, -9.08203125e-02, -9.37500000e-02,\n       -2.67578125e-01,  8.44726562e-02, -1.42578125e-01,  4.29687500e-01,\n        5.21850586e-03,  2.61718750e-01, -2.53906250e-01, -1.34765625e-01,\n        2.03125000e-01, -5.27954102e-03, -1.25000000e-01,  1.28906250e-01,\n       -2.65625000e-01,  1.79443359e-02, -3.65234375e-01, -3.53515625e-01,\n        2.26562500e-01,  3.94531250e-01, -1.56250000e-02,  8.91113281e-03,\n       -2.52685547e-02,  1.67968750e-01,  3.44238281e-02, -6.44531250e-02,\n       -2.85156250e-01,  5.20019531e-02, -9.91210938e-02,  1.47094727e-02,\n       -1.47460938e-01, -2.19726562e-01,  6.13281250e-01,  1.71875000e-01,\n       -2.47070312e-01,  2.04101562e-01,  9.22851562e-02,  3.32031250e-01,\n       -1.55639648e-03,  7.81250000e-02, -2.92968750e-01,  2.79296875e-01,\n        5.17578125e-02, -1.66992188e-01, -4.68750000e-02,  3.05175781e-02,\n       -1.51367188e-01,  2.33154297e-02,  4.85839844e-02, -3.08593750e-01,\n       -1.39648438e-01, -3.08593750e-01,  2.20703125e-01,  1.21459961e-02,\n       -1.68457031e-02, -3.73046875e-01,  9.13085938e-02, -2.06298828e-02,\n        3.69140625e-01, -3.08593750e-01, -5.19531250e-01,  1.87988281e-02,\n       -1.59179688e-01, -3.20312500e-01,  1.12915039e-03, -1.02050781e-01,\n        2.20947266e-02,  2.84423828e-02, -4.95605469e-02, -2.35351562e-01,\n       -1.55273438e-01,  4.29687500e-02,  7.61718750e-02,  2.17773438e-01,\n        2.26562500e-01, -3.86718750e-01,  8.00781250e-02,  1.79443359e-02,\n        5.32226562e-02,  5.83496094e-02, -2.80761719e-02, -3.22265625e-02,\n       -1.47819519e-04, -3.32031250e-01,  7.32421875e-02, -1.61132812e-01,\n       -6.29882812e-02, -2.17773438e-01, -3.46679688e-02, -3.51562500e-01,\n       -5.63964844e-02,  1.87500000e-01, -4.27246094e-02,  2.79296875e-01,\n        1.53198242e-02,  1.23046875e-01, -1.23046875e-01, -2.17773438e-01,\n        2.40234375e-01, -2.26562500e-01,  4.12597656e-02, -6.29882812e-02,\n       -3.92578125e-01,  2.13867188e-01, -4.16015625e-01,  1.48437500e-01,\n        2.44140625e-01, -9.08203125e-02, -1.66015625e-01, -2.69531250e-01,\n        1.46484375e-01, -7.10937500e-01,  2.11914062e-01, -2.21679688e-01,\n       -4.10079956e-04,  1.16699219e-01, -1.88476562e-01, -4.76074219e-02,\n        2.91015625e-01,  2.96875000e-01,  9.57031250e-02, -1.75781250e-01,\n        5.22460938e-02,  1.33666992e-02,  4.49218750e-01,  4.98046875e-02,\n        4.05273438e-02, -2.00195312e-01,  2.77343750e-01,  1.77734375e-01,\n       -1.80664062e-01,  2.98828125e-01, -2.81250000e-01, -4.17968750e-01,\n       -1.40625000e-01, -1.45507812e-01, -1.87988281e-02, -3.06640625e-01,\n       -2.47070312e-01, -1.74804688e-01, -1.15203857e-03, -9.22851562e-02,\n       -4.94140625e-01,  3.55468750e-01,  1.07910156e-01, -1.79443359e-02,\n       -5.23437500e-01,  1.12304688e-01, -4.43359375e-01, -1.48925781e-02,\n        1.99218750e-01, -2.15820312e-01,  8.39843750e-02,  5.98144531e-02,\n       -3.41796875e-02, -1.80664062e-01,  1.73828125e-01, -1.75781250e-02,\n       -1.78710938e-01, -7.08007812e-02,  4.41894531e-02, -1.64062500e-01,\n        8.44726562e-02, -3.53515625e-01,  1.23535156e-01,  7.81250000e-03,\n        2.69531250e-01, -2.65625000e-01, -8.93554688e-02,  2.49023438e-01,\n       -6.34765625e-02,  1.20605469e-01, -2.14843750e-02, -3.83300781e-02,\n        1.48437500e-01,  1.83593750e-01, -2.28271484e-02,  2.69531250e-01,\n        6.29882812e-02,  1.22070312e-01, -1.45507812e-01,  3.39843750e-01,\n       -1.65039062e-01,  3.66210938e-03,  3.35937500e-01,  3.44238281e-02,\n        1.57226562e-01,  3.06640625e-01, -1.22680664e-02, -8.74023438e-02,\n       -1.39648438e-01, -1.51367188e-02,  5.50781250e-01,  6.54296875e-02,\n        9.81445312e-02, -2.69531250e-01,  2.05078125e-01, -1.23535156e-01,\n       -4.19921875e-02,  7.86132812e-02, -3.33984375e-01, -1.64062500e-01,\n       -8.59375000e-02, -2.94921875e-01, -1.61132812e-01, -1.88476562e-01,\n        6.54296875e-02,  1.69921875e-01,  3.24707031e-02, -1.08886719e-01,\n        1.64062500e-01,  4.56542969e-02, -3.20312500e-01,  1.54296875e-01,\n       -1.90429688e-01,  1.24511719e-02,  2.77343750e-01,  1.10351562e-01,\n       -1.69921875e-01,  1.71875000e-01, -8.34960938e-02,  1.58203125e-01,\n       -1.35803223e-03, -4.95605469e-02,  1.84570312e-01,  3.47656250e-01,\n        1.00097656e-01,  4.19921875e-01, -1.39648438e-01, -2.13623047e-02,\n       -2.53906250e-01,  6.15234375e-02,  2.16796875e-01,  1.19628906e-01],\n      dtype=float32)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google300.get_vector('europe')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T12:33:08.286221334Z",
     "start_time": "2023-11-09T12:33:08.243793350Z"
    }
   },
   "id": "8d4ec26e9b706ae4"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "122664"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google300.get_index('europe')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T12:33:39.828219900Z",
     "start_time": "2023-11-09T12:33:39.742305784Z"
    }
   },
   "id": "af300a423fb6face"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f981a1f5a8f83275"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "18304f69d11154d1"
  },
  {
   "cell_type": "markdown",
   "id": "83dcef8d-74fc-4311-b6bb-2fc53154c2d3",
   "metadata": {},
   "source": [
    "## Compute Topic-Related Word Clouds\n",
    "\n",
    "- Apply the function `get_word_embeddings(model, wordlist, topn)` to a number of topic-specific words `topics = yoda pizza orca hamburg mojito mercedes`, by converting `topics` to a `wordlist`. In addition, please select `topn=10`, resulting in the top-10 closest neighbors (words with the highest similarity) for each of those target words. As a result, the function `get_word_embeddings(model, wordlist, topn)` returns the word cloud object (10 surrounding word embeddings + 1 target word embedding for each topic). In case $topn=0$ only the target word (no neighboring words) should be reported!\n",
    "- Compute the topic-related `word cloud` information for both models: `Word2Vec` and `GloVe`\n",
    "- The `word cloud` and its underlying high-dimensional word embeddings can now be used to visualize compact word relations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea90aa4-7871-4f0c-985d-ce0e233dfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "google300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e6592-6b2e-41b1-a201-dd19c638322c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a0acb-e56a-49bc-af37-6d6319bada62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6365c7b4-ce1b-4ed7-99e8-37d09dbab88f",
   "metadata": {},
   "source": [
    "## Visualization Word2Vec/GloVe embeddings\n",
    "- Execute the code-snippet below, by calling the function: `tsne_plot(word_cloud, topn)` which takes the previous calculated `word cloud` information of the `topics`. The method reduces the dimensionality of the exisitng $M \\times N$-large matrix to $M \\times N=2$ dimensions, in order to visualize the individual `word clouds` on a 2D-grid. As type of dimensionality reduction technique a combination between `Principal Component Analysis (PCA)` and subsequent `t-distributed Stochastic Neighbor Embedding (TSNE)` is used.\n",
    "\n",
    "You can consider the dimensionality reduction approach as `BlackBox`, however, you can also look it up in the respective documentations of `scikit-learn` (will be part of the `Machine Learning Lecture`)\n",
    "- `TSNE` = https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "- `PCA` = https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d775462-d363-4f7c-adbb-f7fa45c44ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_embeddings(word_cloud):\n",
    "    embeddings=[]\n",
    "    labels=[]\n",
    "    for key in word_cloud:\n",
    "        word_embed = word_cloud.get(key)\n",
    "        for word in word_embed:\n",
    "            labels.append(word)\n",
    "            embeddings.append(word_embed.get(word))\n",
    "    \n",
    "    return np.array(labels), np.array(embeddings)\n",
    "\n",
    "def tsne_plot(word_cloud, topn):\n",
    "    labels, tokens = get_labels_embeddings(word_cloud)\n",
    "\n",
    "    if topn != 0:\n",
    "        colors = cm.rainbow(np.linspace(0, 1, len(labels))) \n",
    "    else:\n",
    "        color = [\"orange\"]\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=4, n_components=2, init='pca', n_iter=3500, random_state=10)\n",
    "    tsne_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for value in tsne_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 10)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if topn != 0:\n",
    "            if i % (topn+1) == 0:\n",
    "                color = colors[i-1]\n",
    "        plt.scatter(x[i],y[i],color=color)\n",
    "        plt.annotate(labels[i], xy=(x[i], y[i]))\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e8316-907e-4175-a427-a141b0308fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68b65b58-2315-4aa8-a928-a334df7f7ed8",
   "metadata": {},
   "source": [
    "## Build and Train your own Word2Vec-System \n",
    "\n",
    "- Train your own `Word2Vec` model using the `gensim Word2Vec` module together with the pre-processed Trump-Tweet sentence corpora `[[\"my\", \"first\", \"sentences\"], (...), [\"my\", \"second\", \"sentence\"]]` and the following hyperparameters: `vector_size=300, window=5, min_count=1, workers=4` (have a look at the documentation, what does each parameters mean!)\n",
    "- Report the vocabulary (`words` and `size`) of your trained `Word2Vec` (Hint: `index_to_key` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2ea2d-ba1d-465a-9e61-05d1ba08b475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af51e2-5127-41c4-8419-2aacb33a0c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bf55d-fae7-44c8-aaa2-07623eb346dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30b23f48-54ad-4b2a-ac67-7726efa125ce",
   "metadata": {},
   "source": [
    "## Evaluation of the Language Model \n",
    "- Evaluate your model by reporting word similarities for `america vs. great`, `great vs. again`, `trump vs. clinton`\n",
    "- In addition also present the most similar words (top-15) for the word `fake`\n",
    "- Feel free to try other word combinations with more and different evaluation techniques (as shown above for the pre-trained models)\n",
    "- You can also visualize your entire vocabulary! Get your vocabulary as a list of words (`index_to_key`), compute the `get_word_embeddings(model, wordlist, topn)` with `topn=0`, because no other similar words should be shown (only target  word). Use the output of `get_word_embeddings(model, wordlist, topn)` and call `tsne_plot` with the same `topn=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc95df-bc48-4478-9935-4ee821e30de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df5314-ec50-4849-9af5-22997d2a3669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e9aeb-b498-4b89-bcac-569ebe62bcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c38484c0-b994-4f90-9d6d-5a47fd7a7c46",
   "metadata": {},
   "source": [
    "## Save and Store your final Model\n",
    "- Store the trained and final `Word2Vec` model \n",
    "- Use  the `save` method to store your model at a given output path\n",
    "- Call the `load` method to load your model based on a given input path. Verfiy the model after loading by computing the $cos(\\theta)$ similarity for `trump vs. clinton`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde955f-a4e8-43e5-81eb-cde003ffa337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff75c87-635e-45e1-8591-2d2457144558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdf77a-ed0a-4d0b-a7e8-1b289dc3bcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
