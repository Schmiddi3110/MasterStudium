Providedproperattributionisprovided,Googleherebygrantspermissionto
reproducethetablesandinthispapersolelyforuseinjournalisticor
scholarlyworks.
AttentionIsAllYouNeed
AshishVaswani

GoogleBrain
avaswani@google.com
NoamShazeer

GoogleBrain
noam@google.com
NikiParmar

GoogleResearch
nikip@google.com
JakobUszkoreit

GoogleResearch
usz@google.com
LlionJones

GoogleResearch
llion@google.com
AidanN.Gomez
y
UniversityofToronto
aidan@cs.toronto.edu
Kaiser

GoogleBrain
lukaszkaiser@google.com
IlliaPolosukhin
z
illia.polosukhin@gmail.com
Abstract
Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor
convolutionalneuralnetworksthatincludeanencoderandadecoder.Thebest
performingmodelsalsoconnecttheencoderanddecoderthroughanattention
mechanism.Weproposeanewsimplenetworkarchitecture,theTransformer,
basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions
entirely.Experimentsontwomachinetranslationtasksshowthesemodelsto
besuperiorinqualitywhilebeingmoreparallelizableandrequiring
lesstimetotrain.Ourmodelachieves28.4BLEUontheWMT2014English-
to-Germantranslationtask,improvingovertheexistingbestresults,including
ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,
ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after
trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe
bestmodelsfromtheliterature.WeshowthattheTransformergeneralizeswellto
othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith
largeandlimitedtrainingdata.

Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted
theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedtheTransformermodelsand
hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head
attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery
detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand
tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and
efcientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand
implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating
ourresearch.
y
WorkperformedwhileatGoogleBrain.
z
WorkperformedwhileatGoogleResearch.
31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.
arXiv:1706.03762v7  [cs.CL]  2 Aug 2023
1Introduction
Recurrentneuralnetworks,longshort-termmemory[
13
]andgatedrecurrent[
7
]neuralnetworks
inparticular,havebeenestablishedasstateoftheartapproachesinsequencemodelingand
transductionproblemssuchaslanguagemodelingandmachinetranslation[
35
,
2
,
5
].Numerous
effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder
architectures[38,24,15].
Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput
sequences.Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden
states
h
t
,asafunctionoftheprevioushiddenstate
h
t

1
andtheinputforposition
t
.Thisinherently
sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger
sequencelengths,asmemoryconstraintslimitbatchingacrossexamples.Recentworkhasachieved
improvementsincomputationalefciencythroughfactorizationtricks[
21
]andconditional
computation[
32
],whilealsoimprovingmodelperformanceincaseofthelatter.Thefundamental
constraintofsequentialcomputation,however,remains.
Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-
tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein
theinputoroutputsequences[
2
,
19
].Inallbutafewcases[
27
],however,suchattentionmechanisms
areusedinconjunctionwitharecurrentnetwork.
InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead
relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.
TheTransformerallowsformoreparallelizationandcanreachanewstateoftheartin
translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.
2Background
ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU
[
16
],ByteNet[
18
]andConvS2S[
9
],allofwhichuseconvolutionalneuralnetworksasbasicbuilding
block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,
thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows
inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet.Thismakes
itmorediftolearndependenciesbetweendistantpositions[
12
].IntheTransformerthisis
reducedtoaconstantnumberofoperations,albeitatthecostofreducedeffectiveresolutiondue
toaveragingattention-weightedpositions,aneffectwecounteractwithMulti-HeadAttentionas
describedinsection3.2.
Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions
ofasinglesequenceinordertocomputearepresentationofthesequence.Self-attentionhasbeen
usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,
textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].
End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-
alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand
languagemodelingtasks[34].
Tothebestofourknowledge,however,theTransformeristhetransductionmodelrelying
entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-
alignedRNNsorconvolution.Inthefollowingsections,wewilldescribetheTransformer,motivate
self-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].
3ModelArchitecture
Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[
5
,
2
,
35
].
Here,theencodermapsaninputsequenceofsymbolrepresentations
(
x
1
;:::;x
n
)
toasequence
ofcontinuousrepresentations
z
=(
z
1
;:::;z
n
)
.Given
z
,thedecoderthengeneratesanoutput
sequence
(
y
1
;:::;y
m
)
ofsymbolsoneelementatatime.Ateachstepthemodelisauto-regressive
[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.
2
Figure1:TheTransformer-modelarchitecture.
TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully
connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,
respectively.
3.1EncoderandDecoderStacks
Encoder:
Theencoderiscomposedofastackof
N
=6
identicallayers.Eachlayerhastwo
sub-layers.Theisamulti-headself-attentionmechanism,andthesecondisasimple,position-
wisefullyconnectedfeed-forwardnetwork.Weemployaresidualconnection[
11
]aroundeachof
thetwosub-layers,followedbylayernormalization[
1
].Thatis,theoutputofeachsub-layeris
LayerNorm(
x
+Sublayer(
x
))
,where
Sublayer(
x
)
isthefunctionimplementedbythesub-layer
itself.Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding
layers,produceoutputsofdimension
d
model
=512
.
Decoder:
Thedecoderisalsocomposedofastackof
N
=6
identicallayers.Inadditiontothetwo
sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head
attentionovertheoutputoftheencoderstack.Similartotheencoder,weemployresidualconnections
aroundeachofthesub-layers,followedbylayernormalization.Wealsomodifytheself-attention
sub-layerinthedecoderstacktopreventpositionsfromattendingtosubsequentpositions.This
masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe
predictionsforposition
i
candependonlyontheknownoutputsatpositionslessthan
i
.
3.2Attention
Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,
wherethequery,keys,values,andoutputareallvectors.Theoutputiscomputedasaweightedsum
3
ScaledDot-ProductAttention
Multi-HeadAttention
Figure2:(left)ScaledDot-ProductAttention.(right)Multi-HeadAttentionconsistsofseveral
attentionlayersrunninginparallel.
ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe
querywiththecorrespondingkey.
3.2.1ScaledDot-ProductAttention
Wecallourparticularattention"ScaledDot-ProductAttention"(Figure2).Theinputconsistsof
queriesandkeysofdimension
d
k
,andvaluesofdimension
d
v
.Wecomputethedotproductsofthe
querywithallkeys,divideeachby
p
d
k
,andapplyasoftmaxfunctiontoobtaintheweightsonthe
values.
Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether
intoamatrix
Q
.Thekeysandvaluesarealsopackedtogetherintomatrices
K
and
V
.Wecompute
thematrixofoutputsas:
Attention(
Q;K;V
)=softmax(
QK
T
p
d
k
)
V
(1)
Thetwomostcommonlyusedattentionfunctionsareadditiveattention[
2
],anddot-product(multi-
plicative)attention.Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor
of
1
p
d
k
.Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith
asinglehiddenlayer.Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis
muchfasterandmorespace-efcientinpractice,sinceitcanbeimplementedusinghighlyoptimized
matrixmultiplicationcode.
Whileforsmallvaluesof
d
k
thetwomechanismsperformsimilarly,additiveattentionoutperforms
dotproductattentionwithoutscalingforlargervaluesof
d
k
[
3
].Wesuspectthatforlargevaluesof
d
k
,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas
extremelysmallgradients
4
.Tocounteractthiseffect,wescalethedotproductsby
1
p
d
k
.
3.2.2Multi-HeadAttention
Insteadofperformingasingleattentionfunctionwith
d
model
-dimensionalkeys,valuesandqueries,
wefoundittolinearlyprojectthequeries,keysandvalues
h
timeswithdifferent,learned
linearprojectionsto
d
k
,
d
k
and
d
v
dimensions,respectively.Oneachoftheseprojectedversionsof
queries,keysandvalueswethenperformtheattentionfunctioninparallel,yielding
d
v
-dimensional
4
Toillustratewhythedotproductsgetlarge,assumethatthecomponentsof
q
and
k
areindependentrandom
variableswithmean
0
andvariance
1
.Thentheirdotproduct,
q

k
=
P
d
k
i
=1
q
i
k
i
,hasmean
0
andvariance
d
k
.
4
outputvalues.Theseareconcatenatedandonceagainprojected,resultinginthevalues,as
depictedinFigure2.
Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation
subspacesatdifferentpositions.Withasingleattentionhead,averaginginhibitsthis.
MultiHead(
Q;K;V
)=Concat(head
1
;:::;
head
h
)
W
O
where
head
i
=Attention(
QW
Q
i
;KW
K
i
;VW
V
i
)
Wheretheprojectionsareparametermatrices
W
Q
i
2
R
d
model

d
k
,
W
K
i
2
R
d
model

d
k
,
W
V
i
2
R
d
model

d
v
and
W
O
2
R
hd
v

d
model
.
Inthisworkweemploy
h
=8
parallelattentionlayers,orheads.Foreachoftheseweuse
d
k
=
d
v
=
d
model
=h
=64
.Duetothereduceddimensionofeachhead,thetotalcomputationalcost
issimilartothatofsingle-headattentionwithfulldimensionality.
3.2.3ApplicationsofAttentioninourModel
TheTransformerusesmulti-headattentioninthreedifferentways:
Ł
In"encoder-decoderattention"layers,thequeriescomefromthepreviousdecoderlayer,
andthememorykeysandvaluescomefromtheoutputoftheencoder.Thisallowsevery
positioninthedecodertoattendoverallpositionsintheinputsequence.Thismimicsthe
typicalencoder-decoderattentionmechanismsinsequence-to-sequencemodelssuchas
[38,2,9].
Ł
Theencodercontainsself-attentionlayers.Inaself-attentionlayerallofthekeys,values
andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe
encoder.Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe
encoder.
Ł
Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto
allpositionsinthedecoderuptoandincludingthatposition.Weneedtopreventleftward
informationwinthedecodertopreservetheauto-regressiveproperty.Weimplementthis
insideofscaleddot-productattentionbymaskingout(settingto

)allvaluesintheinput
ofthesoftmaxwhichcorrespondtoillegalconnections.SeeFigure2.
3.3Position-wiseFeed-ForwardNetworks
Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully
connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically.This
consistsoftwolineartransformationswithaReLUactivationinbetween.
FFN(
x
)=max(0
;xW
1
+
b
1
)
W
2
+
b
2
(2)
Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters
fromlayertolayer.Anotherwayofdescribingthisisastwoconvolutionswithkernelsize1.
Thedimensionalityofinputandoutputis
d
model
=512
,andtheinner-layerhasdimensionality
d
ff
=2048
.
3.4EmbeddingsandSoftmax
Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput
tokensandoutputtokenstovectorsofdimension
d
model
.Wealsousetheusuallearnedlineartransfor-
mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities.In
ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-softmax
lineartransformation,similarto[
30
].Intheembeddinglayers,wemultiplythoseweightsby
p
d
model
.
5
Table1:Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations
fordifferentlayertypes.
n
isthesequencelength,
d
istherepresentationdimension,
k
isthekernel
sizeofconvolutionsand
r
thesizeoftheneighborhoodinrestrictedself-attention.
LayerTypeComplexityperLayerSequentialMaximumPathLength
Operations
Self-Attention
O
(
n
2

d
)
O
(1)
O
(1)
Recurrent
O
(
n

d
2
)
O
(
n
)
O
(
n
)
Convolutional
O
(
k

n

d
2
)
O
(1)
O
(
log
k
(
n
))
Self-Attention(restricted)
O
(
r

n

d
)
O
(1)
O
(
n=r
)
3.5PositionalEncoding
Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe
orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe
tokensinthesequence.Tothisend,weadd"positionalencodings"totheinputembeddingsatthe
bottomsoftheencoderanddecoderstacks.Thepositionalencodingshavethesamedimension
d
model
astheembeddings,sothatthetwocanbesummed.Therearemanychoicesofpositionalencodings,
learnedanded[9].
Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies:
PE
(
pos;
2
i
)
=
sin
(
pos=
10000
2
i=d
model
)
PE
(
pos;
2
i
+1)
=
cos
(
pos=
10000
2
i=d
model
)
where
pos
isthepositionand
i
isthedimension.Thatis,eachdimensionofthepositionalencoding
correspondstoasinusoid.Thewavelengthsformageometricprogressionfrom
2
ˇ
to
10000

2
ˇ
.We
chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby
relativepositions,sinceforanyedoffset
k
,
PE
pos
+
k
canberepresentedasalinearfunctionof
PE
pos
.
Wealsoexperimentedwithusinglearnedpositionalembeddings[
9
]instead,andfoundthatthetwo
versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion
becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered
duringtraining.
4WhySelf-Attention
Inthissectionwecomparevariousaspectsofself-attentionlayerstotherecurrentandconvolu-
tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations
(
x
1
;:::;x
n
)
toanothersequenceofequallength
(
z
1
;:::;z
n
)
,with
x
i
;z
i
2
R
d
,suchasahidden
layerinatypicalsequencetransductionencoderordecoder.Motivatingouruseofself-attentionwe
considerthreedesiderata.
Oneisthetotalcomputationalcomplexityperlayer.Anotheristheamountofcomputationthatcan
beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.
Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork.Learninglong-range
dependenciesisakeychallengeinmanysequencetransductiontasks.Onekeyfactoraffectingthe
abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto
traverseinthenetwork.Theshorterthesepathsbetweenanycombinationofpositionsintheinput
andoutputsequences,theeasieritistolearnlong-rangedependencies[
12
].Hencewealsocompare
themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe
differentlayertypes.
AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially
executedoperations,whereasarecurrentlayerrequires
O
(
n
)
sequentialoperations.Intermsof
computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence
6
length
n
issmallerthantherepresentationdimensionality
d
,whichismostoftenthecasewith
sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece
[
38
]andbyte-pair[
31
]representations.Toimprovecomputationalperformancefortasksinvolving
verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsize
r
in
theinputsequencecenteredaroundtherespectiveoutputposition.Thiswouldincreasethemaximum
pathlengthto
O
(
n=r
)
.Weplantoinvestigatethisapproachfurtherinfuturework.
Asingleconvolutionallayerwithkernelwidth
k<n
doesnotconnectallpairsofinputandoutput
positions.Doingsorequiresastackof
O
(
n=k
)
convolutionallayersinthecaseofcontiguouskernels,
or
O
(
log
k
(
n
))
inthecaseofdilatedconvolutions[
18
],increasingthelengthofthelongestpaths
betweenanytwopositionsinthenetwork.Convolutionallayersaregenerallymoreexpensivethan
recurrentlayers,byafactorof
k
.Separableconvolutions[
6
],however,decreasethecomplexity
considerably,to
O
(
k

n

d
+
n

d
2
)
.Evenwith
k
=
n
,however,thecomplexityofaseparable
convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,
theapproachwetakeinourmodel.
Assideself-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions
fromourmodelsandpresentanddiscussexamplesintheappendix.Notonlydoindividualattention
headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic
andsemanticstructureofthesentences.
5Training
Thissectiondescribesthetrainingregimeforourmodels.
5.1TrainingDataandBatching
WetrainedonthestandardWMT2014English-Germandatasetconsistingofabout4.5million
sentencepairs.Sentenceswereencodedusingbyte-pairencoding[
3
],whichhasasharedsource-
targetvocabularyofabout37000tokens.ForEnglish-French,weusedthelargerWMT
2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece
vocabulary[
38
].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining
batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000
targettokens.
5.2HardwareandSchedule
Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs.Forourbasemodelsusing
thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds.We
trainedthebasemodelsforatotalof100,000stepsor12hours.Forourbigmodels,(describedonthe
bottomlineoftable3),steptimewas1.0seconds.Thebigmodelsweretrainedfor300,000steps
(3.5days).
5.3Optimizer
WeusedtheAdamoptimizer[
20
]with

1
=0
:
9
,

2
=0
:
98
and

=10

9
.Wevariedthelearning
rateoverthecourseoftraining,accordingtotheformula:
lrate
=
d

0
:
5
model

min(
step
_
num

0
:
5
;step
_
num

warmup
_
steps

1
:
5
)
(3)
Thiscorrespondstoincreasingthelearningratelinearlyforthe
warmup
_
steps
trainingsteps,
anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber.Weused
warmup
_
steps
=4000
.
5.4Regularization
Weemploythreetypesofregularizationduringtraining:
7
Table2:TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe
English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.
Model
BLEUTrainingCost(FLOPs)
EN-DEEN-FREN-DEEN-FR
ByteNet[18]23.75
Deep-Att+PosUnk[39]39.2
1
:
0

10
20
GNMT+RL[38]24.639.92
2
:
3

10
19
1
:
4

10
20
ConvS2S[9]25.1640.46
9
:
6

10
18
1
:
5

10
20
MoE[32]26.0340.56
2
:
0

10
19
1
:
2

10
20
Deep-Att+PosUnkEnsemble[39]40.4
8
:
0

10
20
GNMT+RLEnsemble[38]26.3041.16
1
:
8

10
20
1
:
1

10
21
ConvS2SEnsemble[9]26.36
41.29
7
:
7

10
19
1
:
2

10
21
Transformer(basemodel)27.338.1
3
:
3

10
18
Transformer(big)
28.441.8
2
:
3

10
19
ResidualDropout
Weapplydropout[
33
]totheoutputofeachsub-layer,beforeitisaddedtothe
sub-layerinputandnormalized.Inaddition,weapplydropouttothesumsoftheembeddingsandthe
positionalencodingsinboththeencoderanddecoderstacks.Forthebasemodel,weusearateof
P
drop
=0
:
1
.
LabelSmoothing
Duringtraining,weemployedlabelsmoothingofvalue

ls
=0
:
1
[
36
].This
hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.
6Results
6.1MachineTranslation
OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)
inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan
2
:
0
BLEU,establishinganewstate-of-the-artBLEUscoreof
28
:
4
.Theofthismodelis
listedinthebottomlineofTable3.Trainingtook
3
:
5
dayson
8
P100GPUs.Evenourbasemodel
surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof
thecompetitivemodels.
OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof
41
:
0
,
outperformingallofthepreviouslypublishedsinglemodels,atlessthan
1
=
4
thetrainingcostofthe
previousstate-of-the-artmodel.TheTransformer(big)modeltrainedforEnglish-to-Frenchused
dropoutrate
P
drop
=0
:
1
,insteadof
0
:
3
.
Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which
werewrittenat10-minuteintervals.Forthebigmodels,weaveragedthelast20checkpoints.We
usedbeamsearchwithabeamsizeof
4
andlengthpenalty

=0
:
6
[
38
].Thesehyperparameters
werechosenafterexperimentationonthedevelopmentset.Wesetthemaximumoutputlengthduring
inferencetoinputlength+
50
,butterminateearlywhenpossible[38].
Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel
architecturesfromtheliterature.Weestimatethenumberofpointoperationsusedtotraina
modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained
single-precisioncapacityofeachGPU
5
.
6.2ModelVariations
ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel
indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe
5
Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.
8
Table3:VariationsontheTransformerarchitecture.Unlistedvaluesareidenticaltothoseofthebase
model.AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013.Listed
perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto
per-wordperplexities.
Nd
model
d
ff
hd
k
d
v
P
drop

ls
train
PPLBLEUparams
steps
(dev)(dev)

10
6
base
65122048864640.10.1100K
4.9225.865
(A)
1512512
5.2924.9
4128128
5.0025.5
163232
4.9125.8
321616
5.0125.4
(B)
16
5.1625.158
32
5.0125.460
(C)
2
6.1123.736
4
5.1925.350
8
4.8825.580
2563232
5.7524.528
1024128128
4.6626.0168
1024
5.1225.453
4096
4.7526.290
(D)
0.0
5.7724.6
0.2
4.9525.5
0.0
4.6725.3
0.2
5.4725.7
(E)
positionalembeddinginsteadofsinusoids
4.9225.7
big
610244096160.3300K
4.3326.4
213
developmentset,newstest2013.Weusedbeamsearchasdescribedintheprevioussection,butno
checkpointaveraging.WepresenttheseresultsinTable3.
InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,
keepingtheamountofcomputationconstant,asdescribedinSection3.2.2.Whilesingle-head
attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.
InTable3rows(B),weobservethatreducingtheattentionkeysize
d
k
hurtsmodelquality.This
suggeststhatdeterminingcompatibilityisnoteasyandthatamoresophisticatedcompatibility
functionthandotproductmaybeWefurtherobserveinrows(C)and(D)that,asexpected,
biggermodelsarebetter,anddropoutisveryhelpfulinavoidingoverInrow(E)wereplaceour
sinusoidalpositionalencodingwithlearnedpositionalembeddings[
9
],andobservenearlyidentical
resultstothebasemodel.
6.3EnglishConstituencyParsing
ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish
constituencyparsing.Thistaskpresentschallenges:theoutputissubjecttostrongstructural
constraintsandislongerthantheinput.Furthermore,RNNsequence-to-sequence
modelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].
Wetraineda4-layertransformerwith
d
model
=1024
ontheWallStreetJournal(WSJ)portionofthe
PennTreebank[
25
],about40Ktrainingsentences.Wealsotraineditinasemi-supervisedsetting,
usingthelargerandBerkleyParsercorporafromwithapproximately17Msentences
[
37
].Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens
forthesemi-supervisedsetting.
Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual
(section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters
remainedunchangedfromtheEnglish-to-Germanbasetranslationmodel.Duringinference,we
9
Table4:TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23
ofWSJ)
Parser
Training
WSJ23F1
Vinyals&Kaiserelal.(2014)[37]
WSJonly,discriminative
88.3
Petrovetal.(2006)[29]
WSJonly,discriminative
90.4
Zhuetal.(2013)[40]
WSJonly,discriminative
90.4
Dyeretal.(2016)[8]
WSJonly,discriminative
91.7
Transformer(4layers)
WSJonly,discriminative
91.3
Zhuetal.(2013)[40]
semi-supervised
91.3
Huang&Harper(2009)[14]
semi-supervised
91.3
McCloskyetal.(2006)[26]
semi-supervised
92.1
Vinyals&Kaiserelal.(2014)[37]
semi-supervised
92.1
Transformer(4layers)
semi-supervised
92.7
Luongetal.(2015)[23]
multi-task
93.0
Dyeretal.(2016)[8]
generative
93.3
increasedthemaximumoutputlengthtoinputlength+
300
.Weusedabeamsizeof
21
and

=0
:
3
forbothWSJonlyandthesemi-supervisedsetting.
OurresultsinTable4showthatdespitethelackoftuningourmodelperformssur-
prisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe
RecurrentNeuralNetworkGrammar[8].
IncontrasttoRNNsequence-to-sequencemodels[
37
],theTransformeroutperformstheBerkeley-
Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.
7Conclusion
Inthiswork,wepresentedtheTransformer,thesequencetransductionmodelbasedentirelyon
attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith
multi-headedself-attention.
Fortranslationtasks,theTransformercanbetrainedfasterthanarchitecturesbased
onrecurrentorconvolutionallayers.OnbothWMT2014English-to-GermanandWMT2014
English-to-Frenchtranslationtasks,weachieveanewstateoftheart.Intheformertaskourbest
modeloutperformsevenallpreviouslyreportedensembles.
Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks.We
plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand
toinvestigatelocal,restrictedattentionmechanismstoefhandlelargeinputsandoutputs
suchasimages,audioandvideo.Makinggenerationlesssequentialisanotherresearchgoalsofours.
Thecodeweusedtotrainandevaluateourmodelsisavailableat
https://github.com/
tensorflow/tensor2tensor
.
Acknowledgements
WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful
comments,correctionsandinspiration.
References
[1]
JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton.Layernormalization.
arXivpreprint
arXiv:1607.06450
,2016.
[2]
DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointly
learningtoalignandtranslate.
CoRR
,abs/1409.0473,2014.
[3]
DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le.Massiveexplorationofneural
machinetranslationarchitectures.
CoRR
,abs/1703.03906,2017.
[4]
JianpengCheng,LiDong,andMirellaLapata.Longshort-termmemory-networksformachine
reading.
arXivpreprintarXiv:1601.06733
,2016.
10
[5]
KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,
andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatistical
machinetranslation.
CoRR
,abs/1406.1078,2014.
[6]
FrancoisChollet.Xception:Deeplearningwithdepthwiseseparableconvolutions.
arXiv
preprintarXiv:1610.02357
,2016.
[7]
JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio.Empiricalevaluation
ofgatedrecurrentneuralnetworksonsequencemodeling.
CoRR
,abs/1412.3555,2014.
[8]
ChrisDyer,AdhigunaKuncoro,MiguelBallesteros,andNoahA.Smith.Recurrentneural
networkgrammars.In
Proc.ofNAACL
,2016.
[9]
JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin.Convolu-
tionalsequencetosequencelearning.
arXivpreprintarXiv:1705.03122v2
,2017.
[10]
AlexGraves.Generatingsequenceswithrecurrentneuralnetworks.
arXivpreprint
arXiv:1308.0850
,2013.
[11]
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforim-
agerecognition.In
ProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition
,pages770Œ778,2016.
[12]
SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber.Gradientwin
recurrentnets:thedifoflearninglong-termdependencies,2001.
[13]
SeppHochreiterandJürgenSchmidhuber.Longshort-termmemory.
Neuralcomputation
,
9(8):1735Œ1780,1997.
[14]
ZhongqiangHuangandMaryHarper.Self-trainingPCFGgrammarswithlatentannotations
acrosslanguages.In
Proceedingsofthe2009ConferenceonEmpiricalMethodsinNatural
LanguageProcessing
,pages832Œ841.ACL,August2009.
[15]
RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.Exploring
thelimitsoflanguagemodeling.
arXivpreprintarXiv:1602.02410
,2016.
[16]
KaiserandSamyBengio.Canactivememoryreplaceattention?In
AdvancesinNeural
InformationProcessingSystems,(NIPS)
,2016.
[17]
KaiserandIlyaSutskever.NeuralGPUslearnalgorithms.In
InternationalConference
onLearningRepresentations(ICLR)
,2016.
[18]
NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-
rayKavukcuoglu.Neuralmachinetranslationinlineartime.
arXivpreprintarXiv:1610.10099v2
,
2017.
[19]
YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush.Structuredattentionnetworks.
In
InternationalConferenceonLearningRepresentations
,2017.
[20]
DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.In
ICLR
,2015.
[21]
OleksiiKuchaievandBorisGinsburg.FactorizationtricksforLSTMnetworks.
arXivpreprint
arXiv:1703.10722
,2017.
[22]
ZhouhanLin,MinweiFeng,CiceroNogueiradosSantos,MoYu,BingXiang,Bowen
Zhou,andYoshuaBengio.Astructuredself-attentivesentenceembedding.
arXivpreprint
arXiv:1703.03130
,2017.
[23]
Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser.Multi-task
sequencetosequencelearning.
arXivpreprintarXiv:1511.06114
,2015.
[24]
Minh-ThangLuong,HieuPham,andChristopherDManning.Effectiveapproachestoattention-
basedneuralmachinetranslation.
arXivpreprintarXiv:1508.04025
,2015.
11
[25]
MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated
corpusofenglish:Thepenntreebank.
Computationallinguistics
,19(2):313Œ330,1993.
[26]
DavidMcClosky,EugeneCharniak,andMarkJohnson.Effectiveself-trainingforparsing.In
ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference
,
pages152Œ159.ACL,June2006.
[27]
AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit.Adecomposableattention
model.In
EmpiricalMethodsinNaturalLanguageProcessing
,2016.
[28]
RomainPaulus,CaimingXiong,andRichardSocher.Adeepreinforcedmodelforabstractive
summarization.
arXivpreprintarXiv:1705.04304
,2017.
[29]
SlavPetrov,LeonBarrett,RomainThibaux,andDanKlein.Learningaccurate,compact,
andinterpretabletreeannotation.In
Proceedingsofthe21stInternationalConferenceon
ComputationalLinguisticsand44thAnnualMeetingoftheACL
,pages433Œ440.ACL,July
2006.
[30]
PressandLiorWolf.Usingtheoutputembeddingtoimprovelanguagemodels.
arXiv
preprintarXiv:1608.05859
,2016.
[31]
RicoSennrich,BarryHaddow,andAlexandraBirch.Neuralmachinetranslationofrarewords
withsubwordunits.
arXivpreprintarXiv:1508.07909
,2015.
[32]
NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,
andJeffDean.Outrageouslylargeneuralnetworks:Thesparsely-gatedmixture-of-experts
layer.
arXivpreprintarXiv:1701.06538
,2017.
[33]
NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-
nov.Dropout:asimplewaytopreventneuralnetworksfromov
JournalofMachine
LearningResearch
,15(1):1929Œ1958,2014.
[34]
SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus.End-to-endmemory
networks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,
AdvancesinNeuralInformationProcessingSystems28
,pages2440Œ2448.CurranAssociates,
Inc.,2015.
[35]
IlyaSutskever,OriolVinyals,andQuocVVLe.Sequencetosequencelearningwithneural
networks.In
AdvancesinNeuralInformationProcessingSystems
,pages3104Œ3112,2014.
[36]
ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.
Rethinkingtheinceptionarchitectureforcomputervision.
CoRR
,abs/1512.00567,2015.
[37]
Vinyals&Kaiser,Koo,Petrov,Sutskever,andHinton.Grammarasaforeignlanguage.In
AdvancesinNeuralInformationProcessingSystems
,2015.
[38]
YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,Wolfgang
Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal.Google'sneuralmachine
translationsystem:Bridgingthegapbetweenhumanandmachinetranslation.
arXivpreprint
arXiv:1609.08144
,2016.
[39]
JieZhou,YingCao,XuguangWang,PengLi,andWeiXu.Deeprecurrentmodelswith
fast-forwardconnectionsforneuralmachinetranslation.
CoRR
,abs/1606.04199,2016.
[40]
MuhuaZhu,YueZhang,WenliangChen,MinZhang,andJingboZhu.Fastandaccurate
shift-reduceconstituentparsing.In
Proceedingsofthe51stAnnualMeetingoftheACL(Volume
1:LongPapers)
,pages434Œ443.ACL,August2013.
12
AttentionVisualizations
Figure3:Anexampleoftheattentionmechanismfollowinglong-distancedependenciesinthe
encoderself-attentioninlayer5of6.Manyoftheattentionheadsattendtoadistantdependencyof
theverb`making',completingthephrase`making...moredifAttentionshereshownonlyfor
theword`making'.Differentcolorsrepresentdifferentheads.Bestviewedincolor.
13
Figure4:Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution.Top:
Fullattentionsforhead5.Bottom:Isolatedattentionsfromjusttheword`its'forattentionheads5
and6.Notethattheattentionsareverysharpforthisword.
14
Figure5:Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe
sentence.Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention
atlayer5of6.Theheadsclearlylearnedtoperformdifferenttasks.
15
